===== ./app/routers/auth.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from datetime import timedelta
from app.utils.auth import create_access_token
from app.config import settings

router = APIRouter(tags=["Authentication"])

# Mock user database - replace with real checks in production
fake_users_db = {
    "nodejs_service": {
        "username": "nodejs_service",
        "password": "nodejs_service_password",
        "id": "nodejs_service",  # Add this line
        "sub": "nodejs_service"   # Add this line for JWT compatibility
    }
}

@router.post("/token")
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    user = fake_users_db.get(form_data.username)
    if not user or user["password"] != form_data.password:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
        )
    
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=timedelta(minutes=settings.JWT_ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    return {"access_token": access_token, "token_type": "bearer"}
===== ./app/routers/voice_processing.py =====
# app/routers/voice_processing.py - Fixed for Node.js integration
import asyncio
import time
import base64
import json
from datetime import datetime
from typing import Dict, Any, Optional

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import JSONResponse

from app.services.voice_service import VoiceService
from app.services.llm_service import OllamaService
from app.dependencies import get_current_user
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

# Initialize services
voice_service = VoiceService()
llm_service = OllamaService()

@router.post("/voice/process-streaming")
async def process_streaming_audio(
    request: dict,
    user: dict = Depends(get_current_user)
):
    """Process streaming audio from Node.js WebSocket - FIXED"""
    try:
        audio_data = request.get("audio_data")
        format_type = request.get("format", "webm")
        session_id = request.get("session_id")
        user_id = request.get("user_id")
        agent_id = request.get("agent_id")
        is_final = request.get("is_final", False)
        
        if not audio_data:
            raise HTTPException(status_code=400, detail="No audio data provided")
        
        logger.info(f"Processing streaming audio for session {session_id}")
        
        # Ensure voice service is initialized
        await voice_service._ensure_models_loaded()
        
        # Convert base64 to bytes
        try:
            audio_bytes = base64.b64decode(audio_data)
        except Exception as e:
            logger.error(f"Failed to decode audio data: {str(e)}")
            raise HTTPException(status_code=400, detail="Invalid audio data format")
        
        # Process with Whisper STT
        result = await voice_service.speech_to_text(
            audio_data=audio_bytes, 
            format=format_type, 
            user_id=user_id
        )
        
        return {
            "success": True,
            "transcription": result.get("text", ""),
            "confidence": result.get("confidence", 0.0),
            "is_final": is_final,
            "session_id": session_id,
            "processing_time": result.get("processing_time", 0)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Streaming audio processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

@router.post("/voice/tts")
async def text_to_speech_endpoint(
    request: dict,
    user: dict = Depends(get_current_user)
):
    """Convert text to speech - FIXED"""
    try:
        text = request.get("text")
        if not text:
            raise HTTPException(status_code=400, detail="No text provided")
        
        logger.info(f"üîä TTS Request: {text[:50]}...")
        
        # Initialize voice service
        await voice_service._ensure_models_loaded()
        
        # Generate TTS
        result = await voice_service.text_to_speech(
            text=text,
            user_id=request.get("user_id"),
            agent_id=request.get("agent_id"),
            voice_config=request.get("voice_config", {})
        )
        
        if not result.get("audio_base64"):
            raise HTTPException(status_code=500, detail="TTS generation failed")
        
        logger.info(f"‚úÖ TTS Success: {len(result['audio_base64'])} chars")
        
        return {
            "success": True,
            "audio_base64": result["audio_base64"],
            "format": "wav",
            "sample_rate": 16000
        }
        
    except Exception as e:
        logger.error(f"‚ùå TTS Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/voice/health")
async def voice_service_health():
    """Health check for voice service"""
    try:
        status_info = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "services": {
                "whisper": "available",
                "tts": "available", 
                "ollama": "checking..."
            }
        }
        
        # Quick Ollama health check
        try:
            ollama_status = await llm_service.check_ollama_status()
            status_info["services"]["ollama"] = "healthy" if ollama_status.get("status") == "healthy" else "degraded"
        except:
            status_info["services"]["ollama"] = "unavailable"
        
        return status_info
        
    except Exception as e:
        logger.error(f"Voice health check failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

===== ./app/routers/voice_websocket.py =====
# app/routers/voice_websocket.py - Fixed imports and error handling
import asyncio
import json
import time
import logging
import uuid
from typing import Dict, Any
from fastapi import APIRouter, WebSocket, WebSocketDisconnect

from app.services.ultra_fast_voice_service import UltraFastVoiceService
from app.services.connection_manager import ConnectionManager
from app.services.voice_session_manager import VoiceSessionManager
from app.utils.performance_monitor import PerformanceMonitor
from app.utils.advanced_vad import AdvancedVAD

router = APIRouter()
logger = logging.getLogger(__name__)

# Global managers
connection_manager = ConnectionManager()
session_manager = VoiceSessionManager()
performance_monitor = PerformanceMonitor()

class UltraFastWebSocketHandler:
    def __init__(self):
        self.voice_service = UltraFastVoiceService()
        self.vad_configs = {
            "ultra_fast": {
                "silenceThreshold": 0.01,
                "speechThreshold": 0.025,
                "minSilenceDuration": 600,
                "minSpeechDuration": 150,
                "maxRecordingTime": 12000,
                "vadSensitivity": 0.8,
                "endpointDetection": True
            }
        }

    async def handle_connection(self, websocket: WebSocket, connection_id: str):
        """Handle new WebSocket connection with ultra-fast setup"""
        try:
            await websocket.accept()
            logger.info(f"Ultra-fast voice WebSocket connected: {connection_id}")
            
            # Initialize connection
            await connection_manager.add_connection(connection_id, websocket)
            
            # Send immediate connection confirmation
            await self._send_message(websocket, {
                "type": "connection_established",
                "connection_id": connection_id,
                "timestamp": time.time(),
                "performance_mode": "ultra_fast"
            })
            
            # Message handling loop
            async for message in websocket.iter_text():
                try:
                    data = json.loads(message)
                    await self._handle_message(connection_id, websocket, data)
                except json.JSONDecodeError:
                    await self._send_error(websocket, "Invalid JSON format")
                except Exception as e:
                    logger.error(f"Message handling error: {str(e)}")
                    await self._send_error(websocket, f"Message processing failed: {str(e)}")
                    
        except WebSocketDisconnect:
            logger.info(f"WebSocket disconnected: {connection_id}")
        except Exception as e:
            logger.error(f"WebSocket connection error: {str(e)}")
        finally:
            await self._cleanup_connection(connection_id)

    async def _handle_message(self, connection_id: str, websocket: WebSocket, data: Dict[str, Any]):
        """Handle incoming WebSocket messages"""
        message_type = data.get("type")
        timestamp = time.time()
        
        try:
            if message_type == "authenticate":
                await self._handle_authentication(connection_id, websocket, data)
            elif message_type == "voice_input":
                await self._handle_voice_input(connection_id, websocket, data, timestamp)
            elif message_type == "text_input":
                await self._handle_text_input(connection_id, websocket, data, timestamp)
            elif message_type == "ping":
                await self._handle_ping(websocket, data.get("timestamp"))
            else:
                await self._send_error(websocket, f"Unknown message type: {message_type}")
                
        except Exception as e:
            logger.error(f"Message handling error for {message_type}: {str(e)}")
            await self._send_error(websocket, f"Failed to process {message_type}")

    async def _handle_authentication(self, connection_id: str, websocket: WebSocket, data: Dict[str, Any]):
        """Handle authentication with session setup"""
        try:
            user_id = data.get("user_id")
            agent_id = data.get("agent_id")
            
            if not user_id or not agent_id:
                await self._send_error(websocket, "user_id and agent_id are required")
                return
            
            # Create voice session (with minimal agent data for now)
            mock_agent = type('MockAgent', (), {
                '_id': agent_id,
                'name': 'Test Agent',
                'systemPrompt': 'You are a helpful assistant.'
            })()
            
            session = await session_manager.create_session(
                connection_id, user_id, agent_id, websocket, mock_agent
            )
            
            # Initialize voice service for this session
            await self.voice_service.initialize_session(session)
            
            await self._send_message(websocket, {
                "type": "authenticated",
                "session_id": connection_id,
                "user_id": user_id,
                "agent_id": agent_id,
                "performance_targets": {
                    "total_response_time": "< 1200ms",
                    "stt_time": "< 300ms",
                    "llm_time": "< 500ms", 
                    "tts_time": "< 400ms"
                }
            })
            
            logger.info(f"Session authenticated: {connection_id} for user {user_id}")
            
        except Exception as e:
            logger.error(f"Authentication error: {str(e)}")
            await self._send_error(websocket, f"Authentication failed: {str(e)}")

    async def _handle_voice_input(self, connection_id: str, websocket: WebSocket, data: Dict[str, Any], timestamp: float):
        """Handle voice input with ultra-fast processing"""
        try:
            session = await session_manager.get_session(connection_id)
            if not session:
                await self._send_error(websocket, "Session not authenticated")
                return
            
            audio_data = data.get("audio_data")
            if not audio_data:
                await self._send_error(websocket, "No audio data provided")
                return
            
            # Send processing acknowledgment
            await self._send_message(websocket, {
                "type": "processing_started",
                "timestamp": timestamp,
                "estimated_completion": timestamp + 1.2
            })
            
            # Process voice with ultra-fast service
            result = await self.voice_service.process_voice_ultra_fast(
                session=session,
                audio_data=audio_data,
                audio_format=data.get("format", "webm"),
                voice_config=data.get("voice_config", {}),
                processing_options=data.get("processing_options", {})
            )
            
            # Send response
            await self._send_message(websocket, {
                "type": "voice_response",
                "transcription": result["transcription"],
                "response_text": result["response"]["text"],
                "audio_data": result["response"]["audio"],
                "audio_format": result["response"].get("format", "wav"),
                "confidence": result["confidence"],
                "processing_time": result["processing_time"],
                "performance_metrics": result.get("performance_metrics", {}),
                "sources": result.get("sources", {}),
                "timestamp": time.time()
            })
            
        except Exception as e:
            logger.error(f"Voice input error: {str(e)}")
            await self._send_error(websocket, f"Voice processing failed: {str(e)}")

    async def _handle_text_input(self, connection_id: str, websocket: WebSocket, data: Dict[str, Any], timestamp: float):
        """Handle text input"""
        try:
            session = await session_manager.get_session(connection_id)
            if not session:
                await self._send_error(websocket, "Session not authenticated")
                return
            
            text = data.get("text", "").strip()
            if not text:
                await self._send_error(websocket, "No text provided")
                return
            
            result = await self.voice_service.process_text_ultra_fast(
                session=session,
                text=text,
                processing_options=data.get("processing_options", {})
            )
            
            await self._send_message(websocket, {
                "type": "text_response",
                "input": text,
                "response": result["response"],
                "processing_time": result["processing_time"],
                "sources": result.get("sources", {}),
                "timestamp": time.time()
            })
            
        except Exception as e:
            logger.error(f"Text input error: {str(e)}")
            await self._send_error(websocket, f"Text processing failed: {str(e)}")

    async def _handle_ping(self, websocket: WebSocket, client_timestamp: float = None):
        """Handle ping with latency measurement"""
        server_timestamp = time.time()
        latency = (server_timestamp - client_timestamp) * 1000 if client_timestamp else 0
        
        await self._send_message(websocket, {
            "type": "pong",
            "client_timestamp": client_timestamp,
            "server_timestamp": server_timestamp,
            "latency_ms": round(latency, 2) if client_timestamp else None
        })

    async def _send_message(self, websocket: WebSocket, message: Dict[str, Any]):
        """Send message with error handling"""
        try:
            await websocket.send_text(json.dumps(message))
        except Exception as e:
            logger.error(f"Failed to send message: {str(e)}")

    async def _send_error(self, websocket: WebSocket, error_message: str):
        """Send error message"""
        try:
            await websocket.send_text(json.dumps({
                "type": "error",
                "message": error_message,
                "timestamp": time.time()
            }))
        except Exception as e:
            logger.error(f"Failed to send error: {str(e)}")

    async def _cleanup_connection(self, connection_id: str):
        """Cleanup connection resources"""
        try:
            await session_manager.remove_session(connection_id)
            await connection_manager.remove_connection(connection_id)
            logger.info(f"Connection cleaned up: {connection_id}")
        except Exception as e:
            logger.error(f"Cleanup error: {str(e)}")

# Global handler instance
websocket_handler = UltraFastWebSocketHandler()

@router.websocket("/voice/ws")
async def voice_websocket_endpoint(websocket: WebSocket):
    """Ultra-fast voice WebSocket endpoint"""
    connection_id = f"voice_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    await websocket_handler.handle_connection(websocket, connection_id)

===== ./app/routers/health.py =====
# ./app/routers/health.py
from fastapi import APIRouter, HTTPException
from app.services.llm_service import OllamaService
import logging

router = APIRouter(prefix="/health", tags=["Health"])
logger = logging.getLogger(__name__)

@router.get("/ollama")
async def check_ollama_health():
    """Check Ollama service health and model availability"""
    try:
        service = OllamaService()  # ‚úÖ Fixed: Use OllamaService instead of LLMService
        status = await service.check_ollama_status()
        return status
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models")
async def list_available_models():
    """List all available models in Ollama"""
    try:
        service = OllamaService()  # ‚úÖ Fixed: Use OllamaService instead of LLMService
        models = await service.list_models()
        return {"models": models}
    except Exception as e:
        logger.error(f"Failed to list models: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/pull-model/{model_name}")
async def pull_model(model_name: str):
    """Pull a specific model"""
    try:
        service = OllamaService()  # ‚úÖ Fixed: Use OllamaService instead of LLMService
        success = await service.pull_model(model_name)
        if success:
            return {"message": f"Model {model_name} pulled successfully"}
        else:
            raise HTTPException(status_code=500, detail=f"Failed to pull model {model_name}")
    except Exception as e:
        logger.error(f"Failed to pull model {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

===== ./app/routers/monitoring.py =====
# ./app/routers/monitoring.py
from fastapi import APIRouter, Depends
from fastapi.responses import JSONResponse
from app.utils.monitoring import get_metrics
from app.dependencies import get_current_user

router = APIRouter(prefix="/monitoring", tags=["Monitoring"])

@router.get("/metrics")
async def metrics_endpoint(user: dict = Depends(get_current_user)):
    """Get Prometheus metrics"""
    if not user.get("is_admin", False):
        return JSONResponse(
            status_code=403,
            content={"detail": "Only admin users can access metrics"}
        )
    return get_metrics()
===== ./app/routers/execute.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.encoders import jsonable_encoder
from app.dependencies import get_current_user, get_db
from app.services.agent_execution_service import execute_agent
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List
import logging
import time

router = APIRouter(prefix="/execute", tags=["Agent Execution"])
logger = logging.getLogger(__name__)

# Enhanced request models
class ExecutionParameters(BaseModel):
    """Enhanced parameters for agent execution"""
    temperature: Optional[float] = Field(0.7, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(2000, ge=1, le=8000)
    top_p: Optional[float] = Field(0.9, ge=0.0, le=1.0)
    presence_penalty: Optional[float] = Field(0.0, ge=-2.0, le=2.0)
    frequency_penalty: Optional[float] = Field(0.0, ge=-2.0, le=2.0)
    
    # Search parameters
    enable_search: Optional[bool] = True
    search_sources: Optional[List[str]] = Field(default=["duckduckgo", "reddit"])
    max_search_results: Optional[int] = Field(3, ge=1, le=10)
    
    # Reasoning parameters - FIXED: Use 'pattern' instead of 'regex'
    reasoning_mode: Optional[str] = Field("balanced", pattern="^(fast|balanced|deep)$")
    context_priority: Optional[str] = Field("recent", pattern="^(recent|relevant|mixed)$")
    response_format: Optional[str] = Field("conversational", pattern="^(conversational|structured|json)$")
    
class ExecutionRequest(BaseModel):
    """Enhanced request model for agent execution"""
    input: str = Field(..., min_length=1, max_length=10000)
    parameters: Optional[ExecutionParameters] = Field(default_factory=ExecutionParameters)
    context: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None

class ExecutionResponse(BaseModel):
    """Standardized response model"""
    success: bool
    response: str
    model: str
    agent_id: str
    execution_time_ms: float
    personality_applied: Dict[str, Any]
    context_length: int
    search_info: Dict[str, Any]
    reasoning_steps: Optional[List[str]] = None
    sources: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

@router.post("/{agent_id}", response_model=ExecutionResponse)
async def execute_agent_endpoint(
    agent_id: str,
    request_data: ExecutionRequest,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Execute an AI agent with enhanced capabilities including internet search.
    
    Features:
    - Personality-aware responses
    - Multi-source internet search (DuckDuckGo, Reddit, Wikipedia, Hacker News)
    - Conversation memory
    - Flexible parameter control
    - Structured error handling
    """
    start_time = time.time()
    
    try:
        # Validate agent_id format
        if not agent_id or len(agent_id.strip()) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Agent ID cannot be empty"
            )
        
        # Convert request to service input format
        input_data = {
            "input": request_data.input,
            "parameters": jsonable_encoder(request_data.parameters.dict()) if request_data.parameters else {},
            "context": request_data.context or {},
            "metadata": request_data.metadata or {}
        }
        
        # Execute agent
        result = await execute_agent(agent_id, user["sub"], input_data, db)
        
        # Calculate execution time
        execution_time = (time.time() - start_time) * 1000
        
        # Build response
        response = ExecutionResponse(
            success=result.get("success", True),
            response=result.get("response", ""),
            model=result.get("model", "unknown"),
            agent_id=agent_id,
            execution_time_ms=round(execution_time, 2),
            personality_applied=result.get("personality_applied", {}),
            context_length=result.get("context_length", 0),
            search_info=result.get("search_info", {"performed": False}),
            reasoning_steps=result.get("reasoning_steps"),
            sources=result.get("sources"),
            metadata={
                "user_id": user["sub"],
                "timestamp": time.time(),
                "request_id": f"{agent_id}_{int(time.time())}"
            },
            error=result.get("error")
        )
        
        return response
        
    except ValueError as ve:
        logger.error(f"Validation error for agent {agent_id}: {str(ve)}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, 
            detail=f"Agent not found: {str(ve)}"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Execution failed for agent {agent_id}: {str(e)}", exc_info=True)
        
        execution_time = (time.time() - start_time) * 1000
        
        # Return structured error response
        return ExecutionResponse(
            success=False,
            response="I apologize, but I encountered an error processing your request.",
            model="unknown",
            agent_id=agent_id,
            execution_time_ms=round(execution_time, 2),
            personality_applied={},
            context_length=0,
            search_info={"performed": False, "error": str(e)},
            error=str(e),
            metadata={
                "user_id": user["sub"],
                "timestamp": time.time(),
                "error_type": type(e).__name__
            }
        )

# Backwards compatibility endpoint
@router.post("/{agent_id}/legacy")
async def execute_agent_legacy(
    agent_id: str,
    request_data: dict,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Legacy endpoint for backwards compatibility"""
    try:
        # Convert legacy format to new format
        execution_request = ExecutionRequest(
            input=request_data.get("input", ""),
            parameters=ExecutionParameters(**request_data.get("parameters", {}))
        )
        
        return await execute_agent_endpoint(agent_id, execution_request, user, db)
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Execution failed: {str(e)}")

===== ./app/routers/__init__.py =====
# app/routers/__init__.py
from .auth import router as auth_router
from .agents import router as agents_router
from .chat import router as chat_router
from .rag import router as rag_router
from .training import router as training_router
from .voice import router as voice_router
from .voice_websocket import router as voice_websocket_router
from .agent_interaction import router as agent_interaction_router
from .health import router as health_router
from .execute import router as execute_router
from .monitoring import router as monitoring_router

__all__ = [
    "auth_router",
    "agents_router", 
    "chat_router",
    "rag_router",
    "training_router",
    "voice_router",
    "voice_websocket_router",
    "agent_interaction_router",
    "health_router",
    "execute_router",
    "monitoring_router"
]

===== ./app/routers/voice_agent.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Path
from fastapi.responses import StreamingResponse
from app.services.voice_service import VoiceService
from app.services.voice_agent_service import VoiceAgentService
from app.services.agent_service import AgentService
from app.dependencies import get_current_user
import io

router = APIRouter()

@router.post("/voice-chat/{agent_id}")
async def voice_chat_with_agent(
    agent_id: int = Path(..., title="The ID of the agent to chat with"),
    audio_file: UploadFile = File(...),
    user: dict = Depends(get_current_user),
    voice_service: VoiceService = Depends(),
    voice_agent_service: VoiceAgentService = Depends(),
    agent_service: AgentService = Depends(),
):
    """
    Handles a full voice conversation with an agent.
    1. Converts user's speech to text.
    2. Gets a conversational response from the agent's brain.
    3. Converts the response back to speech and streams it.
    """
    if not audio_file.content_type.startswith('audio/'):
        raise HTTPException(status_code=400, detail="File must be an audio file")

    try:
        # Get user ID from the token
        user_id = user.get("user_id")
        if not user_id:
            raise HTTPException(status_code=401, detail="Could not validate credentials")

        # 1. Get agent details
        agent = await agent_service.get_agent(agent_id, user_id)
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")

        # 2. Speech-to-Text
        user_text = await voice_service.speech_to_text(audio_file)
        if not user_text:
            # Return empty audio if transcription fails
            return StreamingResponse(io.BytesIO(), media_type="audio/wav")

        # 3. Get response from agent's brain
        agent_response_text = await voice_agent_service.voice_chat(user_id, user_text, agent)

        # 4. Text-to-Speech
        audio_data = await voice_service.text_to_speech(agent_response_text)
        
        return StreamingResponse(io.BytesIO(audio_data), media_type="audio/wav")

    except HTTPException as e:
        # Re-raise HTTP exceptions
        raise e
    except Exception as e:
        # Handle other exceptions
        raise HTTPException(status_code=500, detail=str(e))

===== ./app/routers/agents.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse, AgentPersonality
from app.services.agent_service import AgentService
from app.dependencies import get_db, get_current_user
from typing import List
from sqlalchemy import select, delete
from typing import TYPE_CHECKING
from pydantic import BaseModel, Field
from datetime import datetime
import logging
from app.services.conversation_service import ConversationService
from app.services.rag_service import RAGService
from app.models.db_models import (
    DBAgent, 
    DBConversation, 
    DBMessage, 
    DBDocument, 
    DBTrainingJob
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/agents", tags=["agents"])

if TYPE_CHECKING:
    from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse

class VoiceModelConfig(BaseModel):
    """Configuration for voice models"""
    model_name: str = Field(default="default", description="Voice model to use")
    speaking_rate: float = Field(default=1.0, ge=0.5, le=2.0, description="Speech rate multiplier")
    pitch: float = Field(default=0.0, ge=-20.0, le=20.0, description="Pitch adjustment in semitones")
    volume_gain: float = Field(default=0.0, ge=-96.0, le=16.0, description="Volume gain in dB")

@router.post("", response_model=AgentResponse, status_code=status.HTTP_201_CREATED)
async def create_agent(
    agent_data: AgentCreate,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    try:
        # Initialize default voice config if not provided
        # Fix: Use agent_metadata instead of metadata
        metadata = agent_data.agent_metadata or {}
        
        if "voice_config" not in metadata:
            metadata["voice_config"] = VoiceModelConfig().dict()
        
        # Convert personality to dict for storage
        personality_dict = agent_data.personality.dict() if agent_data.personality else {}
        
        db_agent = DBAgent(
            owner_id=user["sub"],
            name=agent_data.name,
            description=agent_data.description,
            model=agent_data.model,
            system_prompt=agent_data.system_prompt,
            is_public=agent_data.is_public,
            tools=agent_data.tools,
            personality=personality_dict,  # Add personality field
            agent_metadata=metadata,
            voice_enabled=True
        )
        
        db.add(db_agent)
        await db.commit()
        await db.refresh(db_agent)
        
        # Create response with proper field mapping
        response_data = {
            "id": str(db_agent.id),
            "owner_id": db_agent.owner_id,
            "name": db_agent.name,
            "description": db_agent.description,
            "model": db_agent.model,
            "system_prompt": db_agent.system_prompt,
            "is_public": db_agent.is_public,
            "tools": db_agent.tools,
            "personality": AgentPersonality(**db_agent.personality) if db_agent.personality else AgentPersonality(),
            "agent_metadata": db_agent.agent_metadata,
            "created_at": db_agent.created_at,
            "updated_at": db_agent.updated_at
        }
        
        return AgentResponse(**response_data)
        
    except Exception as e:
        logger.error(f"Failed to create agent: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create agent: {str(e)}"
        )






@router.get("", response_model=List[AgentResponse])
async def list_agents(
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    result = await db.execute(
        select(DBAgent).where(DBAgent.owner_id == user["sub"])
    )
    agents = result.scalars().all()
    
    # Convert to response format
    response_agents = []
    for agent in agents:
        response_data = {
            "id": str(agent.id),
            "owner_id": agent.owner_id,
            "name": agent.name,
            "description": agent.description,
            "model": agent.model,
            "system_prompt": agent.system_prompt,
            "is_public": agent.is_public,
            "tools": agent.tools,
            "personality": AgentPersonality(**agent.personality) if agent.personality else AgentPersonality(),
            "agent_metadata": agent.agent_metadata,
            "created_at": agent.created_at,
            "updated_at": agent.updated_at
        }
        response_agents.append(AgentResponse(**response_data))
    
    return response_agents


@router.get("/{agent_id}", response_model=AgentResponse)
async def get_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    result = await db.execute(
        select(DBAgent).where(
            DBAgent.id == agent_id,
            DBAgent.owner_id == user["sub"]
        )
    )
    agent = result.scalars().first()
    
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    
    # Ensure voice config exists
    if not agent.agent_metadata or "voice_config" not in agent.agent_metadata:
        if not agent.agent_metadata:
            agent.agent_metadata = {}
        agent.agent_metadata["voice_config"] = VoiceModelConfig().dict()
        await db.commit()
        await db.refresh(agent)
    
    # Convert to response format
    response_data = {
        "id": str(agent.id),
        "owner_id": agent.owner_id,
        "name": agent.name,
        "description": agent.description,
        "model": agent.model,
        "system_prompt": agent.system_prompt,
        "is_public": agent.is_public,
        "tools": agent.tools,
        "personality": AgentPersonality(**agent.personality) if agent.personality else AgentPersonality(),
        "agent_metadata": agent.agent_metadata,
        "created_at": agent.created_at,
        "updated_at": agent.updated_at
    }
    
    return AgentResponse(**response_data)


@router.put("/{agent_id}", response_model=AgentResponse)
async def update_agent(
    agent_id: str,
    agent_data: AgentUpdate,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    try:
        # Get existing agent
        result = await db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == user["sub"]
            )
        )
        agent = result.scalars().first()
        
        if not agent:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Agent not found"
            )
        
        # Update fields that are provided
        update_data = agent_data.dict(exclude_unset=True)
        
        for field, value in update_data.items():
            if field == "personality" and value is not None:
                # **FIX: Replace personality completely instead of merging**
                if isinstance(value, AgentPersonality):
                    agent.personality = value.dict()
                else:
                    # Handle partial personality updates properly
                    new_personality = AgentPersonality(**value).dict()
                    agent.personality = new_personality
            elif field == "agent_metadata" and value is not None:
                # Merge metadata (this is fine for metadata)
                existing_metadata = agent.agent_metadata or {}
                existing_metadata.update(value)
                agent.agent_metadata = existing_metadata
            elif hasattr(agent, field) and value is not None:
                setattr(agent, field, value)
        
        agent.updated_at = datetime.utcnow()
        await db.commit()
        await db.refresh(agent)
        
        # Convert to response
        response_data = {
            "id": str(agent.id),
            "owner_id": agent.owner_id,
            "name": agent.name,
            "description": agent.description,
            "model": agent.model,
            "system_prompt": agent.system_prompt,
            "is_public": agent.is_public,
            "tools": agent.tools,
            "personality": AgentPersonality(**agent.personality) if agent.personality else AgentPersonality(),
            "agent_metadata": agent.agent_metadata,
            "created_at": agent.created_at,
            "updated_at": agent.updated_at
        }
        
        return AgentResponse(**response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update agent: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update agent: {str(e)}"
        )



@router.patch("/{agent_id}/voice", response_model=AgentResponse)
async def update_voice_config(
    agent_id: str,
    voice_config: VoiceModelConfig,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """Update only the voice configuration for an agent"""
    result = await db.execute(
        select(DBAgent).where(
            DBAgent.id == agent_id,
            DBAgent.owner_id == user["sub"]
        )
    )
    agent = result.scalars().first()
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    
    # Update voice config
    if not agent.agent_metadata:
        agent.agent_metadata = {}
    agent.agent_metadata["voice_config"] = voice_config.dict()
    agent.voice_enabled = True  # Ensure voice stays enabled
    
    await db.commit()
    await db.refresh(agent)
    return agent

@router.delete("/{agent_id}", status_code=status.HTTP_200_OK)
async def delete_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """Delete an agent and all its associated data"""
    try:
        # Get existing agent first
        result = await db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == user["sub"]
            )
        )
        agent = result.scalars().first()
        
        if not agent:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Agent not found"
            )
        
        # Initialize services correctly
        rag_service = RAGService()  # ‚úÖ FIXED - no db parameter
        await rag_service.initialize()  # Initialize the service
        
        conversation_service = ConversationService(db)  # ConversationService does take db
        
        # Clean up associated data
        try:
            # Delete agent's RAG data
            await rag_service.delete_agent_data(agent_id, user["sub"])
            
            # Clear conversation history
            await conversation_service.clear_conversation(agent_id, user["sub"])
            
        except Exception as cleanup_error:
            logger.warning(f"Error during cleanup: {str(cleanup_error)}")
            # Continue with agent deletion even if cleanup fails
        
        # Delete the agent record

        await db.execute(
            delete(DBConversation).where(
                DBConversation.agent_id == agent_id,
                DBConversation.user_id == user["sub"]
            )
        )

# Then delete the agent
        await db.execute(
            delete(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == user["sub"]
            )
        )
        await db.commit()
        
        logger.info(f"Successfully deleted agent {agent_id}")
        
        # Clean up RAG service resources
        await rag_service.close()
        
        return {
            "message": "Agent deleted successfully",
            "agent_id": agent_id,
            "status": "success"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete agent: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete agent: {str(e)}"
        )

===== ./app/routers/voice.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from app.models.voice_model import VoiceResponse, TextToSpeechRequest
from app.services.voice_service import VoiceService
from app.dependencies import get_current_user
from fastapi.responses import StreamingResponse
import io

router = APIRouter()

@router.post("/stt", response_model=VoiceResponse)
async def speech_to_text(
    audio_file: UploadFile = File(...),
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert speech to text"""
    try:
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
        
        text = await voice_service.speech_to_text(audio_file)
        return VoiceResponse(text=text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tts")
async def text_to_speech(
    request: TextToSpeechRequest,
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert text to speech"""
    try:
        audio_data = await voice_service.text_to_speech(request.text)
        return StreamingResponse(
            io.BytesIO(audio_data),
            media_type="audio/wav"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/routers/chat.py =====
# app/routers/chat.py
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
from typing import Optional

from app.models.response_schema import ChatRequest, ChatResponse
from app.services.llm_service import OllamaService
from app.dependencies import get_current_user, get_db
from app.services.conversation_service import ConversationService
from app.models.db_models import DBAgent
from app.services.cache import cache_service
import json

router = APIRouter(prefix="/agents/{agent_id}", tags=["chat"])

async def get_chat_history_from_cache(agent_id: str, user_id: str) -> list:
    """Get chat history from Redis cache"""
    cache_key = f"chat_history:{user_id}:{agent_id}"
    history = await cache_service.get(cache_key)
    return json.loads(history) if history else []

async def save_chat_history_to_cache(agent_id: str, user_id: str, history: list):
    """Save chat history to Redis cache"""
    cache_key = f"chat_history:{user_id}:{agent_id}"
    await cache_service.set(cache_key, json.dumps(history), ttl=86400)  # 24 hours TTL

@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(
    agent_id: str,
    chat_request: ChatRequest,
    user: dict = Depends(get_current_user),
    llm_service: OllamaService = Depends(),
    db: AsyncSession = Depends(get_db),  # Proper dependency injection
):
    # Verify agent exists and user has access
    try:
        result = await db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                or_(
                    DBAgent.owner_id == user["sub"],
                    DBAgent.is_public == True
                )
            )
        )
        agent = result.scalars().first()
        if not agent:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Agent not found or access denied"
            )
        
        # Get conversation history from Redis
        conversation_history = await get_chat_history_from_cache(agent_id, user["sub"])
        
        # Build messages with context
        messages = [
            {"role": "system", "content": agent.system_prompt},
            *conversation_history,
            {"role": "user", "content": chat_request.message}
        ]
        
        # Get response from LLM
        response = await llm_service.chat(
            messages=messages,
            model=agent.model,
            options=chat_request.options
        )
        
        # Update conversation history
        new_history = conversation_history + [
            {"role": "user", "content": chat_request.message},
            {"role": "assistant", "content": response.get("message", {}).get("content", "")}
        ]
        
        # Keep only last 10 messages to avoid too large context
        new_history = new_history[-10:]
        
        # Save updated history to Redis
        await save_chat_history_to_cache(agent_id, user["sub"], new_history)
        
        return ChatResponse(
            message=response.get("message", {}).get("content", ""),
            model=agent.model,
            context=new_history,
            tokens_used=response.get("eval_count", 0)
        )
    finally:
        await db.close()
===== ./app/routers/rag.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, status
from typing import List, Optional
from app.models.response_schema import RAGResponse, DocumentResponse, RAGQueryRequest
from app.services.rag_service import RAGService
from app.dependencies import get_current_user, get_db
from sqlalchemy.ext.asyncio import AsyncSession
import logging

router = APIRouter(prefix="/rag", tags=["RAG Operations"])
logger = logging.getLogger(__name__)

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post(
    "/upload",
    response_model=DocumentResponse,
    status_code=status.HTTP_201_CREATED
)
async def upload_document(
    file: UploadFile = File(...),
    agent_id: Optional[str] = None,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload a document for RAG processing"""
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No filename provided"
        )

    try:
        # Validate file type
        if not file.filename.lower().endswith(('.pdf', '.txt', '.md')):
            raise HTTPException(
                status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail="Only PDF, TXT, and MD files are supported"
            )

        contents = await file.read()
        
        # Extract user ID properly
        user_id = extract_user_id(user)
        
        doc_id = await rag_service.ingest_document(
            db=db,
            user_id=user_id,
            filename=file.filename,
            content=contents,
            agent_id=agent_id
        )

        return DocumentResponse(
            document_id=str(doc_id),
            filename=file.filename,
            status="success"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document upload failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process document"
        )

# rag.py
@router.post("/query", response_model=RAGResponse)
async def query_documents(
    request: RAGQueryRequest,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    agent_id: Optional[str] = None  # Add this parameter
):
    """Enhanced query endpoint with hybrid search capabilities"""
    if not request.query or len(request.query.strip()) < 3:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Query must be at least 3 characters"
        )

    try:
        user_id = extract_user_id(user)
        
        results = await rag_service.query(
            db=db,
            user_id=user_id,
            query=request.query,
            max_results=request.max_results,
            filters=request.filters,
            rewrite_query=request.rewrite_query,
            use_reranking=request.use_reranking,
            agent_id=agent_id  # Pass the agent_id
        )
        
        return RAGResponse(
            answer=results.get("answer", "No answer found"),
            documents=results.get("documents", []),
            context=results.get("context", []),
            sources=results.get("sources", []),
            debug=results.get("debug_info", {}),
            search_method=results.get("debug_info", {}).get("search_method"),
            processed_query=results.get("debug_info", {}).get("processed_query")
        )
    except Exception as e:
        logger.error(f"Query failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process query"
        )

@router.get("/documents", response_model=List[DocumentResponse])
async def list_documents(
    agent_id: Optional[str] = None,  # Add this parameter
    page: int = 1,
    per_page: int = 10,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List all documents for the user with pagination"""
    try:
        if page < 1 or per_page < 1:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Page and per_page must be positive integers"
            )

        user_id = extract_user_id(user)

        return await rag_service.list_documents(
            db=db,
            agent_id=agent_id,
            user_id=user_id,
            page=page,
            per_page=per_page
        )

    except Exception as e:
        logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve documents"
        )

@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    agent_id: Optional[str] = None
):
    """Delete a specific document"""
    try:
        # Clean the document ID by removing any quotes or special characters
        document_id = document_id.strip('"\'')
        user_id = extract_user_id(user)
        
        logger.info(f"Attempting to delete document {document_id} for user {user_id}")
        
        success = await rag_service.delete_document(
            db=db,
            user_id=user_id,
            document_id=document_id,
            agent_id=agent_id
        )
        
        if not success:
            logger.warning(f"Document not found: {document_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or not owned by user"
            )
            
        logger.info(f"Successfully deleted document {document_id}")
        return {"status": "success", "deleted_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document deletion failed for {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete document: {str(e)}"
        )
===== ./app/routers/voice_health.py =====
# app/routers/voice_health.py
from fastapi import APIRouter, HTTPException, status
from fastapi.responses import JSONResponse
from app.services.ultra_fast_voice_service import UltraFastVoiceService
from app.services.connection_manager import ConnectionManager
from app.services.voice_session_manager import VoiceSessionManager
import logging
import time

router = APIRouter()
logger = logging.getLogger(__name__)

# Global instances for health checks
connection_manager = ConnectionManager()
session_manager = VoiceSessionManager()

@router.get("/health")
async def voice_websocket_health():
    """Health check for voice WebSocket service"""
    try:
        health_data = {
            "status": "healthy",
            "timestamp": time.time(),
            "service": "voice_websocket",
            "connections": {
                "active": connection_manager.get_connection_count(),
                "max_allowed": 100
            },
            "sessions": {
                "active": len(session_manager.sessions),
                "total_created": session_manager.get_total_sessions()
            },
            "performance": {
                "average_response_time": "< 1200ms",
                "target_achievement": "95%+"
            },
            "endpoints": {
                "websocket": "/api/v1/voice/ws",
                "health": "/api/v1/voice/ws/health"
            }
        }
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content=health_data
        )
        
    except Exception as e:
        logger.error(f"Voice health check failed: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={
                "status": "unhealthy",
                "error": str(e),
                "timestamp": time.time()
            }
        )

@router.get("/metrics")
async def voice_metrics():
    """Get voice service metrics"""
    try:
        voice_service = UltraFastVoiceService()
        metrics = await voice_service.get_performance_metrics()
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "metrics": metrics,
                "timestamp": time.time()
            }
        )
        
    except Exception as e:
        logger.error(f"Voice metrics failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get metrics: {str(e)}"
        )

@router.post("/test")
async def test_voice_processing():
    """Test voice processing pipeline"""
    try:
        voice_service = UltraFastVoiceService()
        
        # Test with sample data
        test_result = await voice_service.test_pipeline()
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "test_result": test_result,
                "status": "success",
                "timestamp": time.time()
            }
        )
        
    except Exception as e:
        logger.error(f"Voice test failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Voice test failed: {str(e)}"
        )

===== ./app/routers/training.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks, status, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobCreate, 
    TrainingProgress,
    TrainingDataItem,
    TrainingDataType,
    is_supported_file,
    get_file_type,
    FileUploadInfo
)
from app.services.training_service import TrainingService
from app.dependencies import get_current_user
import logging
import uuid
import json
import os
from datetime import datetime

router = APIRouter()
logger = logging.getLogger(__name__)

# Global instance to maintain state across requests
_training_service_instance = TrainingService()

def get_training_service() -> TrainingService:
    """Get training service singleton instance"""
    return _training_service_instance

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post("", response_model=TrainingJobResponse)
async def create_training_job(
    training_data: TrainingJobCreate,
    background_tasks: BackgroundTasks,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a new training job with enhanced data support"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job for user_id: {user_id}, agent_id: {training_data.agent_id}")
        
        # Validate that at least some data is provided
        has_data = any([
            training_data.data_urls,
            training_data.training_data,
            training_data.text_data
        ])
        
        if not has_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one data source must be provided (data_urls, training_data, or text_data)"
            )
            
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=training_data.agent_id,
            data_urls=training_data.data_urls or [],
            training_data=training_data.training_data or [],
            text_data=training_data.text_data or [],
            config=training_data.config or {}
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created enhanced job: {job.id} for user: {user_id}, agent: {training_data.agent_id}")
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.post("/upload", response_model=TrainingJobResponse)
async def create_training_job_with_files(
    files: List[UploadFile],
    background_tasks: BackgroundTasks,
    agent_id: str = Form(...),
    text_data: Optional[str] = Form(None),  # JSON string of text array
    config: Optional[str] = Form("{}"),     # JSON string of config
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a training job with file uploads"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job with files for user_id: {user_id}, agent_id: {agent_id}")
        
        # Validate files
        if not files or len(files) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one file must be uploaded"
            )
        
        uploaded_files = []
        training_data_items = []
        
        for file in files:
            if not file.filename:
                continue
                
            if not is_supported_file(file.filename):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"File type not supported: {file.filename}"
                )
            
            # Generate unique file ID
            file_id = str(uuid.uuid4())
            file_content = await file.read()
            
            # Store file information
            file_info = FileUploadInfo(
                filename=file.filename,
                content_type=file.content_type or "application/octet-stream",
                size=len(file_content),
                file_id=file_id
            )
            uploaded_files.append(file_info)
            
            # Save file temporarily for processing
            temp_path = await training_service.save_uploaded_file(
                user_id, file_id, file.filename, file_content
            )
            
            # Create training data item
            data_item = TrainingDataItem(
                type=get_file_type(file.filename),
                content=temp_path,
                metadata={
                    "filename": file.filename,
                    "file_id": file_id,
                    "content_type": file.content_type,
                    "size": len(file_content)
                }
            )
            training_data_items.append(data_item)
        
        # Parse text data if provided
        parsed_text_data = []
        if text_data:
            try:
                parsed_text_data = json.loads(text_data)
                if not isinstance(parsed_text_data, list):
                    raise ValueError("text_data must be a JSON array")
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for text_data"
                )
        
        # Parse config
        parsed_config = {}
        if config:
            try:
                parsed_config = json.loads(config)
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for config"
                )
        
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=agent_id,
            data_urls=[],
            training_data=training_data_items,
            text_data=parsed_text_data,
            uploaded_files=uploaded_files,
            config=parsed_config
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created file-based job: {job.id} with {len(uploaded_files)} files")
        return job
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job with files: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("", response_model=List[TrainingJobResponse])
async def list_training_jobs(
    agent_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """List training jobs for an agent"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Listing jobs for user_id: {user_id}, agent_id: {agent_id}")
        
        jobs = await training_service.list_jobs(user_id, agent_id)
        logger.info(f"Found {len(jobs)} jobs for user: {user_id}, agent: {agent_id}")
        
        return jobs
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing training jobs: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )

@router.get("/{job_id}/progress", response_model=TrainingProgress)
async def get_training_progress(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get real-time training progress"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting progress for job: {job_id}, user: {user_id}")
        
        progress = await training_service.get_progress(user_id, job_id)
        if not progress:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or access denied"
            )
        
        return progress
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training progress: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.delete("/{job_id}")
async def cancel_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Cancel a training job"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Cancelling job: {job_id} for user: {user_id}")
        
        success = await training_service.cancel_job(user_id, job_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or cannot be cancelled"
            )
        
        return {"message": "Job cancelled successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/debug/all-jobs")
async def debug_all_jobs(
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Debug endpoint to see all jobs in memory"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Debug: Getting all jobs for user: {user_id}")
        
        # Access the jobs dictionary directly for debugging
        all_jobs = []
        for job_id, job in training_service.jobs.items():
            job_info = {
                "job_id": job_id,
                "user_id": job.user_id,
                "agent_id": job.agent_id,
                "status": job.status,
                "created_at": job.created_at,
                "data_sources": {
                    "urls": len(job.data_urls),
                    "training_data": len(job.training_data),
                    "text_data": len(job.text_data),
                    "uploaded_files": len(job.uploaded_files)
                }
            }
            all_jobs.append(job_info)
        
        # Filter for current user
        user_jobs = [job for job in all_jobs if job["user_id"] == user_id]
        
        return {
            "current_user_id": user_id,
            "total_jobs_in_system": len(all_jobs),
            "jobs_for_current_user": len(user_jobs),
            "all_jobs": all_jobs,
            "user_jobs": user_jobs
        }
    except Exception as e:
        logger.error(f"Debug endpoint error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{job_id}", response_model=TrainingJobResponse)
async def get_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get training job details"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting job: {job_id} for user: {user_id}")
        
        job = await training_service.get_job(user_id, job_id)
        if not job:
            logger.warning(f"Job not found: {job_id} for user: {user_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, 
                detail="Job not found"
            )
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )
===== ./app/routers/agent_interaction.py =====
# ./app/routers/agent_interaction.py
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, Form
from fastapi.responses import Response
from app.services.agent_interaction_service import AgentInteractionService
from app.dependencies import get_db, get_current_user
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
import logging
import base64

router = APIRouter(prefix="/interact", tags=["Agent Interaction"])
logger = logging.getLogger(__name__)

@router.post("/chat/{agent_id}")
async def chat_with_agent(
    agent_id: str,
    input_data: dict,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Chat with an agent using text"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            input_text=input_data.get("message"),
            db=db
        )
        return response
    except Exception as e:
        logger.error(f"Chat failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/voice/{agent_id}")
async def voice_interaction(
    agent_id: str,
    audio_file: UploadFile = File(...),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Interact with an agent using voice"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        # Validate audio file
        if audio_file.content_type and not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
        
        logger.info(f"Processing voice interaction for agent {agent_id}")
        logger.info(f"Audio file: {audio_file.filename}, size: {audio_file.size if hasattr(audio_file, 'size') else 'unknown'}")
        
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            audio_file=audio_file,
            db=db
        )
        
        # Convert response to speech
        speech_response = await service.text_to_speech(
            response["text_response"],
            response["emotional_state"]
        )
        
        return {
            "text": response["text_response"],
            "audio": base64.b64encode(speech_response).decode() if speech_response else None,
            "emotion": response["emotional_state"],
            "context_used": response.get("context_used", [])
        }
    except Exception as e:
        logger.error(f"Voice interaction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Update the chat history endpoints in agent_interaction.py

@router.get("/history/{agent_id}")
async def get_conversation_history(
    agent_id: str,
    user: dict = Depends(get_current_user)
):
    """Get conversation history for a user with specific agent"""
    try:
        history = await cache_service.get_chat_history(user["sub"], agent_id)
        return {"history": history or []}
    except Exception as e:
        logger.error(f"Failed to get conversation history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/history/{agent_id}")
async def clear_conversation_history(
    agent_id: str,
    user: dict = Depends(get_current_user)
):
    """Clear conversation history for a user with specific agent"""
    try:
        success = await cache_service.clear_chat_history(user["sub"], agent_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to clear history")
        return {"message": "Conversation history cleared"}
    except Exception as e:
        logger.error(f"Failed to clear conversation history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/middleware/rate_limiting.py =====
# app/middleware/rate_limiting.py
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio
from collections import defaultdict
import time
import logging

logger = logging.getLogger(__name__)

class RateLimitingMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_requests=1000, time_window=60):
        super().__init__(app)
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_counts = defaultdict(lambda: {'count': 0, 'window_start': time.time()})
        self.lock = asyncio.Lock()

    async def dispatch(self, request: Request, call_next):
        client_ip = request.client.host if request.client else "unknown"
        
        async with self.lock:
            current_time = time.time()
            record = self.request_counts[client_ip]
            
            # Reset counter if time window has passed
            if current_time - record['window_start'] > self.time_window:
                record['count'] = 0
                record['window_start'] = current_time
            
            # Check rate limit
            if record['count'] >= self.max_requests:
                logger.warning(f"Rate limit exceeded for {client_ip}")
                return JSONResponse(
                    status_code=429,
                    content={"detail": "Too many requests"},
                    headers={"Retry-After": str(self.time_window)}
                )
            
            record['count'] += 1
        
        response = await call_next(request)
        return response

===== ./app/middleware/logging_middleware.py =====
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response
import logging
from typing import Awaitable, Callable

logger = logging.getLogger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(
        self, 
        request: Request, 
        call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        # Log incoming request
        logger.info(f"Incoming request: {request.method} {request.url}")
        
        try:
            response = await call_next(request)
            # Log successful response
            logger.info(
                f"Request completed: {request.method} {request.url} "
                f"- Status: {response.status_code}"
            )
            return response
        except HTTPException as http_exc:
            # Log HTTP exceptions
            logger.warning(
                f"HTTP Exception: {request.method} {request.url} "
                f"- Status: {http_exc.status_code} - Detail: {http_exc.detail}"
            )
            raise
        except Exception as exc:
            # Log unexpected errors
            logger.error(
                f"Unexpected error: {request.method} {request.url} "
                f"- Error: {str(exc)}",
                exc_info=True
            )
            raise
===== ./app/middleware/rate_limiter.py =====
# middleware/rate_limiter.py

from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio
from collections import defaultdict
import time
import logging

logger = logging.getLogger(__name__)

class RateLimiterMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_requests=1000, time_window=60):
        super().__init__(app)
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_counts = defaultdict(lambda: {'count': 0, 'window_start': time.time()})
        self.lock = asyncio.Lock()

    async def dispatch(self, request: Request, call_next):
        client_ip = request.client.host if request.client else "unknown"
        
        async with self.lock:
            current_time = time.time()
            record = self.request_counts[client_ip]
            
            # Reset counter if time window has passed
            if current_time - record['window_start'] > self.time_window:
                record['count'] = 0
                record['window_start'] = current_time
            
            # Check rate limit
            if record['count'] >= self.max_requests:
                logger.warning(f"Rate limit exceeded for {client_ip}")
                return JSONResponse(
                    status_code=429,
                    content={"detail": "Too many requests"},
                    headers={"Retry-After": str(self.time_window)}
                )
            
            record['count'] += 1
        
        response = await call_next(request)
        return response
===== ./app/middleware/__init__.py =====
# ./app/middleware/__init__.py
from .logging_middleware import LoggingMiddleware
from .rate_limiter import RateLimiterMiddleware

__all__ = ["LoggingMiddleware", "RateLimiterMiddleware"]
===== ./app/middleware/errorHandlingMiddleware.py =====
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import logging
import traceback
import time
from typing import Callable

logger = logging.getLogger(__name__)

class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        start_time = time.time()
        
        try:
            response = await call_next(request)
            return response
            
        except HTTPException as e:
            # Let FastAPI handle HTTP exceptions normally
            raise e
            
        except Exception as e:
            # Log the full error with traceback
            process_time = time.time() - start_time
            
            error_details = {
                "method": request.method,
                "url": str(request.url),
                "client": getattr(request.client, "host", "unknown") if request.client else "unknown",
                "process_time": f"{process_time:.3f}s",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            
            logger.error(
                f"Unhandled exception in {request.method} {request.url.path}: {str(e)}",
                extra=error_details
            )
            
            # Return a generic error response
            return JSONResponse(
                status_code=500,
                content={
                    "detail": "Internal server error",
                    "error_type": type(e).__name__,
                    "timestamp": time.time(),
                    "path": request.url.path
                }
            )
===== ./app/middleware/metrics_middleware.py =====
from prometheus_client import Counter, Histogram, make_asgi_app, REGISTRY
from fastapi import Request, Response
from app.config import settings
import time

# Only initialize metrics once
_metrics_initialized = False
REQUEST_COUNT = None
REQUEST_LATENCY = None
metrics_app = None

def initialize_metrics():
    global _metrics_initialized, REQUEST_COUNT, REQUEST_LATENCY, metrics_app
    
    if _metrics_initialized or not settings.PROMETHEUS_ENABLED:
        return
    
    try:
        # Clear existing metrics to avoid conflicts
        collectors = list(REGISTRY._collector_to_names.keys())
        for collector in collectors:
            try:
                REGISTRY.unregister(collector)
            except KeyError:
                pass
        
        REQUEST_COUNT = Counter(
            "http_requests_total",
            "Total HTTP Requests",
            ["method", "path", "status_code"]
        )
        
        REQUEST_LATENCY = Histogram(
            "http_request_duration_seconds",
            "HTTP Request Latency",
            ["method", "path"]
        )
        
        metrics_app = make_asgi_app()
        _metrics_initialized = True
        
    except Exception as e:
        print(f"Failed to initialize metrics: {e}")

# Initialize metrics
initialize_metrics()

async def metrics_middleware(request: Request, call_next):
    if request.url.path == "/metrics" and metrics_app:
        return await metrics_app(request.scope, request.receive, request.send)
        
    if not settings.PROMETHEUS_ENABLED or not REQUEST_COUNT:
        return await call_next(request)
        
    start_time = time.time()
    method = request.method
    path = request.url.path
    
    try:
        response = await call_next(request)
        status_code = response.status_code
    except Exception:
        status_code = 500
        raise
    finally:
        duration = time.time() - start_time
        REQUEST_COUNT.labels(method, path, status_code).inc()
        REQUEST_LATENCY.labels(method, path).observe(duration)
            
    return response

===== ./app/config.py =====
# config.py

from pydantic_settings import BaseSettings
from typing import List, Optional, Dict, Any
from pydantic import Field, PostgresDsn, validator, RedisDsn, HttpUrl, conint
import logging
from urllib.parse import urlparse
import os

class Settings(BaseSettings):
    # App settings
    APP_VERSION: str = "1.0.0"
    APP_NAME: str = "AI Agent Platform"
    DEBUG: bool = False
    ENVIRONMENT: str = "development"
    
    # API settings
    API_PREFIX: str = "/api/v1"
    CORS_ORIGINS: List[str] = ["*"]
    
    # Database
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_SERVER: str = "localhost"
    POSTGRES_PORT: int = 5432
    POSTGRES_DB: str = "llm_agents"
    DATABASE_URL: Optional[PostgresDsn] = None
    


    

    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
    
    # Fix: Don't add leading slash to database name
        db_name = values.get("POSTGRES_DB", "")
    
        return str(PostgresDsn.build(
            scheme="postgresql+asyncpg",
            username=values.get("POSTGRES_USER"),
            password=values.get("POSTGRES_PASSWORD"),
            host=values.get("POSTGRES_SERVER"),
            port=values.get("POSTGRES_PORT"),
            path=db_name,  # Remove the leading slash
        ))


    
    # Redis
    REDIS_URL: RedisDsn = "redis://localhost:6379/0"
    CACHE_ENABLED: bool = True
    CACHE_TTL: int = 300
    
    # Ollama Configuration
    OLLAMA_URL: HttpUrl = "http://localhost:11434"
    DEFAULT_OLLAMA_MODEL: str = "deepseek-r1:1.5b"
    OLLAMA_TIMEOUT: conint(gt=0) = 60  # Increased timeout
    
    # Qdrant Configuration
    QDRANT_URL: HttpUrl = "http://localhost:6333"
    QDRANT_API_KEY: Optional[str] = None
    QDRANT_MAX_WORKERS: conint(gt=0) = 8
    QDRANT_COLLECTION_NAME: str = "documents"
    QDRANT_BATCH_SIZE: conint(gt=0) = 100
    QDRANT_TIMEOUT: conint(gt=0) = 30

    RATE_LIMITING_ENABLED: bool = True
    RATE_LIMIT_MAX_REQUESTS: int = 1000
    RATE_LIMIT_TIME_WINDOW: int = 60  # seconds

    HEALTH_CHECK_TIMEOUT: int = 5000
    PYTHON_API_TIMEOUT: int = 10000

    # PYTHON_API_URL=http://localhost:8000
    # PYTHON_VOICE_WS_URL=ws://localhost:8000/api/v1/voice/ws


    HYBRID_SEARCH_ENABLED: bool = True
    HYBRID_SPARSE_WEIGHT: float = Field(0.4, ge=0, le=1)
    HYBRID_DENSE_WEIGHT: float = Field(0.6, ge=0, le=1)
    
    # Query Processing
    QUERY_REWRITING_ENABLED: bool = True
    QUERY_EXPANSION_ENABLED: bool = True
    
    # Re-ranking
    RERANKING_ENABLED: bool = True
    RERANKING_MODEL: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
    # Metadata Filtering
    ENABLE_METADATA_FILTERING: bool = True
    DEFAULT_METADATA_FIELDS: List[str] = ["source", "author", "date", "document_type"]
    
    # Embedding Model Configuration
    # Use the same model as default for embeddings initially
    EMBEDDING_MODEL: str = "deepseek-r1:1.5b"  # Changed to use main model
    EXPECTED_EMBEDDING_DIMENSION: Optional[conint(gt=0)] = None
    EMBEDDING_FALLBACK_DIMENSION: conint(gt=0) = 768
    
    # Alternative embedding models to try (in order of preference)
    EMBEDDING_MODEL_ALTERNATIVES: List[str] = [
        "deepseek-r1:1.5b",
        "nomic-embed-text",
        "all-minilm",
        "mxbai-embed-large"
    ]

    # Voice settings
    ENABLE_WHISPER: bool = True
    WHISPER_MODEL: str = "base"  # or "small", "medium", "large"
    ENABLE_TTS: bool = True
    TTS_ENGINE: str = "coqui"  # or "pyttsx3"
    TTS_MODEL: str = "tts_models/en/ljspeech/tacotron2-DDC"
    TTS_VOICE: str = "male"
    TTS_MODEL_ALTERNATIVES: List[str] = [
        "tts_models/en/ljspeech/tacotron2-DDC",    # Most reliable single-speaker
        "tts_models/en/ljspeech/fast_pitch",       # Fast single-speaker
        "tts_models/en/ljspeech/glow-tts",         # Good quality single-speaker
        "tts_models/en/ljspeech/neural_hmm",       # Alternative single-speaker
        "tts_models/en/vctk/vits",                 # Multi-speaker (fallback)
    ]

    TTS_MAX_CHUNK_SIZE: int = 400  # Maximum characters per TTS chunk
    TTS_ENABLE_CHUNKING: bool = True  # Enable text chunking for long responses
    TTS_CHUNK_PAUSE_DURATION: float = 0.3  # Pause between chunks in seconds
    
    # Document Processing
    DEFAULT_CHUNK_SIZE: conint(gt=0) = 1000
    DEFAULT_CHUNK_OVERLAP: conint(ge=0) = 200
    MAX_DOCUMENT_SIZE_MB: conint(gt=0) = 50
    SUPPORTED_FILE_TYPES: List[str] = [".pdf", ".txt", ".docx", ".pptx", ".xlsx"]
    
    # RAG Settings
    RAG_DEFAULT_MAX_RESULTS: conint(gt=0) = 5
    RAG_DEFAULT_MIN_SCORE: float = Field(ge=0, le=1, default=0.3)
    RAG_QUERY_TIMEOUT: conint(gt=0) = 30
    
    # Auth
    JWT_SECRET_KEY: str = "your-secret-key-here"
    JWT_ALGORITHM: str = "HS256"
    JWT_ACCESS_TOKEN_EXPIRE_MINUTES: int = 11440  # 24 hours



    ENABLE_INTERNET_SEARCH: bool = True
    SEARCH_SOURCES: List[str] = ["duckduckgo", "reddit", "hackernews", "wikipedia"]
    MAX_SEARCH_RESULTS_PER_SOURCE: int = 5
    SEARCH_TIMEOUT_SECONDS: int = 15

    # Google Search API (optional - for paid Google Custom Search)
    GOOGLE_SEARCH_API_KEY: Optional[str] = None
    GOOGLE_SEARCH_ENGINE_ID: Optional[str] = None

    # Brave Search API (optional - for Brave Search API)
    BRAVE_SEARCH_API_KEY: Optional[str] = None

    # Rate limiting for search APIs
    SEARCH_RATE_LIMIT_PER_MINUTE: int = 60
    SEARCH_CACHE_TTL_SECONDS: int = 3600  # 1 hour



# WebSocket Configuration
    WEBSOCKET_MAX_CONNECTIONS: int = 100
    WEBSOCKET_TIMEOUT: int = 300
    
    # Monitoring
    PROMETHEUS_ENABLED: bool = True
    PROMETHEUS_PORT: conint(gt=0, le=65535) = 8001
    
    # Logging
    LOG_LEVEL: str = Field(pattern="^(DEBUG|INFO|WARNING|ERROR|CRITICAL)$", default="INFO")
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # Embedding fallback strategy
    USE_DETERMINISTIC_FALLBACK: bool = True
    FALLBACK_EMBEDDING_STRATEGY: str = Field(
        pattern="^(deterministic|hash|features|skip)$", 
        default="deterministic"
    )
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True
        extra = "ignore"

# Create settings instance
settings = Settings()

def validate_config():
    """Validate and log configuration"""
    logger = logging.getLogger(__name__)
    
    try:
        # Validate URLs
        parsed_ollama = urlparse(str(settings.OLLAMA_URL))
        if not all([parsed_ollama.scheme, parsed_ollama.netloc]):
            raise ValueError("Invalid OLLAMA_URL format")
            
        parsed_qdrant = urlparse(str(settings.QDRANT_URL))
        if not all([parsed_qdrant.scheme, parsed_qdrant.netloc]):
            raise ValueError("Invalid QDRANT_URL format")
        
        # Log important config
        logger.info("=== Application Configuration ===")
        logger.info(f"Environment: {settings.ENVIRONMENT}")
        logger.info(f"Debug Mode: {settings.DEBUG}")
        logger.info("")
        logger.info("=== Ollama Configuration ===")
        logger.info(f"Ollama URL: {settings.OLLAMA_URL}")
        logger.info(f"Default Model: {settings.DEFAULT_OLLAMA_MODEL}")
        logger.info(f"Embedding Model: {settings.EMBEDDING_MODEL}")
        logger.info(f"Timeout: {settings.OLLAMA_TIMEOUT}s")
        logger.info(f"Alternative Models: {settings.EMBEDDING_MODEL_ALTERNATIVES}")
        logger.info("")
        logger.info("=== Vector Database Configuration ===")
        logger.info(f"Qdrant URL: {settings.QDRANT_URL}")
        logger.info(f"Collection Name: {settings.QDRANT_COLLECTION_NAME}")
        logger.info(f"Expected Dimension: {settings.EXPECTED_EMBEDDING_DIMENSION or 'Auto-detect'}")
        logger.info(f"Fallback Dimension: {settings.EMBEDDING_FALLBACK_DIMENSION}")
        logger.info("")
        logger.info("=== Document Processing ===")
        logger.info(f"Chunk Size: {settings.DEFAULT_CHUNK_SIZE}")
        logger.info(f"Chunk Overlap: {settings.DEFAULT_CHUNK_OVERLAP}")
        logger.info(f"Max File Size: {settings.MAX_DOCUMENT_SIZE_MB}MB")
        logger.info(f"Supported Types: {settings.SUPPORTED_FILE_TYPES}")
        logger.info("")
        logger.info("=== RAG Configuration ===")
        logger.info(f"Max Results: {settings.RAG_DEFAULT_MAX_RESULTS}")
        logger.info(f"Min Score Threshold: {settings.RAG_DEFAULT_MIN_SCORE}")
        logger.info(f"Query Timeout: {settings.RAG_QUERY_TIMEOUT}s")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {str(e)}")
        raise

# Validate on import (only in production-like environments)
if "pytest" not in os.sys.modules and __name__ != "__main__":
    try:
        validate_config()
    except Exception as e:
        print(f"Warning: Configuration validation failed: {e}")
        # Don't raise in case this is being imported during setup
===== ./app/database.py =====
# app/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Get the database URL as string and ensure it uses asyncpg
database_url = str(settings.DATABASE_URL).replace(
    "postgresql://", 
    "postgresql+asyncpg://"
)

engine = create_async_engine(
    database_url,
    echo=settings.DEBUG,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    connect_args={
        "server_settings": {
            "application_name": settings.APP_NAME,
            "search_path": "llm,public"
        }
    }
)

AsyncSessionLocal = sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False
)

Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception as e:
            logger.error(f"Database error: {str(e)}")
            await session.rollback()
            raise
===== ./app/__init__.py =====
# Initialize application package
from .config import settings

__all__ = ["settings"]
===== ./app/utils/auth.py =====
from datetime import datetime, timedelta
from jose import JWTError, jwt
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from app.config import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/auth/token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode, 
        settings.JWT_SECRET_KEY, 
        algorithm=settings.JWT_ALGORITHM
    )
    return encoded_jwt

# In utils/auth.py
async def verify_token(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            token, 
            settings.JWT_SECRET_KEY, 
            algorithms=[settings.JWT_ALGORITHM]
        )
        # Ensure sub (subject) is present
        if "sub" not in payload:
            raise credentials_exception
        return payload
    except JWTError:
        raise credentials_exception

# Add this function
def get_current_user(token_payload: dict = Depends(verify_token)):
    """
    Dependency that can be used to get the current user from the token
    """
    return token_payload
===== ./app/utils/logging.py =====
import logging
from logging.config import dictConfig
from app.config import settings
import sys

def configure_logging():
    logging_config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": settings.LOG_FORMAT,
                "datefmt": "%Y-%m-%d %H:%M:%S"
            },
        },
        "handlers": {
            "console": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.StreamHandler",
                "stream": sys.stdout,
            },
            "file": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "app.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "encoding": "utf8"
            },
        },
        "loggers": {
            "app": {
                "handlers": ["console", "file"],
                "level": settings.LOG_LEVEL,
                "propagate": False
            },
            "sqlalchemy": {
                "handlers": ["console"],
                "level": "WARNING",
                "propagate": False
            },
        }
    }
    
    dictConfig(logging_config)
    logger = logging.getLogger("app")
    logger.info("Logging configured successfully")
    return logger

logger = configure_logging()
===== ./app/utils/cache_decorator.py =====
from typing import Any, Optional, Callable, Awaitable
from functools import wraps
from app.services.cache import cache_service
import json
import hashlib
import inspect

def cached(key_pattern: str, ttl: Optional[int] = None):
    """Decorator for caching async function results with smart key generation"""
    def decorator(func: Callable[..., Awaitable[Any]]):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not cache_service.enabled:
                return await func(*args, **kwargs)
            
            try:
                # Get function signature to map args to parameter names
                sig = inspect.signature(func)
                bound_args = sig.bind(*args, **kwargs)
                bound_args.apply_defaults()
                
                # Create a dictionary of all parameters
                params = dict(bound_args.arguments)
                
                # Generate query_hash if needed and not provided
                if 'query_hash' in key_pattern and 'query_hash' not in params:
                    query = params.get('query', '')
                    if query:
                        params['query_hash'] = hashlib.md5(query.encode()).hexdigest()[:8]
                    else:
                        params['query_hash'] = 'no_query'
                
                # Format the cache key with available parameters
                try:
                    cache_key = key_pattern.format(**params)
                except KeyError as e:
                    # If formatting fails, create a simple fallback key
                    func_name = func.__name__
                    args_hash = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()[:8]
                    cache_key = f"{func_name}:{args_hash}"
                
                # Try to get from cache
                cached_data = await cache_service.get(cache_key)
                if cached_data is not None:
                    return cached_data
                
                # Execute function and cache result
                result = await func(*args, **kwargs)
                await cache_service.set(cache_key, result, ttl)
                return result
                
            except Exception as e:
                # If caching fails, just execute the function
                return await func(*args, **kwargs)
                
        return wrapper
    return decorator
===== ./app/utils/monitoring.py =====
# ./app/utils/monitoring.py
from prometheus_client import start_http_server, Counter, Histogram, Gauge
from prometheus_client import generate_latest, REGISTRY
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP Requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP Request Latency',
    ['method', 'endpoint']
)

AGENT_EXECUTIONS = Counter(
    'agent_executions_total',
    'Total Agent Executions',
    ['agent_id', 'status']
)

RAG_REQUESTS = Counter(
    'rag_requests_total',
    'Total RAG Requests',
    ['status']
)

async def initialize():
    if settings.PROMETHEUS_ENABLED:
        start_http_server(settings.PROMETHEUS_PORT)
        logger.info(f"Metrics server started on port {settings.PROMETHEUS_PORT}")

def get_metrics():
    return generate_latest(REGISTRY)

def record_request(method: str, endpoint: str, status_code: int, duration: float):
    REQUEST_COUNT.labels(method, endpoint, status_code).inc()
    REQUEST_LATENCY.labels(method, endpoint).observe(duration)

def record_agent_execution(agent_id: str, success: bool):
    status = "success" if success else "failure"
    AGENT_EXECUTIONS.labels(agent_id, status).inc()

def record_rag_request(success: bool):
    status = "success" if success else "failure"
    RAG_REQUESTS.labels(status).inc()
===== ./app/utils/tts_model_manager.py =====
# app/utils/tts_model_manager.py
import logging
import numpy as np
from typing import Optional, Dict, Any, List
from TTS.api import TTS

logger = logging.getLogger(__name__)

class TTSModelManager:
    """Utility class to manage TTS model loading and validation"""
    
    RELIABLE_MODELS = [
        "tts_models/en/ljspeech/tacotron2-DDC",    # Most reliable single-speaker
        "tts_models/en/ljspeech/fast_pitch",       # Fast single-speaker
        "tts_models/en/ljspeech/glow-tts",         # Good quality single-speaker
        "tts_models/en/ljspeech/neural_hmm",       # Alternative single-speaker
    ]
    
    MULTI_SPEAKER_MODELS = [
        "tts_models/en/vctk/vits",                 # Multi-speaker
        "tts_models/multilingual/multi-dataset/your_tts",  # Multi-lingual multi-speaker
    ]
    
    @staticmethod
    def load_best_available_model(preferred_models: List[str] = None) -> Optional[TTS]:
        """Load the best available TTS model"""
        models_to_try = preferred_models or (TTSModelManager.RELIABLE_MODELS + TTSModelManager.MULTI_SPEAKER_MODELS)
        
        for model_name in models_to_try:
            try:
                logger.info(f"Attempting to load TTS model: {model_name}")
                tts = TTS(model_name=model_name)
                
                if TTSModelManager.validate_model(tts, model_name):
                    logger.info(f"‚úÖ Successfully loaded and validated: {model_name}")
                    return tts
                else:
                    logger.warning(f"Model {model_name} failed validation")
                    
            except Exception as e:
                logger.warning(f"Failed to load {model_name}: {str(e)}")
                continue
        
        logger.error("No TTS models could be loaded")
        return None
    
    @staticmethod
    def validate_model(tts: TTS, model_name: str) -> bool:
        """Validate that a TTS model works correctly"""
        try:
            test_text = "Hello world"
            is_multi_speaker = hasattr(tts, 'speakers') and tts.speakers and len(tts.speakers) > 0
            
            # Generate test audio
            if is_multi_speaker:
                speaker = tts.speakers[0]
                wav_data = tts.tts(text=test_text, speaker=speaker)
            else:
                wav_data = tts.tts(text=test_text)
            
            # Validate output
            return TTSModelManager._validate_audio_output(wav_data, model_name)
            
        except Exception as e:
            logger.error(f"Model validation failed for {model_name}: {str(e)}")
            return False
    
    @staticmethod
    def _validate_audio_output(wav_data: Any, model_name: str) -> bool:
        """Validate the audio output from TTS"""
        try:
            # Handle list of float32 values (common Coqui output)
            if isinstance(wav_data, list):
                if len(wav_data) == 0:
                    return False
                
                # Check if it's a list of audio samples
                if isinstance(wav_data[0], (np.float32, np.float64, float)):
                    # Convert to numpy array
                    audio_array = np.array(wav_data, dtype=np.float32)
                    return audio_array.size > 0
                
                # Check if it's a list of numpy arrays
                elif isinstance(wav_data[0], np.ndarray):
                    return all(arr.size > 0 for arr in wav_data)
                
                # Check if it's a list of sentences (needs rejoining)
                elif isinstance(wav_data[0], str):
                    return True  # Valid but needs special handling
                
            # Handle numpy arrays
            elif isinstance(wav_data, np.ndarray):
                return wav_data.size > 0 and wav_data.ndim >= 1
            
            # Handle bytes
            elif isinstance(wav_data, (bytes, bytearray)):
                return len(wav_data) > 0
            
            return False
            
        except Exception as e:
            logger.error(f"Audio validation error: {str(e)}")
            return False
    
    @staticmethod  
    def convert_to_numpy_array(wav_data: Any) -> np.ndarray:
        """Convert various TTS output formats to numpy array"""
        if isinstance(wav_data, list):
            if len(wav_data) == 0:
                raise ValueError("Empty audio data")
            
            # Handle list of float32 values
            if isinstance(wav_data[0], (np.float32, np.float64, float)):
                return np.array(wav_data, dtype=np.float32)
            
            # Handle list of numpy arrays
            elif isinstance(wav_data[0], np.ndarray):
                valid_arrays = [arr for arr in wav_data if arr.size > 0]
                if not valid_arrays:
                    raise ValueError("No valid audio arrays")
                return np.concatenate(valid_arrays)
            
            else:
                raise ValueError(f"Unsupported list content type: {type(wav_data[0])}")
        
        elif isinstance(wav_data, np.ndarray):
            if wav_data.size == 0:
                raise ValueError("Empty numpy array")
            if wav_data.ndim > 1:
                wav_data = wav_data.flatten()
            return wav_data.astype(np.float32)
        
        else:
            raise ValueError(f"Unsupported audio data type: {type(wav_data)}")

===== ./app/utils/performance_monitor.py =====
# app/utils/performance_monitor.py - Real-time performance monitoring
import time
import asyncio
import logging
from typing import Dict, Any, List, Optional
from collections import defaultdict, deque
from dataclasses import dataclass
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

@dataclass
class PerformanceTarget:
    stt_max: float = 300  # ms
    llm_max: float = 500  # ms
    tts_max: float = 400  # ms
    total_max: float = 1200  # ms

class PerformanceMonitor:
    def __init__(self, max_history_size: int = 10000):
        self.max_history_size = max_history_size
        self.targets = PerformanceTarget()
        
        # Performance metrics storage
        self.metrics = {
            "response_times": deque(maxlen=max_history_size),
            "stt_times": deque(maxlen=max_history_size),
            "llm_times": deque(maxlen=max_history_size),
            "tts_times": deque(maxlen=max_history_size),
            "success_count": 0,
            "failure_count": 0,
            "total_requests": 0
        }
        
        # Real-time averages
        self.current_averages = {
            "response_time": 0.0,
            "stt_time": 0.0,
            "llm_time": 0.0,
            "tts_time": 0.0,
            "success_rate": 100.0
        }
        
        # Target achievement tracking
        self.target_achievement = {
            "stt": 0.0,
            "llm": 0.0,
            "tts": 0.0,
            "total": 0.0
        }
        
        # Alert thresholds
        self.alert_thresholds = {
            "response_time_critical": 2000,  # ms
            "error_rate_critical": 10,      # %
            "target_miss_critical": 50      # %
        }
        
        # Performance windows for analysis
        self.performance_windows = {
            "1m": {"size": 60, "data": deque(maxlen=60)},
            "5m": {"size": 300, "data": deque(maxlen=300)},
            "15m": {"size": 900, "data": deque(maxlen=900)}
        }
        
        # Active alerts
        self.active_alerts = {}
        self.alert_history = deque(maxlen=100)
        
        logger.info("Ultra-fast performance monitor initialized")

    def record_voice_processing(self, request_id: str, processing_times: Dict[str, float], 
                               success: bool = True, metadata: Optional[Dict] = None):
        """Record voice processing performance metrics"""
        try:
            timestamp = time.time()
            total_time = processing_times.get("total", 0) * 1000  # Convert to ms
            stt_time = processing_times.get("stt", 0) * 1000
            llm_time = processing_times.get("llm", 0) * 1000
            tts_time = processing_times.get("tts", 0) * 1000
            
            # Store raw metrics
            self.metrics["response_times"].append(total_time)
            self.metrics["stt_times"].append(stt_time)
            self.metrics["llm_times"].append(llm_time)
            self.metrics["tts_times"].append(tts_time)
            
            # Update counters
            self.metrics["total_requests"] += 1
            if success:
                self.metrics["success_count"] += 1
            else:
                self.metrics["failure_count"] += 1
            
            # Update real-time averages
            self._update_averages()
            
            # Update target achievement rates
            self._update_target_achievement(stt_time, llm_time, tts_time, total_time)
            
            # Add to performance windows
            self._update_performance_windows(timestamp, total_time, success, metadata)
            
            # Check for performance alerts
            self._check_performance_alerts(total_time, stt_time, llm_time, tts_time, success)
            
            # Log performance warnings
            if total_time > self.targets.total_max:
                logger.warning(f"Response time exceeded target: {total_time:.0f}ms > {self.targets.total_max}ms")
            
            logger.debug(f"Performance recorded: {request_id} - {total_time:.0f}ms total")
            
        except Exception as e:
            logger.error(f"Error recording performance metrics: {str(e)}")

    def _update_averages(self):
        """Update rolling averages efficiently"""
        try:
            if self.metrics["response_times"]:
                self.current_averages["response_time"] = sum(self.metrics["response_times"]) / len(self.metrics["response_times"])
            
            if self.metrics["stt_times"]:
                self.current_averages["stt_time"] = sum(self.metrics["stt_times"]) / len(self.metrics["stt_times"])
            
            if self.metrics["llm_times"]:
                self.current_averages["llm_time"] = sum(self.metrics["llm_times"]) / len(self.metrics["llm_times"])
            
            if self.metrics["tts_times"]:
                self.current_averages["tts_time"] = sum(self.metrics["tts_times"]) / len(self.metrics["tts_times"])
            
            if self.metrics["total_requests"] > 0:
                self.current_averages["success_rate"] = (self.metrics["success_count"] / self.metrics["total_requests"]) * 100
                
        except Exception as e:
            logger.error(f"Error updating averages: {str(e)}")

    def _update_target_achievement(self, stt_time: float, llm_time: float, tts_time: float, total_time: float):
        """Update target achievement rates"""
        try:
            # Calculate if targets were met
            targets_met = {
                "stt": stt_time <= self.targets.stt_max,
                "llm": llm_time <= self.targets.llm_max,
                "tts": tts_time <= self.targets.tts_max,
                "total": total_time <= self.targets.total_max
            }
            
            # Update rolling achievement rates (exponential moving average)
            alpha = 0.1  # Smoothing factor
            for component, met in targets_met.items():
                current_rate = self.target_achievement[component]
                self.target_achievement[component] = (1 - alpha) * current_rate + alpha * (1.0 if met else 0.0)
                
        except Exception as e:
            logger.error(f"Error updating target achievement: {str(e)}")

    def _update_performance_windows(self, timestamp: float, total_time: float, success: bool, metadata: Optional[Dict]):
        """Update sliding window performance data"""
        try:
            data_point = {
                "timestamp": timestamp,
                "response_time": total_time,
                "success": success,
                "metadata": metadata or {}
            }
            
            for window_name, window_data in self.performance_windows.items():
                window_data["data"].append(data_point)
                
        except Exception as e:
            logger.error(f"Error updating performance windows: {str(e)}")

    def _check_performance_alerts(self, total_time: float, stt_time: float, llm_time: float, tts_time: float, success: bool):
        """Check for performance alerts and trigger notifications"""
        try:
            current_time = time.time()
            
            # Critical response time alert
            if total_time > self.alert_thresholds["response_time_critical"]:
                self._trigger_alert("critical_response_time", {
                    "response_time": total_time,
                    "threshold": self.alert_thresholds["response_time_critical"],
                    "severity": "critical"
                })
            
            # Component performance alerts
            component_times = {
                "stt": (stt_time, self.targets.stt_max),
                "llm": (llm_time, self.targets.llm_max),
                "tts": (tts_time, self.targets.tts_max)
            }
            
            for component, (actual_time, target_time) in component_times.items():
                if actual_time > target_time * 1.5:  # 50% over target
                    self._trigger_alert(f"{component}_performance_degraded", {
                        "component": component,
                        "actual_time": actual_time,
                        "target_time": target_time,
                        "severity": "warning"
                    })
            
            # Target achievement rate alerts
            for component, achievement_rate in self.target_achievement.items():
                if achievement_rate < 0.5:  # Less than 50% achievement
                    self._trigger_alert(f"{component}_target_missed", {
                        "component": component,
                        "achievement_rate": achievement_rate * 100,
                        "severity": "warning"
                    })
            
            # Error rate alert
            if self.metrics["total_requests"] >= 10:  # Only check after minimum requests
                error_rate = (self.metrics["failure_count"] / self.metrics["total_requests"]) * 100
                if error_rate > self.alert_thresholds["error_rate_critical"]:
                    self._trigger_alert("high_error_rate", {
                        "error_rate": error_rate,
                        "threshold": self.alert_thresholds["error_rate_critical"],
                        "severity": "critical"
                    })
                    
        except Exception as e:
            logger.error(f"Error checking performance alerts: {str(e)}")

    def _trigger_alert(self, alert_type: str, details: Dict[str, Any]):
        """Trigger performance alert"""
        try:
            current_time = time.time()
            alert_key = f"{alert_type}_{details.get('component', 'system')}"
            
            # Prevent alert spam (5 minute cooldown)
            if alert_key in self.active_alerts:
                last_alert_time = self.active_alerts[alert_key]["timestamp"]
                if current_time - last_alert_time < 300:  # 5 minutes
                    return
            
            alert = {
                "type": alert_type,
                "details": details,
                "timestamp": current_time,
                "severity": details.get("severity", "info")
            }
            
            # Store active alert
            self.active_alerts[alert_key] = alert
            
            # Add to history
            self.alert_history.append(alert)
            
            # Log alert
            severity = details.get("severity", "info")
            log_level = logging.ERROR if severity == "critical" else logging.WARNING
            logger.log(log_level, f"Performance Alert [{alert_type}]: {details}")
            
        except Exception as e:
            logger.error(f"Error triggering alert: {str(e)}")

    def get_current_performance(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        try:
            return {
                "averages": self.current_averages.copy(),
                "target_achievement": {k: v * 100 for k, v in self.target_achievement.items()},
                "total_requests": self.metrics["total_requests"],
                "success_count": self.metrics["success_count"],
                "failure_count": self.metrics["failure_count"],
                "active_alerts": len(self.active_alerts),
                "timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"Error getting current performance: {str(e)}")
            return {}

    def get_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        try:
            current_time = time.time()

        # Compute current performance averages
            averages = self.current_averages.copy()
        
        # Convert achievement rates to percentages
            target_rates = {k: v * 100 for k, v in self.target_achievement.items()}

        # Build sliding-window summaries
            window_stats = {}
            for name, window in self.performance_windows.items():
                data = window["data"]
                if data:
                    latencies = [d["response_time"] for d in data]
                    window_stats[name] = {
                        "count": len(latencies),
                        "p50": np.percentile(latencies, 50),
                        "p95": np.percentile(latencies, 95),
                        "p99": np.percentile(latencies, 99),
                    }
                else:
                    window_stats[name] = {"count": 0, "p50": 0, "p95": 0, "p99": 0}

            return {
                "timestamp": current_time,
                "averages": averages,
                "target_achievement_percent": target_rates,
                "total_requests": self.metrics["total_requests"],
                "success_rate": self.current_averages["success_rate"],
                "windowed_statistics": window_stats,
                "active_alerts": len(self.active_alerts),
                "alert_history_last_5": list(self.alert_history)[-5:]
            }

        except Exception as e:
            logger.error(f"Error generating performance report: {e}", exc_info=True)
        # Return an empty or minimal safe report on failure
            return {
                "timestamp": time.time(),
                "error": str(e),
                "averages": {},
                "target_achievement_percent": {},
                "total_requests": 0,
                "success_rate": 0.0,
                "windowed_statistics": {},
                "active_alerts": 0,
                "alert_history_last_5": []
            }


===== ./app/utils/__init__.py =====
# Empty file to make utils a package
===== ./app/utils/chroma_async.py =====
# utils/chroma_async.py
import chromadb
import logging
from typing import List, Dict, Any, Optional, Callable, AsyncIterator
from concurrent.futures import ThreadPoolExecutor
import asyncio
from functools import partial
from contextlib import asynccontextmanager
import time
import threading
from collections import defaultdict
from app.config import settings

logger = logging.getLogger(__name__)

class AsyncChromaClient:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, path: str, embedding_function: Optional[Callable] = None, max_workers: int = 4):
        if not hasattr(self, '_initialized'):
            self.path = path
            self.embedding_function = embedding_function
            self.executor = ThreadPoolExecutor(
                max_workers=max_workers, 
                thread_name_prefix="chroma",
            )
            self._local = threading.local()
            self.collections = defaultdict(dict)
            self._initialized = True
            self._closed = False
    
    async def initialize(self):
        """Initialize the client in a thread-safe way"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            loop = asyncio.get_running_loop()
            self._local.client = await loop.run_in_executor(
                self.executor,
                partial(
                    chromadb.PersistentClient,
                    path=self.path,
                )
            )
            logger.info("AsyncChromaDB client initialized for thread %s", threading.current_thread().name)
    
    @asynccontextmanager
    async def get_collection(self, name: str, embedding_function=None, metadata=None) -> AsyncIterator[chromadb.Collection]:
        """Async context manager for collection access with connection pooling"""
        if self._closed:
            raise RuntimeError("Client is closed")
            
        await self.initialize()
        
        thread_id = threading.current_thread().ident
        collection_key = f"{name}_{thread_id}"
        
        if collection_key not in self.collections[thread_id]:
            loop = asyncio.get_running_loop()
            
            # Fix: Don't pass metadata parameter if it's None or empty
            collection_kwargs = {
                'name': name,
                'embedding_function': embedding_function or self.embedding_function,
            }
            
            # Only add metadata if it's not None and not empty
            if metadata:
                collection_kwargs['metadata'] = metadata
                
            collection = await loop.run_in_executor(
                self.executor,
                partial(
                    self._local.client.get_or_create_collection,
                    **collection_kwargs
                )
            )
            self.collections[thread_id][collection_key] = collection
        
        try:
            yield self.collections[thread_id][collection_key]
        except Exception as e:
            logger.error(f"Error in collection context for {name}: {str(e)}")
            raise
        finally:
            # Clean up if this is the last reference
            pass
    
    async def add_documents(
        self, 
        collection_name: str, 
        documents: List[str], 
        metadatas: List[Dict], 
        ids: List[str],
        embeddings: Optional[List[List[float]]] = None,
        batch_size: int = 100
    ):
        """Async document addition with improved batching and error handling"""
        if not documents:
            return
            
        async with self.get_collection(collection_name) as collection:
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i + batch_size]
                batch_metas = metadatas[i:i + batch_size] if metadatas else None
                batch_ids = ids[i:i + batch_size] if ids else None
                batch_embeds = embeddings[i:i + batch_size] if embeddings else None
                
                loop = asyncio.get_running_loop()
                
                try:
                    await loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.add,
                            documents=batch_docs,
                            metadatas=batch_metas,
                            ids=batch_ids,
                            embeddings=batch_embeds
                        )
                    )
                    
                    # Small delay between batches to prevent overwhelming the system
                    if i + batch_size < len(documents):
                        await asyncio.sleep(0.05)
                        
                except Exception as e:
                    logger.error(f"Failed to add batch {i//batch_size + 1}: {str(e)}")
                    raise
    
    async def query_collection(
        self, 
        collection_name: str, 
        query_texts: Optional[List[str]] = None, 
        query_embeddings: Optional[List[List[float]]] = None,
        n_results: int = 5, 
        where: Optional[Dict] = None, 
        where_document: Optional[Dict] = None,
        include: Optional[List[str]] = None,
        timeout: float = 30.0
    ) -> Dict[str, Any]:
        """Async query with improved error handling and timeout"""
        if not query_texts and not query_embeddings:
            raise ValueError("Either query_texts or query_embeddings must be provided")
            
        async with self.get_collection(collection_name) as collection:
            loop = asyncio.get_running_loop()
            
            try:
                return await asyncio.wait_for(
                    loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.query,
                            query_texts=query_texts,
                            query_embeddings=query_embeddings,
                            n_results=n_results,
                            where=where,
                            where_document=where_document,
                            include=include or ["documents", "distances", "metadatas"]
                        )
                    ),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.error(f"Query timeout for collection {collection_name}")
                raise
            except Exception as e:
                logger.error(f"Query failed for collection {collection_name}: {str(e)}")
                raise
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        try:
            # Clear collections cache
            self.collections.clear()
            
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
                logger.info("AsyncChromaDB client closed")
                
        except Exception as e:
            logger.error(f"Error closing AsyncChromaDB client: {str(e)}")
        finally:
            self._instance = None

# Global instance with lazy initialization
_async_chroma_client = None
_async_chroma_lock = asyncio.Lock()

async def get_async_chroma_client() -> AsyncChromaClient:
    global _async_chroma_client
    async with _async_chroma_lock:
        if _async_chroma_client is None or _async_chroma_client._closed:
            _async_chroma_client = AsyncChromaClient(
                path=settings.CHROMA_PATH,
                embedding_function=None,  # Will be set per request
                max_workers=settings.CHROMA_MAX_WORKERS
            )
        return _async_chroma_client
===== ./app/utils/file_processing_training.py =====
import os
import logging
import requests
from typing import Optional, List
from io import BytesIO
import tempfile
from pathlib import Path

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup
import json
import csv

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> Optional[str]:
    """Extract text from PDF content"""
    try:
        pdf_file = BytesIO(content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        return None

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        from app.models.training_model import SUPPORTED_EXTENSIONS
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }
===== ./app/utils/file_processing.py =====
import os
import logging
import requests
from typing import Optional, List, Tuple, Dict, Any
from io import BytesIO
import tempfile
from pathlib import Path
import fitz  # PyMuPDF
import re
import json
import csv

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = os.path.splitext(filename.lower())[1]
    return ext in SUPPORTED_EXTENSIONS

def get_file_type(filename: str) -> str:
    """Determine the file type based on extension"""
    ext = os.path.splitext(filename.lower())[1]
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return 'image'
    elif ext in {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}:
        return 'audio'
    elif ext in {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'}:
        return 'video'
    else:
        return 'document'

def clean_text(text: str) -> str:
    """Clean text by removing excessive whitespace"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> str:
    """Extract text from PDF bytes using PyMuPDF"""
    try:
        with fitz.open(stream=content, filetype="pdf") as doc:
            text = "\n".join([page.get_text() for page in doc])
        return clean_text(text)
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {e}")
        return ""

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }

    
===== ./app/utils/advanced_vad.py =====
# app/utils/advanced_vad.py - Advanced Voice Activity Detection
import logging
import time
import numpy as np
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class AdvancedVAD:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {
            'silenceThreshold': 0.01,
            'speechThreshold': 0.03,
            'minSilenceDuration': 800,
            'minSpeechDuration': 200,
            'maxRecordingTime': 15000,
            'vadSensitivity': 0.7,
            'endpointDetection': True
        }
        
        # State tracking
        self.current_state = 'silence'
        self.speech_start_time = 0
        self.silence_start_time = 0
        self.energy_history = []
        self.background_noise = 0.001
        
        # Performance metrics
        self.processing_times = []
        self.detection_accuracy = 0.95
        
        logger.info("Advanced VAD initialized")

    def process_audio_frame(self, audio_data: np.ndarray, timestamp: float = None) -> Dict[str, Any]:
        """Process audio frame and detect voice activity"""
        start_time = time.time()
        
        if timestamp is None:
            timestamp = time.time() * 1000  # Convert to milliseconds
        
        try:
            # Calculate energy
            energy = self._calculate_energy(audio_data)
            
            # Update energy history
            self._update_energy_history(energy)
            
            # Voice activity detection
            result = self._detect_voice_activity(energy, timestamp)
            
            # Record processing time
            processing_time = (time.time() - start_time) * 1000
            self.processing_times.append(processing_time)
            
            # Keep processing times history manageable
            if len(self.processing_times) > 100:
                self.processing_times.pop(0)
            
            return {
                'is_voice_active': result['is_voice_active'],
                'energy': energy,
                'confidence': result['confidence'],
                'state': self.current_state,
                'processing_time': processing_time
            }
            
        except Exception as e:
            logger.error(f"VAD processing error: {str(e)}")
            return {
                'is_voice_active': False,
                'energy': 0.0,
                'confidence': 0.0,
                'state': 'error',
                'processing_time': 0.0
            }

    def _calculate_energy(self, audio_data: np.ndarray) -> float:
        """Calculate RMS energy of audio frame"""
        try:
            if len(audio_data) == 0:
                return 0.0
            
            # Convert to numpy array if needed
            if not isinstance(audio_data, np.ndarray):
                audio_data = np.array(audio_data, dtype=np.float32)
            
            # Calculate RMS energy
            rms = np.sqrt(np.mean(audio_data ** 2))
            
            # Normalize and clamp
            energy = min(max(rms * 10, 0.0), 1.0)
            
            return energy
            
        except Exception as e:
            logger.error(f"Energy calculation error: {str(e)}")
            return 0.0

    def _update_energy_history(self, energy: float):
        """Update energy history for adaptive processing"""
        self.energy_history.append(energy)
        
        # Keep history size manageable
        if len(self.energy_history) > 50:
            self.energy_history.pop(0)
        
        # Update background noise estimate
        if len(self.energy_history) >= 10:
            sorted_history = sorted(self.energy_history)
            self.background_noise = sorted_history[len(sorted_history) // 3]

    def _detect_voice_activity(self, energy: float, timestamp: float) -> Dict[str, Any]:
        """Main voice activity detection logic"""
        speech_threshold = self.config['speechThreshold']
        silence_threshold = self.config['silenceThreshold']
        min_speech_duration = self.config['minSpeechDuration']
        min_silence_duration = self.config['minSilenceDuration']
        
        is_voice_active = False
        confidence = 0.0
        
        # Adaptive thresholds based on background noise
        adaptive_speech_threshold = max(speech_threshold, self.background_noise * 3)
        adaptive_silence_threshold = max(silence_threshold, self.background_noise * 1.5)
        
        if self.current_state == 'silence':
            if energy > adaptive_speech_threshold:
                if self.speech_start_time == 0:
                    self.speech_start_time = timestamp
                elif timestamp - self.speech_start_time >= min_speech_duration:
                    self.current_state = 'speech'
                    is_voice_active = True
                    confidence = min((energy - adaptive_speech_threshold) / adaptive_speech_threshold, 1.0)
            else:
                self.speech_start_time = 0
                
        elif self.current_state == 'speech':
            if energy < adaptive_silence_threshold:
                if self.silence_start_time == 0:
                    self.silence_start_time = timestamp
                elif timestamp - self.silence_start_time >= min_silence_duration:
                    self.current_state = 'silence'
                    is_voice_active = False
                    confidence = 0.0
                else:
                    # Still in speech state
                    is_voice_active = True
                    confidence = min(energy / adaptive_speech_threshold, 1.0)
            else:
                # Continue speech
                self.silence_start_time = 0
                is_voice_active = True
                confidence = min(energy / adaptive_speech_threshold, 1.0)
        
        return {
            'is_voice_active': is_voice_active,
            'confidence': confidence
        }

    def optimize_for_speed(self):
        """Optimize VAD for speed over accuracy"""
        self.config.update({
            'minSilenceDuration': 600,
            'minSpeechDuration': 150,
            'vadSensitivity': 0.8
        })
        logger.info("VAD optimized for speed")

    def optimize_for_accuracy(self):
        """Optimize VAD for accuracy over speed"""
        self.config.update({
            'minSilenceDuration': 1000,
            'minSpeechDuration': 300,
            'vadSensitivity': 0.6
        })
        logger.info("VAD optimized for accuracy")

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics"""
        if not self.processing_times:
            return {
                'average_processing_time': 0.0,
                'detection_accuracy': self.detection_accuracy,
                'total_frames': 0
            }
        
        avg_time = sum(self.processing_times) / len(self.processing_times)
        
        return {
            'average_processing_time': avg_time,
            'detection_accuracy': self.detection_accuracy,
            'total_frames': len(self.processing_times),
            'config': self.config.copy(),
            'current_state': self.current_state,
            'background_noise': self.background_noise
        }

    def reset(self):
        """Reset VAD state"""
        self.current_state = 'silence'
        self.speech_start_time = 0
        self.silence_start_time = 0
        self.energy_history = []
        self.processing_times = []
        logger.info("VAD state reset")

    async def cleanup(self):
        """Cleanup resources"""
        try:
            self.reset()
            logger.info("VAD cleanup completed")
        except Exception as e:
            logger.error(f"VAD cleanup error: {e}")

===== ./app/utils/health_check.py =====
from fastapi import APIRouter, status
from fastapi.responses import JSONResponse
from app.config import settings
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/health", tags=["Health Check"])
async def health_check():
    """Basic health check endpoint"""
    try:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "healthy",
                "version": settings.APP_VERSION,
                "environment": settings.ENVIRONMENT
            }
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"status": "unhealthy", "error": str(e)}
        )
===== ./app/utils/helpers.py =====
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Convert Pydantic PostgresDsn to string and ensure proper format
def get_database_url() -> str:
    url = str(settings.DATABASE_URL)
    # Ensure there's no double slash before the database name
    if '//' in url.split('@')[-1]:
        parts = url.split('@')
        parts[-1] = parts[-1].replace('//', '/')
        url = '@'.join(parts)
    return url

# Create engine with properly formatted URL string
engine = create_async_engine(
    get_database_url(),
    future=True,
    echo=True
)

AsyncSessionLocal = sessionmaker(
    engine, 
    class_=AsyncSession,
    expire_on_commit=False
)

Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as db:
        yield db
===== ./app/utils/qdrant_async.py =====
import logging
import asyncio
import threading
from typing import List, Dict, Any, Optional, Tuple, Union
from contextlib import asynccontextmanager
from functools import partial
from concurrent.futures import ThreadPoolExecutor
import uuid
import time
from dataclasses import dataclass

from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse, ResponseHandlingException
from qdrant_client.http.models import (
    Distance, 
    VectorParams, 
    PointStruct, 
    Filter, 
    FieldCondition, 
    MatchValue,
    MatchAny,
    Range
)
from app.config import settings # Ensure this import path is correct based on your project structure

logger = logging.getLogger(__name__)

class QdrantConnectionError(Exception):
    """Custom exception for Qdrant connection issues"""
    pass

class QdrantOperationError(Exception):
    """Custom exception for Qdrant operation failures"""
    pass

@dataclass
class CollectionInfo:
    name: str
    status: str
    vector_size: int
    distance: str
    points_count: int

class AsyncQdrantClient:
    """Thread-safe async wrapper for Qdrant client with improved error handling"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, url: str = None, api_key: str = None, max_workers: int = None):
        if self._initialized:
            return
            
        self.url = url or str(settings.QDRANT_URL)
        self.api_key = api_key or settings.QDRANT_API_KEY
        self.max_workers = max_workers or settings.QDRANT_MAX_WORKERS
        self.timeout = settings.QDRANT_TIMEOUT
        self.batch_size = settings.QDRANT_BATCH_SIZE
        
        self.executor = ThreadPoolExecutor(
            max_workers=self.max_workers, 
            thread_name_prefix="qdrant_worker"
        )
        self._local = threading.local()
        self._initialized = True
        self._closed = False
        self._connection_verified = False
        
        logger.info(f"AsyncQdrantClient initialized with URL: {self.url}")
    
    def _get_client(self) -> QdrantClient:
        """Get thread-local client instance"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            try:
                self._local.client = QdrantClient(
                    url=self.url,
                    api_key=self.api_key,
                    prefer_grpc=False, # Use HTTP API
                    timeout=self.timeout,
                    grpc_options={ # These are ignored if prefer_grpc is False, but kept for completeness
                        "grpc.keepalive_time_ms": 30000,
                        "grpc.max_receive_message_length": 100 * 1024 * 1024  # 100MB
                    }
                )
                logger.debug(f"Created new Qdrant client for thread {threading.current_thread().name}")
            except Exception as e:
                logger.error(f"Failed to create Qdrant client: {str(e)}", exc_info=True)
                raise QdrantConnectionError(f"Cannot connect to Qdrant at {self.url}: {str(e)}")
        
        return self._local.client
    
    async def _verify_connection(self) -> bool:
        """Verify Qdrant connection and service availability"""
        if self._connection_verified:
            return True
            
        try:
            loop = asyncio.get_running_loop()
            
            def _check_connection():
                client = self._get_client()
                try:
                    # First try to get collections as basic health check
                    collections = client.get_collections()
                    logger.info(f"Qdrant connection verified - {len(collections.collections)} collections found")
                    return True
                except Exception as e:
                    logger.error(f"Qdrant connection check failed: {str(e)}")
                    raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
            
            result = await loop.run_in_executor(self.executor, _check_connection)
            self._connection_verified = result
            logger.info("Qdrant connection verified successfully")
            return result
            
        except Exception as e:
            logger.error(f"Qdrant connection verification failed: {str(e)}", exc_info=True)
            raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
    
    async def initialize(self):
        """Initialize and verify the client connection"""
        if self._closed:
            raise QdrantConnectionError("Client has been closed")
            
        await self._verify_connection()
        logger.info("AsyncQdrant client initialized and verified")
    
    async def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine",
        recreate_if_exists: bool = False
    ) -> bool:
        """Create a new collection with specified parameters"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _create_collection():
                client = self._get_client()
                
                # Check if collection exists
                try:
                    collections = client.get_collections()
                    existing_names = [col.name for col in collections.collections]
                    
                    if collection_name in existing_names:
                        if recreate_if_exists:
                            logger.info(f"Recreating existing collection: {collection_name}")
                            client.delete_collection(collection_name)
                            # Give a small moment for deletion to propagate if recreating
                            time.sleep(0.1) 
                        else:
                            logger.info(f"Collection {collection_name} already exists")
                            return True
                except Exception as e:
                    logger.warning(f"Error checking existing collections: {str(e)}")
                    # If checking collections fails, and we are not forcing recreate, raise
                    if not recreate_if_exists:
                        raise QdrantOperationError(f"Could not verify collection existence: {str(e)}")
            
                # Create collection with proper configuration
                distance_enum = Distance[distance.upper()] if hasattr(Distance, distance.upper()) else Distance.COSINE
                
                result = client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=distance_enum
                    )
                )
                
                logger.info(f"Created collection {collection_name} with {vector_size}D vectors")
                return True
            
            return await loop.run_in_executor(self.executor, _create_collection)
            
        except Exception as e:
            logger.error(f"Failed to create collection {collection_name}: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Collection creation failed: {str(e)}")
    
    async def collection_exists(self, collection_name: str) -> bool:
        """Check if collection exists"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _check_collection():
                client = self._get_client()
                try:
                    collections = client.get_collections()
                    return collection_name in [col.name for col in collections.collections]
                except Exception as e:
                    logger.warning(f"Error checking collection existence: {str(e)}")
                    return False
            
            return await loop.run_in_executor(self.executor, _check_collection)
            
        except Exception as e:
            logger.error(f"Error checking collection existence: {str(e)}", exc_info=True)
            return False
    
    async def get_collection_info(self, collection_name: str) -> Optional[CollectionInfo]:
        """Get collection information"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _get_info():
                client = self._get_client()
                try:
                    info = client.get_collection(collection_name)
                    return CollectionInfo(
                        name=collection_name,
                        status=info.status.value if hasattr(info.status, 'value') else str(info.status),
                        vector_size=info.config.params.vectors.size,
                        distance=info.config.params.vectors.distance.value,
                        points_count=info.points_count
                    )
                except Exception as e:
                    logger.error(f"Error getting collection info: {str(e)}")
                    return None
            
            return await loop.run_in_executor(self.executor, _get_info)
            
        except Exception as e:
            logger.error(f"Failed to get collection info: {str(e)}", exc_info=True)
            return None
    
    async def upsert(
        self,
        collection_name: str,
        points: List[PointStruct],
        batch_size: int = None
    ) -> bool:
        """Upsert points into collection with batching"""
        await self.initialize()
        
        if not points:
            logger.warning("No points to upsert")
            return True
        
        batch_size = batch_size or self.batch_size
        
        try:
            loop = asyncio.get_running_loop()
            
            def _upsert_batch(batch_points):
                client = self._get_client()
                
                # Validate points before upsert
                for point in batch_points:
                    if not isinstance(point.vector, list) or len(point.vector) == 0:
                        raise ValueError(f"Invalid vector for point {point.id}")
                    if not isinstance(point.payload, dict):
                        raise ValueError(f"Invalid payload for point {point.id}")
                
                result = client.upsert(
                    collection_name=collection_name,
                    points=batch_points,
                    wait=True
                )
                return result
            
            # Process in batches
            total_points = len(points)
            logger.info(f"Upserting {total_points} points in batches of {batch_size}")
            
            success_count = 0
            for i in range(0, total_points, batch_size):
                batch = points[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_points + batch_size - 1) // batch_size
                
                logger.debug(f"Processing batch {batch_num}/{total_batches} ({len(batch)} points)")
                
                try:
                    result = await loop.run_in_executor(self.executor, _upsert_batch, batch)
                    if result and result.status == models.UpdateStatus.COMPLETED:
                        success_count += len(batch)
                    else:
                        logger.error(f"Batch {batch_num} upsert failed with status: {result.status if result else 'N/A'}")
                        raise QdrantOperationError(f"Batch {batch_num} upsert failed")
                    
                    # Small delay between batches to avoid overwhelming Qdrant
                    if batch_num < total_batches:
                        await asyncio.sleep(0.01)
                except Exception as e:
                    logger.error(f"Failed to upsert batch {batch_num}: {str(e)}")
                    raise
            
            logger.info(f"Successfully upserted {success_count}/{total_points} points to {collection_name}")
            return success_count == total_points
            
        except Exception as e:
            logger.error(f"Failed to upsert points: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Upsert operation failed: {str(e)}")
    
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        query_filter: Optional[Filter] = None,
        limit: int = 5,
        with_vectors: bool = False,
        with_payload: bool = True,
        score_threshold: Optional[float] = None
    ) -> List[models.ScoredPoint]:
        """Search collection with query vector"""
        await self.initialize()
        
        if not query_vector:
            raise ValueError("Query vector cannot be empty")
        
        try:
            loop = asyncio.get_running_loop()
            
            def _search():
                client = self._get_client()
                
                results = client.search(
                    collection_name=collection_name,
                    query_vector=query_vector,
                    query_filter=query_filter,
                    limit=max(1, min(limit, 1000)),
                    with_vectors=with_vectors,
                    with_payload=with_payload,
                    score_threshold=score_threshold
                )
                return results
            
            results = await loop.run_in_executor(self.executor, _search)
            
            logger.debug(f"Search returned {len(results)} results for collection {collection_name}")
            return results
            
        except ValueError as ve:
            logger.error(f"Validation error in search: {str(ve)}")
            raise
        except Exception as e:
            logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Search operation failed: {str(e)}")
    
    async def scroll(
        self,
        collection_name: str,
        scroll_filter: Optional[Filter] = None,
        limit: int = 10,
        offset: Optional[str] = None,
        with_vectors: bool = False,
        with_payload: bool = True
    ) -> Tuple[List[models.Record], Optional[str]]:
        """Scroll through collection records"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _scroll():
                client = self._get_client()
                records, next_page_offset = client.scroll(
                    collection_name=collection_name,
                    scroll_filter=scroll_filter,
                    limit=max(1, min(limit, 1000)),
                    offset=offset,
                    with_vectors=with_vectors,
                    with_payload=with_payload
                )
                return records, next_page_offset
            
            records, next_offset = await loop.run_in_executor(self.executor, _scroll)
            logger.debug(f"Scroll returned {len(records)} records from {collection_name}")
            return records, next_offset
            
        except Exception as e:
            logger.error(f"Scroll failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Scroll operation failed: {str(e)}")
    
    async def delete(
        self,
        collection_name: str,
        points_selector: models.PointsSelector
    ) -> bool:
        """Delete points from collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _delete():
                client = self._get_client()
                result = client.delete(
                    collection_name=collection_name,
                    points_selector=points_selector,
                    wait=True
                )
                return result
            
            result = await loop.run_in_executor(self.executor, _delete)
            logger.info(f"Delete operation completed for collection {collection_name}")
            return bool(result and result.status == models.UpdateStatus.COMPLETED)
            
        except Exception as e:
            logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Delete operation failed: {str(e)}")
    
    async def count_points(
        self,
        collection_name: str,
        count_filter: Optional[Filter] = None
    ) -> int:
        """Count points in collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _count():
                client = self._get_client()
                result = client.count(
                    collection_name=collection_name,
                    count_filter=count_filter,
                    exact=True
                )
                return result.count
            
            count = await loop.run_in_executor(self.executor, _count)
            return count
            
        except Exception as e:
            logger.error(f"Count failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Count operation failed: {str(e)}")
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        try:
            await self.initialize()
            
            loop = asyncio.get_running_loop()
            
            def _health_check():
                client = self._get_client()
                start_time = time.time()
                
                try:
                    collections = client.get_collections()
                    
                    end_time = time.time()
                    
                    return {
                        "status": "healthy",
                        "url": self.url,
                        "response_time_ms": round((end_time - start_time) * 1000, 2),
                        "collections_count": len(collections.collections),
                        "timestamp": time.time()
                    }
                except Exception as e:
                    return {
                        "status": "unhealthy",
                        "url": self.url,
                        "error": str(e),
                        "timestamp": time.time()
                    }
            
            return await loop.run_in_executor(self.executor, _health_check)
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "url": self.url,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        
        try:
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
                logger.info("AsyncQdrant executor shutdown completed")
                
            if hasattr(self._local, 'client') and self._local.client is not None:
                try:
                    self._local.client.close()
                    logger.info("Main thread Qdrant client closed.")
                except Exception as e:
                    logger.warning(f"Error closing main thread client: {str(e)}")
        except Exception as e:
            logger.error(f"Error closing AsyncQdrant client: {str(e)}", exc_info=True)
        finally:
            # Reset the singleton instance to allow re-initialization if needed
            AsyncQdrantClient._instance = None 
            logger.info("AsyncQdrant client closed")

# Global instance management
_async_qdrant_client: Optional[AsyncQdrantClient] = None
_async_qdrant_lock = asyncio.Lock()

async def get_async_qdrant_client() -> AsyncQdrantClient:
    """Get or create an async Qdrant client instance"""
    global _async_qdrant_client
    
    async with _async_qdrant_lock:
        if _async_qdrant_client is None or _async_qdrant_client._closed:
            _async_qdrant_client = AsyncQdrantClient()
            await _async_qdrant_client.initialize()
        return _async_qdrant_client

@asynccontextmanager
async def qdrant_client_context():
    """Context manager for Qdrant client"""
    client = await get_async_qdrant_client()
    try:
        yield client
    finally:
        # In a global singleton scenario, we generally don't close the client here
        # as it's meant to be reused across requests.
        # Closing should be handled at application shutdown.
        pass

# Utility functions
def create_user_filter(user_id: str, additional_filters: Optional[Dict] = None) -> Filter:
    """Create a filter for user documents with optional additional filters"""
    must = [
        FieldCondition(
            key="user_id",
            match=MatchValue(value=user_id)
        )
    ]
    
    if additional_filters:
        for key, value in additional_filters.items():
            if isinstance(value, dict):
                # Handle range filters like {"gte": 2020}
                range_params = {}
                for op, op_value in value.items():
                    if op == "gte":
                        range_params["gte"] = op_value
                    elif op == "gt":
                        range_params["gt"] = op_value
                    elif op == "lte":
                        range_params["lte"] = op_value
                    elif op == "lt":
                        range_params["lt"] = op_value
                    elif op == "eq":
                        range_params["gte"] = op_value
                        range_params["lte"] = op_value
                
                if range_params:
                    must.append(FieldCondition(
                        key=key,
                        range=Range(**range_params)
                    ))
            elif isinstance(value, list):
                must.append(FieldCondition(
                    key=key,
                    match=MatchAny(any=value)
                ))
            else:
                must.append(FieldCondition(
                    key=key,
                    match=MatchValue(value=value)
                ))
    
    return Filter(must=must)

def create_document_filter(user_id: str, doc_id: str) -> Filter:
    """Create a filter for specific document"""
    return Filter(
        must=[
            FieldCondition(
                key="user_id",
                match=MatchValue(value=user_id)
            ),
            FieldCondition(
                key="doc_id",
                match=MatchValue(value=doc_id)
            )
        ]
    )

def validate_vector(vector: List[float], expected_dim: int) -> bool:
    """Validate vector dimensions and values"""
    if not isinstance(vector, list):
        return False
    if len(vector) != expected_dim:
        return False
    if not all(isinstance(x, (int, float)) for x in vector):
        return False
    return True
===== ./app/models/agent_model.py =====
# ./app/models/agent_model.py
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any, Union

class PersonalityTrait(str, Enum):
    FRIENDLY = "friendly"
    PROFESSIONAL = "professional"
    WITTY = "witty"
    EMPATHETIC = "empathetic"
    ENTHUSIASTIC = "enthusiastic"
    ANALYTICAL = "analytical"
    CREATIVE = "creative"
    SUPPORTIVE = "supportive"
    HUMOROUS = "humorous"
    SERIOUS = "serious"

class EmotionalAwarenessConfig(BaseModel):
    detect_emotion: bool = True
    adjust_tone: bool = True
    empathy_level: float = Field(0.8, ge=0, le=1)
    max_emotional_response_time: float = Field(1.5, gt=0)

class MemoryConfig(BaseModel):
    context_window: int = Field(10, gt=0, le=50)
    long_term_memory: bool = False
    memory_refresh_interval: int = Field(300, gt=0)

class AgentPersonality(BaseModel):
    traits: List[Union[PersonalityTrait, str]] = [PersonalityTrait.FRIENDLY]
    base_tone: str = "helpful and knowledgeable"
    base_tone_temperature: float = Field(0.7, ge=0.0, le=2.0)  # ADD THIS LINE
    emotional_awareness: EmotionalAwarenessConfig = Field(default_factory=EmotionalAwarenessConfig)
    memory: MemoryConfig = Field(default_factory=MemoryConfig)
    
    @validator('traits', pre=True)
    def validate_traits(cls, v):
        if isinstance(v, list):
            validated_traits = []
            for trait in v:
                if isinstance(trait, str):
                    validated_traits.append(trait)
                else:
                    validated_traits.append(trait.value if hasattr(trait, 'value') else str(trait))
            return validated_traits
        return v
    
    def dict(self, **kwargs):
        """Override dict method to handle partial updates correctly"""
        result = super().dict(**kwargs)
        return result




class VoiceModelConfig(BaseModel):
    model_name: str = Field(default="default", description="Voice model to use")
    speaking_rate: float = Field(default=1.0, ge=0.5, le=2.0)
    pitch: float = Field(default=0.0, ge=-20.0, le=20.0)
    volume_gain: float = Field(default=0.0, ge=-96.0, le=16.0)

class AgentBase(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: str = Field(..., description="Ollama model to use")
    system_prompt: str = Field(..., description="System prompt defining behavior")
    is_public: bool = Field(False)
    tools: List[str] = Field(default_factory=list)
    personality: AgentPersonality = Field(default_factory=AgentPersonality)
    agent_metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)

class AgentCreate(AgentBase):
    pass

class AgentUpdate(BaseModel):
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    is_public: Optional[bool] = None
    tools: Optional[List[str]] = None
    personality: Optional[AgentPersonality] = None
    agent_metadata: Optional[Dict[str, Any]] = None

class Agent(AgentBase):
    id: str
    owner_id: str
    rag_enabled: bool = False

class AgentResponse(AgentBase):
    id: str
    owner_id: str
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True




class AgentPermissions(str, Enum):
    EXECUTE = "execute"
    TRAIN = "train"
    SHARE = "share"
    MANAGE = "manage"

class AgentVisibility(str, Enum):
    PRIVATE = "private"
    TEAM = "team"
    ORGANIZATION = "organization"
    PUBLIC = "public"

class AgentRateLimit(BaseModel):
    requests_per_minute: int = Field(100, gt=0)
    concurrent_executions: int = Field(10, gt=0)

class AgentDeploymentConfig(BaseModel):
    gpu_required: bool = False
    memory_requirements: str = "1Gi"
    timeout_seconds: int = 300

class EnterpriseAgentConfig(BaseModel):
    visibility: AgentVisibility = AgentVisibility.PRIVATE
    rate_limits: AgentRateLimit = Field(default_factory=AgentRateLimit)
    deployment: AgentDeploymentConfig = Field(default_factory=AgentDeploymentConfig)
    allowed_domains: List[str] = []
    permissions: List[AgentPermissions] = [AgentPermissions.EXECUTE]
===== ./app/models/voice_model.py =====
# app/models/voice_model.py
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
from datetime import datetime

class AudioData(BaseModel):
    content: str = Field(..., description="Base64 encoded audio data")
    format: str = Field(default="webm", description="Audio format")
    size: Optional[int] = Field(None, description="Audio data size in bytes")

class VoiceConfig(BaseModel):
    model_name: str = Field(default="default", description="Voice model to use")
    speaking_rate: float = Field(default=1.0, ge=0.5, le=2.0, description="Speech rate")
    pitch: float = Field(default=0.0, ge=-20.0, le=20.0, description="Pitch adjustment")
    volume_gain: float = Field(default=0.0, ge=-10.0, le=10.0, description="Volume gain")
    voice: str = Field(default="alloy", description="Voice type")

class PersonalitySettings(BaseModel):
    traits: List[str] = Field(default_factory=list, description="Personality traits")
    base_tone: str = Field(default="friendly", description="Base tone of voice")
    emotional_awareness: Dict[str, Any] = Field(default_factory=dict)

class VoiceProcessingRequest(BaseModel):
    audio_data: str = Field(..., description="Base64 encoded audio data")
    format: Optional[str] = Field(default="webm", description="Audio format")
    session_id: str = Field(..., description="WebSocket session ID")
    user_id: str = Field(..., description="User ID")
    agent_id: str = Field(..., description="Agent ID")
    is_final: Optional[bool] = Field(default=False, description="Is final audio chunk")
    voice_config: Optional[Dict[str, Any]] = Field(default_factory=dict)
    system_prompt: Optional[str] = Field(default="")
    personality: Optional[Dict[str, Any]] = Field(default_factory=dict)

class VoiceResponse(BaseModel):
    success: bool
    transcription: str
    confidence: float
    is_final: bool
    session_id: str
    processing_time: float

class TextToSpeechRequest(BaseModel):
    text: str = Field(..., description="Text to convert to speech")
    voice_config: Optional[VoiceConfig] = Field(default_factory=VoiceConfig)
    user_id: Optional[str] = Field(None, description="User ID")
    agent_id: Optional[str] = Field(None, description="Agent ID")

class SpeechToTextRequest(BaseModel):
    audio_data: AudioData = Field(..., description="Audio data to transcribe")
    user_id: str = Field(..., description="User ID")
    language: Optional[str] = Field("en-US", description="Expected language")
    config: Optional[Dict[str, Any]] = Field(default_factory=dict)

===== ./app/models/__init__.py =====
# Empty file to make models a package
===== ./app/models/db_models.py =====
# app/models/db_models.py
from sqlalchemy import Column, String, Boolean, DateTime, ForeignKey, Integer, or_
from sqlalchemy.dialects.postgresql import JSONB, ARRAY, UUID
from sqlalchemy.orm import relationship
from app.database import Base
from datetime import datetime
import uuid

class DBAgent(Base):
    __tablename__ = "agents"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    owner_id = Column(String, nullable=False, index=True)
    name = Column(String(100), nullable=False)
    description = Column(String(500))
    model = Column(String, nullable=False)
    system_prompt = Column(String, nullable=False)
    is_public = Column(Boolean, default=False, index=True)
    tools = Column(ARRAY(String))
    personality = Column(JSONB)  # Add this field if missing
    agent_metadata = Column(JSONB)
    voice_enabled = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    
    # Relationships
    documents = relationship("DBDocument", back_populates="agent", cascade="all, delete-orphan")
    training_jobs = relationship("DBTrainingJob", back_populates="agent", cascade="all, delete-orphan")
    conversations = relationship("DBConversation", back_populates="agent", cascade="all, delete-orphan")

class DBDocument(Base):
    __tablename__ = "documents"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    filename = Column(String, nullable=False)
    content_type = Column(String)
    size = Column(Integer)
    qdrant_id = Column(String)
    document_metadata = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="documents")

class DBTrainingJob(Base):
    __tablename__ = "training_jobs"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    status = Column(String, nullable=False)
    config = Column(JSONB)
    result = Column(JSONB)
    error_message = Column(String)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="training_jobs")

class DBConversation(Base):
    __tablename__ = "conversations"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    memory_id = Column(String)
    context = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="conversations")
    messages = relationship("DBMessage", back_populates="conversation", cascade="all, delete-orphan")

class DBMessage(Base):
    __tablename__ = "messages"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    conversation_id = Column(String, ForeignKey('llm.conversations.id'), nullable=False, index=True)
    role = Column(String, nullable=False)
    content = Column(String, nullable=False)
    message_metadata = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    conversation = relationship("DBConversation", back_populates="messages")
===== ./app/models/training_model.py =====
from enum import Enum
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, validator
import mimetypes

class TrainingJobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PROCESSING_FILES = "processing_files"
    EXTRACTING_CONTENT = "extracting_content"

class TrainingDataType(str, Enum):
    URL = "url"
    TEXT = "text"
    FILE = "file"
    IMAGE = "image"

class TrainingDataItem(BaseModel):
    type: TrainingDataType
    content: str  # URL, text content, or file path
    metadata: Optional[Dict[str, Any]] = {}
    
    @validator('content')
    def validate_content(cls, v, values):
        data_type = values.get('type')
        if data_type == TrainingDataType.URL:
            # Basic URL validation
            if not v.startswith(('http://', 'https://')):
                raise ValueError('URL must start with http:// or https://')
        elif data_type == TrainingDataType.TEXT:
            if len(v.strip()) < 10:
                raise ValueError('Text content must be at least 10 characters')
        return v

class TrainingJobCreate(BaseModel):
    agent_id: str
    # Backward compatibility - keep data_urls for existing API
    data_urls: Optional[List[str]] = None
    # New enhanced data structure
    training_data: Optional[List[TrainingDataItem]] = None
    # Direct text input
    text_data: Optional[List[str]] = None
    # Training configuration
    config: Optional[Dict[str, Any]] = {}
    
    @validator('training_data', 'data_urls', 'text_data')
    def validate_data_sources(cls, v, values):
        # Ensure at least one data source is provided
        data_urls = values.get('data_urls')
        text_data = values.get('text_data')
        
        if not any([v, data_urls, text_data]):
            raise ValueError('At least one data source must be provided')
        return v

class FileUploadInfo(BaseModel):
    filename: str
    content_type: str
    size: int
    file_id: str

class TrainingJobResponse(BaseModel):
    id: str
    user_id: str
    agent_id: str
    data_urls: List[str] = []
    training_data: List[TrainingDataItem] = []
    text_data: List[str] = []
    uploaded_files: List[FileUploadInfo] = []
    status: TrainingJobStatus
    created_at: datetime
    updated_at: datetime
    progress: int
    current_step: Optional[str] = None
    total_steps: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0

class TrainingProgress(BaseModel):
    job_id: str
    status: TrainingJobStatus
    progress: int
    current_step: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0
    message: Optional[str] = None

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def get_file_type(filename: str) -> TrainingDataType:
    """Determine the training data type based on file extension"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    ext = f'.{ext}'
    
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return TrainingDataType.IMAGE
    else:
        return TrainingDataType.FILE

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    return f'.{ext}' in SUPPORTED_EXTENSIONS
===== ./app/models/response_schema.py =====
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    options: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    message: str
    model: str
    context: List[Any]
    tokens_used: int



# response_schema.py
class RAGQueryRequest(BaseModel):
    query: str
    max_results: int = Field(5, gt=0, le=20)
    rewrite_query: bool = Field(True, description="Enable query rewriting")
    use_reranking: bool = Field(True, description="Enable cross-encoder re-ranking")
    filters: Optional[Dict[str, Any]] = Field(None, description="Metadata filters for retrieval")
    hybrid_search: bool = Field(True, description="Enable hybrid sparse+dense search")

class RAGResponse(BaseModel):
    answer: str
    documents: List[str]
    sources: List[str]
    context: List[str]
    debug: Optional[Dict[str, Any]] = Field(None, description="Debug information about the search process")
    search_method: Optional[str] = Field(None, description="Search method used")
    processed_query: Optional[str] = Field(None, description="Processed query after rewriting")

class DocumentResponse(BaseModel):
    document_id: str
    filename: str
    status: str = "success"

class HybridSearchConfig(BaseModel):
    sparse_weight: float = Field(0.4, ge=0, le=1)
    dense_weight: float = Field(0.6, ge=0, le=1)
    enable_hybrid: bool = True
===== ./app/scripts/fix_dimension_mismatch.py =====
# scripts/fix_dimension_mismatch.py
"""
Script to fix ChromaDB embedding dimension mismatch issues.
Run this script to reset your ChromaDB collection when you encounter dimension errors.
"""

import asyncio
import logging
import sys
import os
from pathlib import Path

# Add the parent directory to the path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.rag_service import RAGService
from app.services.llm_service import LLMService
from app.config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def check_embedding_model():
    """Check what embedding dimension your current model produces"""
    try:
        ollama = LLMService()
        test_text = "This is a test sentence to check embedding dimensions."
        
        logger.info("Testing embedding model...")
        embedding = await ollama.create_embedding(test_text)
        
        if embedding:
            logger.info(f"‚úÖ Embedding model working")
            logger.info(f"‚úÖ Current embedding dimension: {len(embedding)}")
            logger.info(f"‚úÖ Model: {settings.EMBEDDING_MODEL}")
            return len(embedding)
        else:
            logger.error("‚ùå Failed to get embedding from model")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Error testing embedding model: {str(e)}")
        return None

async def reset_chroma_collection():
    """Reset the ChromaDB collection to fix dimension issues"""
    try:
        logger.info("Initializing RAG service...")
        rag_service = RAGService()
        
        logger.info("Resetting ChromaDB collection...")
        await rag_service.reset_collection()
        
        logger.info("‚úÖ Collection reset successfully!")
        
        # Clean up
        await rag_service.close()
        
    except Exception as e:
        logger.error(f"‚ùå Error resetting collection: {str(e)}")
        raise

async def verify_setup():
    """Verify that everything is working correctly"""
    try:
        logger.info("Verifying setup...")
        
        # Check embedding model
        embedding_dim = await check_embedding_model()
        if not embedding_dim:
            return False
            
        # Test RAG service initialization
        rag_service = RAGService()
        await rag_service.initialize()
        
        logger.info("‚úÖ Setup verification completed successfully!")
        
        # Clean up
        await rag_service.close()
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Setup verification failed: {str(e)}")
        return False

async def main():
    """Main function to fix dimension mismatch issues"""
    print("üîß ChromaDB Dimension Fix Tool")
    print("=" * 40)
    
    # Step 1: Check current embedding model
    print("\nüìä Step 1: Checking embedding model...")
    embedding_dim = await check_embedding_model()
    
    if not embedding_dim:
        print("‚ùå Cannot proceed without working embedding model")
        print("Please check your Ollama installation and model configuration")
        return
    
    # Step 2: Show current configuration
    print(f"\n‚öôÔ∏è  Current Configuration:")
    print(f"   - Embedding Model: {settings.EMBEDDING_MODEL}")
    print(f"   - Embedding Dimension: {embedding_dim}")
    print(f"   - ChromaDB Path: {settings.CHROMA_PATH}")
    print(f"   - Collection Name: {settings.CHROMA_COLLECTION_NAME}")
    
    # Step 3: Ask user if they want to reset
    print(f"\nüîÑ The embedding dimension mismatch error occurs when:")
    print(f"   - Your collection expects one dimension (e.g., 384)")
    print(f"   - Your embedding model produces another (e.g., 1536)")
    print(f"   - Current model produces: {embedding_dim} dimensions")
    
    response = input(f"\n‚ùì Do you want to reset the ChromaDB collection? (y/N): ").lower()
    
    if response in ['y', 'yes']:
        print(f"\nüóëÔ∏è  Step 3: Resetting ChromaDB collection...")
        await reset_chroma_collection()
        
        print(f"\n‚úÖ Step 4: Verifying setup...")
        success = await verify_setup()
        
        if success:
            print(f"\nüéâ All done! Your ChromaDB is now ready to use.")
            print(f"   - Collection reset successfully")
            print(f"   - Embedding dimension: {embedding_dim}")
            print(f"   - You can now upload documents without dimension errors")
        else:
            print(f"\n‚ùå Verification failed. Please check the logs for errors.")
    else:
        print(f"\nüìù To fix this manually:")
        print(f"   1. Delete the ChromaDB directory: {settings.CHROMA_PATH}")
        print(f"   2. Or use a different embedding model with 384 dimensions")
        print(f"   3. Or modify your code to handle {embedding_dim} dimensions")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n\nüëã Exiting...")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        sys.exit(1)
===== ./app/main.py =====
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from app.config import settings
from app.database import engine, Base
from app.utils.logging import configure_logging
from app.routers.voice_websocket import router as voice_websocket_router
from app.routers.voice_health import router as voice_health_router
from app.routers import voice_processing

# Import all routers
from app.routers import (
    auth_router,
    agents_router, 
    chat_router,
    rag_router,
    training_router,
    voice_router,
    agent_interaction_router,
    health_router,
    execute_router,
    monitoring_router,
    voice_websocket_router
)

# Import middleware
from app.middleware import LoggingMiddleware, RateLimiterMiddleware
from app.middleware.errorHandlingMiddleware import ErrorHandlingMiddleware
from app.middleware.metrics_middleware import metrics_middleware

import logging

# Configure logging
logger = configure_logging()

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="AI Agent Platform with Multi-Source Internet Search",
    docs_url="/docs" if settings.DEBUG else None,
    redoc_url="/redoc" if settings.DEBUG else None,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add custom middleware
app.add_middleware(ErrorHandlingMiddleware)
app.add_middleware(LoggingMiddleware)

if settings.RATE_LIMITING_ENABLED:
    app.add_middleware(
        RateLimiterMiddleware,
        max_requests=settings.RATE_LIMIT_MAX_REQUESTS,
        time_window=settings.RATE_LIMIT_TIME_WINDOW
    )

# Add metrics middleware if enabled
app.middleware("http")(metrics_middleware)

# Include routers with proper prefixes
app.include_router(auth_router, prefix=settings.API_PREFIX + "/auth")
app.include_router(agents_router, prefix=settings.API_PREFIX)
app.include_router(chat_router, prefix=settings.API_PREFIX)
app.include_router(rag_router, prefix=settings.API_PREFIX)
app.include_router(training_router, prefix=settings.API_PREFIX + "/training")

# Add the voice WebSocket router with correct prefix
app.include_router(voice_websocket_router, prefix=settings.API_PREFIX)

# Continue with other routers...
app.include_router(agent_interaction_router, prefix=settings.API_PREFIX)
app.include_router(health_router, prefix=settings.API_PREFIX)
app.include_router(execute_router, prefix=settings.API_PREFIX)
app.include_router(monitoring_router, prefix=settings.API_PREFIX)
app.include_router(voice_health_router, prefix=settings.API_PREFIX + "/voice/ws")
app.include_router(voice_processing.router,prefix=settings.API_PREFIX)

@app.on_event("startup")
async def startup_event():
    """Application startup tasks"""
    try:
        # Create database tables
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        logger.info("=== AI Agent Platform Started ===")
        logger.info(f"Environment: {settings.ENVIRONMENT}")
        logger.info(f"Debug Mode: {settings.DEBUG}")
        logger.info(f"Internet Search: {'Enabled' if settings.ENABLE_INTERNET_SEARCH else 'Disabled'}")
        if settings.ENABLE_INTERNET_SEARCH:
            logger.info(f"Search Sources: {', '.join(settings.SEARCH_SOURCES)}")
        logger.info("=== Ready to Accept Requests ===")
        
    except Exception as e:
        logger.error(f"Startup failed: {str(e)}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """Application shutdown tasks"""
    logger.info("Shutting down AI Agent Platform...")

@app.get("/")
async def root():
    """Root endpoint with API information"""
    return {
        "message": "AI Agent Platform with Internet Search",
        "version": settings.APP_VERSION,
        "features": {
            "internet_search": settings.ENABLE_INTERNET_SEARCH,
            "search_sources": settings.SEARCH_SOURCES if settings.ENABLE_INTERNET_SEARCH else [],
            "api_docs": "/docs" if settings.DEBUG else "disabled"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "timestamp": __import__('time').time()
    }

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error occurred",
            "type": type(exc).__name__
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower()
    )

===== ./app/services/voice_session_manager.py =====
# app/services/voice_session_manager.py - Enhanced with better error handling
import time
import uuid
import logging
import asyncio
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
from fastapi import WebSocket

logger = logging.getLogger(__name__)

@dataclass
class VoiceSession:
    session_id: str
    user_id: str
    agent_id: str
    websocket: WebSocket
    agent: Any
    created_at: float
    last_activity: float
    conversation_history: list = field(default_factory=list)
    is_active: bool = True
    error_count: int = 0
    processing_count: int = 0
    
    def add_to_history(self, user_message: str, agent_response: str):
        """Add conversation to history with automatic cleanup"""
        self.conversation_history.append({
            "timestamp": time.time(),
            "user": user_message,
            "agent": agent_response,
            "session_id": self.session_id
        })
        
        # Keep only last 20 exchanges
        if len(self.conversation_history) > 20:
            self.conversation_history = self.conversation_history[-20:]
        
        self.last_activity = time.time()
    
    def get_recent_history(self, count: int = 4) -> list:
        """Get recent conversation history formatted for LLM"""
        recent = self.conversation_history[-count:] if count > 0 else []
        formatted = []
        
        for item in recent:
            formatted.extend([
                {"role": "user", "content": item["user"]},
                {"role": "assistant", "content": item["agent"]}
            ])
        
        return formatted
    
    def get_conversation_summary(self) -> str:
        """Get a summary of the conversation"""
        if not self.conversation_history:
            return "New conversation"
        
        # Simple summary - last few exchanges
        recent = self.conversation_history[-3:]
        summary_parts = []
        
        for item in recent:
            user_preview = item['user'][:30] + "..." if len(item['user']) > 30 else item['user']
            agent_preview = item['agent'][:30] + "..." if len(item['agent']) > 30 else item['agent']
            summary_parts.append(f"User: {user_preview} | Agent: {agent_preview}")
        
        return " | ".join(summary_parts)
    
    def is_first_message(self) -> bool:
        """Check if this is the first message in the session"""
        return len(self.conversation_history) == 0
    
    def increment_error(self):
        """Increment error count"""
        self.error_count += 1
        self.last_activity = time.time()
    
    def increment_processing(self):
        """Increment processing count"""
        self.processing_count += 1
        self.last_activity = time.time()
    
    def get_session_stats(self) -> Dict[str, Any]:
        """Get session statistics"""
        current_time = time.time()
        return {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "agent_id": self.agent_id,
            "duration": round(current_time - self.created_at, 2),
            "last_activity": round(current_time - self.last_activity, 2),
            "message_count": len(self.conversation_history),
            "error_count": self.error_count,
            "processing_count": self.processing_count,
            "is_active": self.is_active
        }

class VoiceSessionManager:
    def __init__(self):
        self.sessions: Dict[str, VoiceSession] = {}
        self.user_sessions: Dict[str, set] = {}
        self.session_timeout = 1800  # 30 minutes
        self.max_sessions_per_user = 5
        self.cleanup_task = None
        self.total_sessions_created = 0
        
        logger.info("Enhanced Voice Session Manager initialized")

    async def create_session(
        self, 
        connection_id: str, 
        user_id: str, 
        agent_id: str, 
        websocket: WebSocket, 
        agent: Any
    ) -> VoiceSession:
        """Create a new voice session with enhanced validation"""
        try:
            # Check user session limits
            if user_id in self.user_sessions:
                if len(self.user_sessions[user_id]) >= self.max_sessions_per_user:
                    # Clean up old sessions for this user
                    await self._cleanup_user_sessions(user_id)
                    
                    if len(self.user_sessions[user_id]) >= self.max_sessions_per_user:
                        raise Exception(f"User {user_id} has too many active sessions")
            
            current_time = time.time()
            
            session = VoiceSession(
                session_id=connection_id,
                user_id=user_id,
                agent_id=agent_id,
                websocket=websocket,
                agent=agent,
                created_at=current_time,
                last_activity=current_time
            )
            
            # Store session
            self.sessions[connection_id] = session
            
            # Track user sessions
            if user_id not in self.user_sessions:
                self.user_sessions[user_id] = set()
            self.user_sessions[user_id].add(connection_id)
            
            # Start cleanup task if needed
            if not self.cleanup_task:
                self.cleanup_task = asyncio.create_task(self._periodic_cleanup())
            
            self.total_sessions_created += 1
            
            logger.info(f"Voice session created: {connection_id} for user {user_id}")
            return session
            
        except Exception as e:
            logger.error(f"Failed to create voice session: {str(e)}")
            raise

    async def get_session(self, session_id: str) -> Optional[VoiceSession]:
        """Get session with activity update"""
        session = self.sessions.get(session_id)
        if session and session.is_active:
            session.last_activity = time.time()
            return session
        return None

    async def remove_session(self, session_id: str):
        """Remove session with proper cleanup"""
        try:
            if session_id not in self.sessions:
                logger.warning(f"Session not found for removal: {session_id}")
                return
            
            session = self.sessions[session_id]
            
            # Mark as inactive
            session.is_active = False
            
            # Remove from user tracking
            if session.user_id in self.user_sessions:
                self.user_sessions[session.user_id].discard(session_id)
                if not self.user_sessions[session.user_id]:
                    del self.user_sessions[session.user_id]
            
            # Cleanup session resources
            await session.cleanup() if hasattr(session, 'cleanup') else None
            
            # Remove from sessions
            del self.sessions[session_id]
            
            logger.info(f"Voice session removed: {session_id}")
            
        except Exception as e:
            logger.error(f"Error removing session {session_id}: {str(e)}")

    async def _cleanup_user_sessions(self, user_id: str):
        """Clean up old sessions for a specific user"""
        if user_id not in self.user_sessions:
            return
        
        user_session_ids = list(self.user_sessions[user_id])
        current_time = time.time()
        
        # Sort by last activity (oldest first)
        sessions_with_activity = []
        for session_id in user_session_ids:
            session = self.sessions.get(session_id)
            if session:
                sessions_with_activity.append((session_id, session.last_activity))
        
        sessions_with_activity.sort(key=lambda x: x[1])
        
        # Remove oldest sessions if over limit
        sessions_to_remove = len(sessions_with_activity) - self.max_sessions_per_user + 1
        
        for i in range(min(sessions_to_remove, len(sessions_with_activity))):
            session_id = sessions_with_activity[i][0]
            await self.remove_session(session_id)
            logger.info(f"Removed old session for user {user_id}: {session_id}")

    async def _cleanup_stale_sessions(self):
        """Clean up stale sessions"""
        try:
            current_time = time.time()
            stale_sessions = []
            
            for session_id, session in self.sessions.items():
                # Check if session is stale
                if (current_time - session.last_activity) > self.session_timeout:
                    stale_sessions.append(session_id)
                    continue
                
                # Check if session has too many errors
                if session.error_count > 10:
                    stale_sessions.append(session_id)
                    continue
                
                # Check if WebSocket is still alive
                try:
                    await session.websocket.ping()
                except Exception:
                    stale_sessions.append(session_id)
            
            # Remove stale sessions
            for session_id in stale_sessions:
                await self.remove_session(session_id)
            
            if stale_sessions:
                logger.info(f"Cleaned up {len(stale_sessions)} stale sessions")
                
        except Exception as e:
            logger.error(f"Session cleanup error: {str(e)}")

    async def _periodic_cleanup(self):
        """Periodic cleanup task"""
        try:
            while True:
                await asyncio.sleep(300)  # Run every 5 minutes
                
                if not self.sessions:
                    # No sessions, stop cleanup task
                    self.cleanup_task = None
                    break
                
                await self._cleanup_stale_sessions()
                
        except asyncio.CancelledError:
            logger.info("Session cleanup task cancelled")
        except Exception as e:
            logger.error(f"Periodic session cleanup error: {str(e)}")

    def get_total_sessions(self) -> int:
        """Get total number of sessions ever created"""
        return self.total_sessions_created

    def get_active_session_count(self) -> int:
        """Get number of active sessions"""
        return len([s for s in self.sessions.values() if s.is_active])

    async def get_session_stats(self) -> Dict[str, Any]:
        """Get comprehensive session statistics"""
        try:
            current_time = time.time()
            active_sessions = 0
            total_messages = 0
            total_errors = 0
            avg_duration = 0
            
            for session in self.sessions.values():
                if session.is_active:
                    active_sessions += 1
                    avg_duration += (current_time - session.created_at)
                total_messages += len(session.conversation_history)
                total_errors += session.error_count
            
            if active_sessions > 0:
                avg_duration = avg_duration / active_sessions
            
            return {
                "total_sessions": len(self.sessions),
                "active_sessions": active_sessions,
                "unique_users": len(self.user_sessions),
                "total_sessions_created": self.total_sessions_created,
                "total_messages": total_messages,
                "total_errors": total_errors,
                "average_duration": round(avg_duration, 2),
                "session_timeout": self.session_timeout,
                "max_sessions_per_user": self.max_sessions_per_user
            }
            
        except Exception as e:
            logger.error(f"Error getting session stats: {str(e)}")
            return {}

    async def broadcast_to_user_sessions(self, user_id: str, message: Dict[str, Any]) -> int:
        """Broadcast message to all sessions for a user"""
        if user_id not in self.user_sessions:
            return 0
        
        successful_sends = 0
        session_ids = list(self.user_sessions[user_id])
        
        for session_id in session_ids:
            session = await self.get_session(session_id)
            if session and session.websocket:
                try:
                    import json
                    await session.websocket.send_text(json.dumps(message))
                    successful_sends += 1
                except Exception as e:
                    logger.error(f"Failed to send to session {session_id}: {str(e)}")
                    session.increment_error()
        
        return successful_sends

    async def cleanup(self):
        """Cleanup all sessions and resources"""
        try:
            # Cancel cleanup task
            if self.cleanup_task and not self.cleanup_task.done():
                self.cleanup_task.cancel()
            
            # Remove all sessions
            session_ids = list(self.sessions.keys())
            for session_id in session_ids:
                await self.remove_session(session_id)
            
            # Clear data structures
            self.sessions.clear()
            self.user_sessions.clear()
            
            logger.info("Voice Session Manager cleanup completed")
            
        except Exception as e:
            logger.error(f"Voice Session Manager cleanup error: {str(e)}")

===== ./app/services/multi_search_service.py =====
import asyncio
import aiohttp
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json
import hashlib
from urllib.parse import quote_plus
from app.config import settings

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    title: str
    content: str
    url: str
    source: str
    relevance_score: float
    timestamp: Optional[datetime] = None
    metadata: Dict[str, Any] = None

class MultiSearchService:
    def __init__(self):
        self.session_timeout = aiohttp.ClientTimeout(total=30)
        self.enabled_sources = getattr(settings, 'SEARCH_SOURCES', ['duckduckgo', 'reddit'])
        
    async def search_all_sources(
        self,
        query: str,
        sources: List[str] = None,
        max_results_per_source: int = 5
    ) -> Dict[str, List[SearchResult]]:
        """Search across multiple sources simultaneously"""
        
        if sources is None:
            sources = self.enabled_sources
            
        # Filter to available sources only
        available_sources = ['duckduckgo', 'reddit', 'hackernews', 'wikipedia']
        sources = [s for s in sources if s in available_sources]
        
        if not sources:
            logger.warning("No valid search sources provided")
            return {}
        
        tasks = []
        
        # Create search tasks for each source
        for source in sources:
            if source == 'duckduckgo':
                tasks.append(self._search_duckduckgo(query, max_results_per_source))
            elif source == 'reddit':
                tasks.append(self._search_reddit(query, max_results_per_source))
            elif source == 'hackernews':
                tasks.append(self._search_hackernews(query, max_results_per_source))
            elif source == 'wikipedia':
                tasks.append(self._search_wikipedia(query, max_results_per_source))
        
        # Execute all searches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Combine and organize results
        combined_results = {}
        for i, result in enumerate(results):
            source_name = sources[i] if i < len(sources) else f"source_{i}"
            if isinstance(result, Exception):
                logger.error(f"Search failed for {source_name}: {str(result)}")
                combined_results[source_name] = []
            else:
                combined_results[source_name] = result
                
        return combined_results
    
    async def _search_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """Search using DuckDuckGo Instant Answer API"""
        try:
            async with aiohttp.ClientSession(timeout=self.session_timeout) as session:
                url = "https://api.duckduckgo.com/"
                params = {
                    'q': query,
                    'format': 'json',
                    'no_redirect': '1',
                    'no_html': '1',
                    'skip_disambig': '1'
                }
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_duckduckgo_results(data, max_results)
                    else:
                        logger.error(f"DuckDuckGo search failed: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"DuckDuckGo search error: {str(e)}")
            return []
    
    async def _search_reddit(self, query: str, max_results: int) -> List[SearchResult]:
        """Search Reddit using their JSON API"""
        try:
            async with aiohttp.ClientSession(timeout=self.session_timeout) as session:
                headers = {'User-Agent': 'AI-Agent-Search/1.0'}
                url = f"https://www.reddit.com/search.json"
                params = {
                    'q': query,
                    'sort': 'relevance',
                    'limit': min(max_results, 25),
                    't': 'all'
                }
                
                async with session.get(url, params=params, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_reddit_results(data, max_results)
                    else:
                        logger.error(f"Reddit search failed: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"Reddit search error: {str(e)}")
            return []
    
    async def _search_hackernews(self, query: str, max_results: int) -> List[SearchResult]:
        """Search Hacker News using Algolia API"""
        try:
            async with aiohttp.ClientSession(timeout=self.session_timeout) as session:
                url = "https://hn.algolia.com/api/v1/search"
                params = {
                    'query': query,
                    'tags': 'story',
                    'hitsPerPage': min(max_results, 50)
                }
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_hackernews_results(data, max_results)
                    else:
                        logger.error(f"Hacker News search failed: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"Hacker News search error: {str(e)}")
            return []
    
    async def _search_wikipedia(self, query: str, max_results: int) -> List[SearchResult]:
        """Search Wikipedia using their API"""
        try:
            async with aiohttp.ClientSession(timeout=self.session_timeout) as session:
                url = "https://en.wikipedia.org/api/rest_v1/page/summary/"
                encoded_query = quote_plus(query)
                
                async with session.get(f"{url}{encoded_query}") as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_wikipedia_results(data, query)
                    elif response.status == 404:
                        # Try search API if direct lookup fails
                        return await self._search_wikipedia_opensearch(session, query, max_results)
                    else:
                        logger.error(f"Wikipedia search failed: {response.status}")
                        return []
                        
        except Exception as e:
            logger.error(f"Wikipedia search error: {str(e)}")
            return []
    
    async def _search_wikipedia_opensearch(self, session: aiohttp.ClientSession, query: str, max_results: int) -> List[SearchResult]:
        """Fallback Wikipedia search using OpenSearch API"""
        try:
            url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'opensearch',
                'search': query,
                'limit': min(max_results, 10),
                'namespace': 0,
                'format': 'json'
            }
            
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    if len(data) >= 4:
                        titles, descriptions, urls = data[1], data[2], data[3]
                        for i, (title, desc, url) in enumerate(zip(titles, descriptions, urls)):
                            if i >= max_results:
                                break
                            results.append(SearchResult(
                                title=title,
                                content=desc or f"Wikipedia article about {title}",
                                url=url,
                                source='wikipedia',
                                relevance_score=0.7 - (i * 0.1),
                                timestamp=datetime.now(),
                                metadata={'type': 'wikipedia_article'}
                            ))
                    return results
                else:
                    return []
                    
        except Exception as e:
            logger.error(f"Wikipedia OpenSearch error: {str(e)}")
            return []
    
    def _parse_duckduckgo_results(self, data: dict, max_results: int) -> List[SearchResult]:
        """Parse DuckDuckGo API response"""
        results = []
        
        # Parse instant answer
        if data.get('Answer'):
            results.append(SearchResult(
                title=data.get('Heading', 'DuckDuckGo Answer'),
                content=data['Answer'],
                url=data.get('AnswerURL', ''),
                source='duckduckgo',
                relevance_score=0.9,
                timestamp=datetime.now(),
                metadata={'type': 'instant_answer'}
            ))
        
        # Parse abstract
        if data.get('Abstract'):
            results.append(SearchResult(
                title=data.get('Heading', 'Abstract'),
                content=data['Abstract'],
                url=data.get('AbstractURL', ''),
                source='duckduckgo',
                relevance_score=0.8,
                timestamp=datetime.now(),
                metadata={'type': 'abstract'}
            ))
        
        # Parse related topics
        for topic in data.get('RelatedTopics', [])[:max_results]:
            if isinstance(topic, dict) and topic.get('Text'):
                results.append(SearchResult(
                    title=topic.get('FirstURL', '').split('/')[-1].replace('_', ' '),
                    content=topic['Text'],
                    url=topic.get('FirstURL', ''),
                    source='duckduckgo',
                    relevance_score=0.6,
                    timestamp=datetime.now(),
                    metadata={'type': 'related_topic'}
                ))
        
        return results[:max_results]
    
    def _parse_reddit_results(self, data: dict, max_results: int) -> List[SearchResult]:
        """Parse Reddit API response"""
        results = []
        
        if 'data' in data and 'children' in data['data']:
            for i, post in enumerate(data['data']['children'][:max_results]):
                if i >= max_results:
                    break
                    
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                content = post_data.get('selftext', '') or post_data.get('url', '')
                
                results.append(SearchResult(
                    title=title,
                    content=content[:500] + '...' if len(content) > 500 else content,
                    url=f"https://reddit.com{post_data.get('permalink', '')}",
                    source='reddit',
                    relevance_score=0.7 - (i * 0.05),
                    timestamp=datetime.fromtimestamp(post_data.get('created_utc', 0)) if post_data.get('created_utc') else datetime.now(),
                    metadata={
                        'subreddit': post_data.get('subreddit', ''),
                        'score': post_data.get('score', 0),
                        'num_comments': post_data.get('num_comments', 0)
                    }
                ))
        
        return results
    
    def _parse_hackernews_results(self, data: dict, max_results: int) -> List[SearchResult]:
        """Parse Hacker News API response"""
        results = []
        
        for i, hit in enumerate(data.get('hits', [])[:max_results]):
            if i >= max_results:
                break
                
            title = hit.get('title', '')
            url = hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID', '')}")
            
            results.append(SearchResult(
                title=title,
                content=hit.get('story_text', '') or f"Hacker News discussion: {title}",
                url=url,
                source='hackernews',
                relevance_score=0.75 - (i * 0.05),
                timestamp=datetime.fromisoformat(hit.get('created_at', '').replace('Z', '+00:00')) if hit.get('created_at') else datetime.now(),
                metadata={
                    'points': hit.get('points', 0),
                    'num_comments': hit.get('num_comments', 0),
                    'author': hit.get('author', '')
                }
            ))
        
        return results
    
    def _parse_wikipedia_results(self, data: dict, query: str) -> List[SearchResult]:
        """Parse Wikipedia API response"""
        if data.get('type') == 'standard':
            return [SearchResult(
                title=data.get('title', ''),
                content=data.get('extract', ''),
                url=data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                source='wikipedia',
                relevance_score=0.8,
                timestamp=datetime.now(),
                metadata={
                    'type': 'wikipedia_summary',
                    'thumbnail': data.get('thumbnail', {}).get('source') if data.get('thumbnail') else None
                }
            )]
        return []

class SearchResultAggregator:
    def __init__(self):
        self.source_weights = {
            'duckduckgo': 0.9,
            'wikipedia': 0.8,
            'reddit': 0.7,
            'hackernews': 0.75
        }
    
    def aggregate_results(
        self,
        search_results: Dict[str, List[SearchResult]],
        max_final_results: int = 10
    ) -> List[SearchResult]:
        """Combine and rank results from multiple sources"""
        
        all_results = []
        
        # Flatten results and apply source weights
        for source, results in search_results.items():
            weight = self.source_weights.get(source, 0.5)
            for result in results:
                result.relevance_score *= weight
                all_results.append(result)
        
        # Remove duplicates based on URL similarity
        unique_results = self._deduplicate_results(all_results)
        
        # Sort by relevance score
        unique_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return unique_results[:max_final_results]
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Remove duplicate results based on URL and content similarity"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            # Normalize URL for comparison
            normalized_url = self._normalize_url(result.url)
            
            if normalized_url and normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_results.append(result)
            elif not normalized_url:
                # If no URL, check content similarity (basic)
                content_hash = hashlib.md5(result.content[:100].encode()).hexdigest()
                if content_hash not in seen_urls:
                    seen_urls.add(content_hash)
                    unique_results.append(result)
        
        return unique_results
    
    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication"""
        if not url:
            return ""
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(url)
            return f"{parsed.netloc}{parsed.path}".lower()
        except:
            return url.lower()

===== ./app/services/agent_interaction_service.py =====
# ./app/services/agent_interaction_service.py
import logging
from typing import Dict, Optional, List, AsyncIterator, Any  # Added Any here
from fastapi import UploadFile
from app.services.llm_service import OllamaService
from app.services.voice_service import VoiceService
from app.services.rag_service import RAGService
from app.models.agent_model import AgentPersonality
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio

logger = logging.getLogger(__name__)

class EmotionalAnalyzer:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        
    async def analyze_emotion(self, text: str) -> Dict[str, float]:
        """Analyze emotional tone from text"""
        prompt = """Analyze the emotional tone of this text. Respond ONLY with a JSON object containing 
        emotion scores between 0-1 for: happiness, sadness, anger, fear, surprise, neutral.

        Text: {text}""".format(text=text)
        
        try:
            response = await self.ollama.generate(
                prompt=prompt,
                model="deepseek-r1:1.5b",
                options={"temperature": 0.7}
            )
            
            # Parse the JSON response
            import json
            return json.loads(response["response"])
        except Exception as e:
            logger.error(f"Emotion analysis failed: {str(e)}")
            return {
                "happiness": 0.5,
                "sadness": 0.0,
                "anger": 0.0,
                "fear": 0.0,
                "surprise": 0.0,
                "neutral": 0.5
            }

class AgentInteractionService:
    def __init__(self):
        self.ollama = OllamaService()
        self.voice = VoiceService()
        self.rag = RAGService()
        self.emotion_analyzer = EmotionalAnalyzer(self.ollama)
        self.conversation_memory = {}
        
    async def initialize(self):
        """Initialize all required services"""
        await self.rag.initialize()
        
    async def process_input(
        self,
        agent_id: str,
        user_id: str,
        input_text: Optional[str] = None,
        audio_file: Optional[UploadFile] = None,
        db: Optional[AsyncSession] = None
    ) -> Dict[str, Any]:
        """Process user input (text or voice) and generate response"""
        # Convert audio to text if provided
        if audio_file:
            input_text = await self.voice.speech_to_text(audio_file)
            
        if not input_text:
            raise ValueError("No input text provided")
            
        # Get or create conversation memory
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = {
                "history": [],
                "last_updated": datetime.now(),
                "emotional_state": {}
            }
            
        # Analyze emotion
        emotion_scores = await self.emotion_analyzer.analyze_emotion(input_text)
        self.conversation_memory[user_id]["emotional_state"] = emotion_scores
        
        # Retrieve relevant context
        rag_results = await self.rag.query(
            db=db,
            user_id=user_id,
            query=input_text
        )
        
        # Generate response considering emotion and context
        response = await self._generate_response(
            agent_id=agent_id,
            user_id=user_id,
            input_text=input_text,
            emotion_scores=emotion_scores,
            context=rag_results.get("documents", [])
        )
        
        # Update conversation history
        self.conversation_memory[user_id]["history"].append({
            "input": input_text,
            "response": response,
            "timestamp": datetime.now()
        })
        
        return {
            "text_response": response,
            "emotional_state": emotion_scores,
            "context_used": rag_results.get("sources", [])
        }
        
    async def _generate_response(
        self,
        agent_id: str,
        user_id: str,
        input_text: str,
        emotion_scores: Dict[str, float],
        context: List[str]
    ) -> str:
        """Generate response considering emotional state and context"""
        # Get dominant emotion
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        # Build system prompt based on emotion and context
        system_prompt = self._build_system_prompt(
            agent_id=agent_id,
            dominant_emotion=dominant_emotion,
            context=context
        )
        
        # Generate response
        response = await self.ollama.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": input_text}
            ]
        )
        
        return response.get("message", {}).get("content", "")
        
    def _build_system_prompt(
        self,
        agent_id: str,
        dominant_emotion: str,
        context: List[str]
    ) -> str:
        """Build dynamic system prompt based on context and emotion"""
        # TODO: Fetch agent personality from database
        personality = AgentPersonality()  # Default personality
        
        # Base prompt
        prompt_lines = [
            f"You are {agent_id}, a highly intelligent AI assistant.",
            f"Your personality traits: {', '.join([t.value for t in personality.traits])}.",
            f"Current user emotional state: {dominant_emotion}. Adjust your tone accordingly.",
            "",
            "Context from knowledge base:",
            "\n".join(context) if context else "No relevant context found",
            "",
            "Guidelines:",
            f"- Be {personality.base_tone}",
            "- Acknowledge user's emotional state if strong",
            "- Use context when relevant but don't force it",
            "- Keep responses concise but thorough when needed",
            "- Maintain natural conversation flow"
        ]
        
        # Emotion-specific adjustments
        if dominant_emotion == "sadness":
            prompt_lines.append("- Show extra empathy and support")
        elif dominant_emotion == "anger":
            prompt_lines.append("- Remain calm and solution-focused")
        elif dominant_emotion == "happiness":
            prompt_lines.append("- Match the positive energy but stay professional")
            
        return "\n".join(prompt_lines)
        
    async def text_to_speech(
        self,
        text: str,
        emotional_state: Optional[Dict[str, float]] = None
    ) -> bytes:
        """Convert text to speech with emotional inflection"""
        # TODO: Adjust TTS parameters based on emotional state
        return await self.voice.text_to_speech(text)
        
    async def get_conversation_history(self, user_id: str) -> List[Dict[str, Any]]:
        """Get conversation history for a user"""
        return self.conversation_memory.get(user_id, {}).get("history", [])
        
    async def clear_memory(self, user_id: str):
        """Clear conversation memory for a user"""
        if user_id in self.conversation_memory:
            del self.conversation_memory[user_id]
===== ./app/services/conversation_service.py =====
import logging
from typing import List, Dict, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc, delete
from app.models.db_models import DBConversation, DBMessage
from datetime import datetime
import uuid

logger = logging.getLogger(__name__)

class ConversationService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.memory_cache = {}
    
    async def get_conversation(self, agent_id: str, user_id: str) -> DBConversation:
        """Get or create a conversation between user and agent"""
        result = await self.db.execute(
            select(DBConversation)
            .where(
                DBConversation.agent_id == agent_id,
                DBConversation.user_id == user_id
            )
            .order_by(desc(DBConversation.updated_at))
            .limit(1)
        )
        conversation = result.scalars().first()
        
        if not conversation:
            conversation = DBConversation(
                id=str(uuid.uuid4()),
                agent_id=agent_id,
                user_id=user_id,
                memory_id=str(uuid.uuid4()),
                context={"messages": []},
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            self.db.add(conversation)
            await self.db.commit()
            await self.db.refresh(conversation)
        
        return conversation
    
    async def add_message(
        self,
        conversation_id: str,
        role: str,
        content: str,
        metadata: Optional[Dict] = None
    ) -> DBMessage:
        """Add a message to the conversation"""
        message = DBMessage(
            id=str(uuid.uuid4()),
            conversation_id=conversation_id,
            role=role,
            content=content,
            metadata=metadata or {},
            created_at=datetime.utcnow()
        )
        self.db.add(message)
        await self.db.commit()
        await self.db.refresh(message)
        return message
    
    async def get_conversation_history(
        self,
        user_id: str,
        agent_id: str,
        limit: int = 10
    ) -> List[Dict[str, str]]:
        """Get conversation history for context"""
        conversation = await self.get_conversation(agent_id, user_id)
        
        result = await self.db.execute(
            select(DBMessage)
            .where(DBMessage.conversation_id == conversation.id)
            .order_by(desc(DBMessage.created_at))
            .limit(limit)
        )
        
        messages = result.scalars().all()
        
        # Format for LLM: [{"role": "user", "content": "..."}, ...]
        history = []
        for msg in reversed(messages):  # Oldest first
            history.append({
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.created_at.isoformat()
            })
        
        return history
    
    async def save_interaction(
        self,
        user_id: str,
        agent_id: str,
        user_message: str,
        agent_response: str,
        metadata: Optional[Dict] = None
    ):
        """Save full interaction to database"""
        conversation = await self.get_conversation(agent_id, user_id)
        
        # Save user message
        await self.add_message(
            conversation_id=conversation.id,
            role="user",
            content=user_message,
            metadata=metadata
        )
        
        # Save agent response
        await self.add_message(
            conversation_id=conversation.id,
            role="agent",
            content=agent_response,
            metadata=metadata
        )
        
        # Update conversation timestamp
        conversation.updated_at = datetime.utcnow()
        await self.db.commit()
    
    async def clear_conversation(
        self,
        agent_id: str,
        user_id: str
    ) -> bool:
        """Clear conversation history for an agent-user pair"""
        try:
            # Get conversation
            conversation = await self.get_conversation(agent_id, user_id)
            
            # Delete all messages
            await self.db.execute(
                delete(DBMessage)
                .where(DBMessage.conversation_id == conversation.id)
            )
            
            # Reset conversation context
            conversation.context = {"messages": []}
            conversation.updated_at = datetime.utcnow()
            
            await self.db.commit()
            return True
        except Exception as e:
            logger.error(f"Error clearing conversation: {str(e)}")
            await self.db.rollback()
            return False
===== ./app/services/enhanced_agent_service.py =====
# ./app/services/enhanced_agent_service.py
from typing import Dict, Any, List, Optional
from enum import Enum
import asyncio
import time
from app.services.llm_service import OllamaService
from app.services.rag_service import RAGService

class ReasoningMode(str, Enum):
    FAST = "fast"
    BALANCED = "balanced" 
    DEEP = "deep"

class ResponseFormat(str, Enum):
    CONVERSATIONAL = "conversational"
    STRUCTURED = "structured"
    JSON = "json"

class EnhancedAgentExecutor:
    def __init__(self):
        self.llm_service = OllamaService()
        self.rag_service = RAGService()
    
    async def execute_with_reasoning(
        self,
        agent,
        input_text: str,
        parameters: Dict[str, Any],
        reasoning_mode: ReasoningMode = ReasoningMode.BALANCED
    ) -> Dict[str, Any]:
        """Execute agent with enhanced reasoning capabilities"""
        
        reasoning_steps = []
        start_time = time.time()
        
        try:
            # Step 1: Analyze intent and complexity
            if reasoning_mode == ReasoningMode.DEEP:
                intent_analysis = await self._analyze_intent(input_text)
                reasoning_steps.append(f"Intent Analysis: {intent_analysis}")
            
            # Step 2: Gather context (RAG, memory, etc.)
            context = await self._gather_context(agent, input_text, parameters)
            reasoning_steps.append(f"Context gathered: {len(context.get('documents', []))} relevant documents")
            
            # Step 3: Generate response with reasoning
            if reasoning_mode == ReasoningMode.DEEP:
                # Multi-step reasoning for complex queries
                response = await self._deep_reasoning_response(agent, input_text, context, parameters)
            elif reasoning_mode == ReasoningMode.FAST:
                # Single-step for quick responses
                response = await self._fast_response(agent, input_text, context, parameters)
            else:
                # Balanced approach
                response = await self._balanced_response(agent, input_text, context, parameters)
            
            execution_time = time.time() - start_time
            
            return {
                "response": response.get("content", ""),
                "reasoning_steps": reasoning_steps,
                "execution_time": execution_time,
                "sources": context.get("sources", []),
                "context_used": context.get("documents", [])[:3],  # Limit for response size
                "model": agent.model,
                "reasoning_mode": reasoning_mode.value
            }
            
        except Exception as e:
            logger.error(f"Enhanced execution failed: {str(e)}")
            return {
                "response": f"I encountered an error while processing your request: {str(e)}",
                "reasoning_steps": reasoning_steps,
                "execution_time": time.time() - start_time,
                "error": str(e)
            }
    
    async def _analyze_intent(self, input_text: str) -> str:
        """Analyze user intent for complex reasoning"""
        prompt = f"""Analyze the intent and complexity of this user request:
        
        User Input: {input_text}
        
        Provide a brief analysis of:
        1. Primary intent (question, task, creative request, etc.)
        2. Complexity level (simple, moderate, complex)
        3. Required capabilities (reasoning, search, creativity, etc.)
        
        Keep response concise (2-3 sentences)."""
        
        response = await self.llm_service.generate(
            prompt=prompt,
            options={"temperature": 0.3, "max_tokens": 150}
        )
        return response.get("response", "Unable to analyze intent")
    
    async def _gather_context(self, agent, input_text: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Gather relevant context from multiple sources"""
        context = {"documents": [], "sources": []}
        
        # RAG context if enabled
        if parameters.get("enable_rag", True):
            try:
                await self.rag_service.initialize()
                rag_results = await self.rag_service.query(
                    db=None,  # This needs to be passed properly
                    user_id=parameters.get("user_id"),
                    agent_id=agent.id,
                    query=input_text,
                    max_results=5
                )
                context["documents"] = rag_results.get("documents", [])
                context["sources"] = rag_results.get("sources", [])
            except Exception as e:
                logger.warning(f"RAG context gathering failed: {str(e)}")
        
        return context
    
    async def _deep_reasoning_response(self, agent, input_text: str, context: Dict, parameters: Dict) -> Dict[str, Any]:
        """Generate response with deep reasoning (chain of thought)"""
        
        # Build enhanced system prompt for deep reasoning
        system_prompt = f"""{agent.system_prompt}

You are now in DEEP REASONING mode. For complex queries:
1. Break down the problem into steps
2. Show your thinking process
3. Use the provided context when relevant
4. Provide a comprehensive, well-reasoned response

Context Information:
{chr(10).join(context.get('documents', [])[:3])}

Think step by step and show your reasoning."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Please think through this carefully: {input_text}"}
        ]
        
        response = await self.llm_service.chat(
            messages=messages,
            model=agent.model,
            options={
                "temperature": parameters.get("temperature", 0.7),
                "max_tokens": parameters.get("max_tokens", 3000),
            }
        )
        
        return response.get("message", {})
    
    async def _fast_response(self, agent, input_text: str, context: Dict, parameters: Dict) -> Dict[str, Any]:
        """Generate quick response for simple queries"""
        
        system_prompt = f"""{agent.system_prompt}

Provide a direct, concise response. Be helpful but brief."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_text}
        ]
        
        response = await self.llm_service.chat(
            messages=messages,
            model=agent.model,
            options={
                "temperature": parameters.get("temperature", 0.5),
                "max_tokens": min(parameters.get("max_tokens", 1000), 1000),
            }
        )
        
        return response.get("message", {})
    
    async def _balanced_response(self, agent, input_text: str, context: Dict, parameters: Dict) -> Dict[str, Any]:
        """Generate balanced response (default mode)"""
        
        context_text = ""
        if context.get("documents"):
            context_text = f"\n\nRelevant Context:\n{chr(10).join(context['documents'][:2])}"
        
        system_prompt = f"""{agent.system_prompt}

Use the following context if relevant to provide a helpful response.{context_text}"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_text}
        ]
        
        response = await self.llm_service.chat(
            messages=messages,
            model=agent.model,
            options={
                "temperature": parameters.get("temperature", 0.7),
                "max_tokens": parameters.get("max_tokens", 2000),
            }
        )
        
        return response.get("message", {})

# Global instance
enhanced_executor = EnhancedAgentExecutor()

===== ./app/services/training_service.py =====
import os
import asyncio
import aiofiles
import uuid
import logging
import requests
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import tempfile
import shutil
from pathlib import Path

from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobStatus,
    TrainingDataItem,
    TrainingDataType,
    TrainingProgress,
    FileUploadInfo
)
from app.utils.file_processing import (
    process_document,
    process_image,
    extract_text_from_url,
    chunk_text
)

logger = logging.getLogger(__name__)

class TrainingService:
    def __init__(self):
        self.jobs = {}
        self.executor = ThreadPoolExecutor(max_workers=10)
        # Create temporary directory for file uploads
        self.temp_dir = Path(tempfile.gettempdir()) / "training_uploads"  
        self.temp_dir.mkdir(exist_ok=True)
        
        # File storage with user isolation
        self.user_files = {}  # user_id -> {file_id: file_path}

    async def create_job(
        self,
        user_id: str,
        agent_id: str,
        data_urls: List[str] = None,
        training_data: List[TrainingDataItem] = None,
        text_data: List[str] = None,
        uploaded_files: List[FileUploadInfo] = None,
        config: Dict[str, Any] = None
    ) -> TrainingJobResponse:
        """Create a new enhanced training job"""
        job_id = str(uuid.uuid4())
        now = datetime.utcnow()
        
        # Convert legacy data_urls to training_data format for backward compatibility
        enhanced_training_data = training_data or []
        if data_urls:
            for url in data_urls:
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.URL,
                        content=url,
                        metadata={"source": "legacy_data_urls"}
                    )
                )
        
        # Add text data as training items
        if text_data:
            for i, text in enumerate(text_data):
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.TEXT,
                        content=text,
                        metadata={"index": i, "source": "text_input"}
                    )
                )
        
        job = TrainingJobResponse(
            id=job_id,
            user_id=user_id,
            agent_id=agent_id,
            data_urls=data_urls or [],
            training_data=enhanced_training_data,
            text_data=text_data or [],
            uploaded_files=uploaded_files or [],
            status=TrainingJobStatus.PENDING,
            created_at=now,
            updated_at=now,
            progress=0,
            current_step="Initializing",
            total_steps=self._calculate_total_steps(enhanced_training_data),
            result=None,
            processed_items=0,
            total_items=len(enhanced_training_data)
        )
        
        self.jobs[job_id] = job
        logger.info(f"Created job {job_id} with {len(enhanced_training_data)} training items")
        return job

    def _calculate_total_steps(self, training_data: List[TrainingDataItem]) -> int:
        """Calculate total processing steps based on data types"""
        steps = 1  # Initialization
        
        for item in training_data:
            if item.type == TrainingDataType.URL:
                steps += 2  # Download + Process
            elif item.type == TrainingDataType.FILE:
                steps += 2  # Read + Process
            elif item.type == TrainingDataType.IMAGE:
                steps += 2  # Read + OCR/Analysis
            else:  # TEXT
                steps += 1  # Process
        
        steps += 2  # Finalization + Model training
        return steps

    async def save_uploaded_file(
        self,
        user_id: str,
        file_id: str,
        filename: str,
        content: bytes
    ) -> str:
        """Save uploaded file to temporary storage"""
        try:
            # Create user-specific directory
            user_dir = self.temp_dir / user_id
            user_dir.mkdir(exist_ok=True)
            
            # Save file with unique name
            file_path = user_dir / f"{file_id}_{filename}"
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # Track file for cleanup
            if user_id not in self.user_files:
                self.user_files[user_id] = {}
            self.user_files[user_id][file_id] = str(file_path)
            
            logger.info(f"Saved file {filename} for user {user_id} as {file_id}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"Error saving file: {str(e)}")
            raise

    async def list_jobs(self, user_id: str, agent_id: str) -> List[TrainingJobResponse]:
        """List training jobs for a user and agent"""
        user_jobs = []
        for job in self.jobs.values():
            if job.user_id == user_id and job.agent_id == agent_id:
                user_jobs.append(job)
        
        # Sort by creation date, newest first
        user_jobs.sort(key=lambda x: x.created_at, reverse=True)
        return user_jobs

    async def get_job(self, user_id: str, job_id: str) -> Optional[TrainingJobResponse]:
        """Get a specific job for a user"""
        job = self.jobs.get(job_id)
        if job and job.user_id == user_id:
            return job
        return None

    async def get_progress(self, user_id: str, job_id: str) -> Optional[TrainingProgress]:
        """Get training progress for a job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return None
        
        return TrainingProgress(
            job_id=job.id,
            status=job.status,
            progress=job.progress,
            current_step=job.current_step,
            processed_items=job.processed_items,
            total_items=job.total_items,
            message=job.error_message if job.status == TrainingJobStatus.FAILED else None
        )

    async def cancel_job(self, user_id: str, job_id: str) -> bool:
        """Cancel a training job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return False
        
        if job.status in [TrainingJobStatus.PENDING, TrainingJobStatus.PROCESSING_FILES, 
                         TrainingJobStatus.EXTRACTING_CONTENT, TrainingJobStatus.RUNNING]:
            job.status = TrainingJobStatus.FAILED
            job.error_message = "Job cancelled by user"
            job.updated_at = datetime.utcnow()
            logger.info(f"Cancelled job {job_id} for user {user_id}")
            return True
        
        return False

    async def run_training(self, job_id: str, user_id: str):
        """Run enhanced training process in background"""
        if job_id not in self.jobs:
            logger.error(f"Job {job_id} not found")
            return
        
        job = self.jobs[job_id]
        if job.user_id != user_id:
            logger.error(f"User {user_id} not authorized for job {job_id}")
            return
        
        try:
            await self._execute_training_pipeline(job)
        except Exception as e:
            logger.error(f"Training failed for job {job_id}: {str(e)}")
            job.status = TrainingJobStatus.FAILED
            job.error_message = str(e)
            job.updated_at = datetime.utcnow()
        finally:
            # Cleanup uploaded files
            await self._cleanup_user_files(user_id)

    async def _cleanup_user_files(self, user_id: str):
        """Clean up uploaded files for a user"""
        try:
            if user_id in self.user_files:
                for file_id, file_path in self.user_files[user_id].items():
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                            logger.info(f"Cleaned up file: {file_path}")
                    except Exception as e:
                        logger.error(f"Error cleaning up file {file_path}: {str(e)}")
                
                # Remove user directory if empty
                user_dir = self.temp_dir / user_id
                if user_dir.exists() and not any(user_dir.iterdir()):
                    user_dir.rmdir()
                
                # Clear from tracking
                del self.user_files[user_id]
                
        except Exception as e:
            logger.error(f"Error during cleanup for user {user_id}: {str(e)}")

    async def _execute_training_pipeline(self, job: TrainingJobResponse):
        """Execute the complete training pipeline"""
        logger.info(f"Starting training pipeline for job {job.id}")
        
        job.status = TrainingJobStatus.PROCESSING_FILES
        job.current_step = "Processing input data"
        job.updated_at = datetime.utcnow()
        
        processed_content = []
        current_step = 0
        
        # Process each training data item
        for i, item in enumerate(job.training_data):
            try:
                job.current_step = f"Processing item {i+1}/{len(job.training_data)}: {item.type.value}"
                job.processed_items = i
                job.progress = int((current_step / job.total_steps) * 100)
                job.updated_at = datetime.utcnow()
                
                content = await self._process_training_item(item)
                if content:
                    processed_content.extend(content)
                
                current_step += 2 if item.type != TrainingDataType.TEXT else 1
                
            except Exception as e:
                logger.error(f"Error processing item {i}: {str(e)}")
                # Continue with other items instead of failing completely
                continue
        
        # Update progress
        job.status = TrainingJobStatus.EXTRACTING_CONTENT
        job.current_step = "Extracting and chunking content"
        job.progress = 80
        job.updated_at = datetime.utcnow()
        
        # Chunk all processed content
        all_chunks = []
        for content in processed_content:
            chunks = chunk_text(content)
            all_chunks.extend(chunks)
        
        current_step += 1
        
        # Simulate model training
        job.status = TrainingJobStatus.RUNNING
        job.current_step = "Training model"
        job.progress = 90
        job.updated_at = datetime.utcnow()
        
        # Simulate training time based on content size
        training_time = min(len(all_chunks) * 0.1, 10)  # Max 10 seconds
        await asyncio.sleep(training_time)
        
        # Complete the job
        job.status = TrainingJobStatus.COMPLETED
        job.current_step = "Training completed"
        job.progress = 100
        job.processed_items = len(job.training_data)
        job.result = {
            "total_content_items": len(processed_content),
            "total_chunks": len(all_chunks),
            "average_chunk_size": sum(len(chunk) for chunk in all_chunks) / len(all_chunks) if all_chunks else 0,
            "training_time_seconds": training_time,
            "data_types_processed": list(set(item.type.value for item in job.training_data)),
            "accuracy": 0.95,  # Simulated
            "loss": 0.15       # Simulated
        }
        job.updated_at = datetime.utcnow()
        
        logger.info(f"Training completed for job {job.id} with {len(all_chunks)} chunks")

    async def _process_training_item(self, item: TrainingDataItem) -> List[str]:
        """Process a single training data item"""
        try:
            if item.type == TrainingDataType.URL:
                return await self._process_url(item.content)
            elif item.type == TrainingDataType.FILE:
                return await self._process_file(item.content, item.metadata)
            elif item.type == TrainingDataType.IMAGE:
                return await self._process_image_file(item.content, item.metadata)
            elif item.type == TrainingDataType.TEXT:
                return [item.content]
            else:
                logger.warning(f"Unknown data type: {item.type}")
                return []
                
        except Exception as e:
            logger.error(f"Error processing {item.type} item: {str(e)}")
            return []

    async def _process_url(self, url: str) -> List[str]:
        """Process content from URL"""
        try:
            # Run in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor,
                extract_text_from_url,
                url
            )
            return [content] if content else []
        except Exception as e:
            logger.error(f"Error processing URL {url}: {str(e)}")
            return []

    async def _process_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process uploaded file"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read file content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process document
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_document,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {str(e)}")
            return []

    async def _process_image_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process image file with OCR"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"Image file not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read image content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process image
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_image,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing image {file_path}: {str(e)}")
            return []
===== ./app/services/rag_service.py =====
# app/services/rag_service.py
import logging
import os
import asyncio
import uuid
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from app.config import settings
from app.utils.file_processing import process_document
from app.services.llm_service import OllamaService
from sqlalchemy.ext.asyncio import AsyncSession
from app.utils.qdrant_async import get_async_qdrant_client
from qdrant_client.http import models as qdrant_models
from qdrant_client.http.models import Distance, VectorParams
from sentence_transformers import CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import _stop_words
import re
from datetime import datetime
from collections import defaultdict
import hashlib

logger = logging.getLogger(__name__)

class OllamaEmbeddingFunction:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self._embedding_dim = None
        self._model_checked = False
    
    async def _verify_embedding_model(self):
        """Verify the embedding model produces consistent dimensions"""
        if self._model_checked:
            return
            
        try:
            # Test with multiple inputs to verify consistency
            test_texts = ["test", "another test", "embedding verification"]
            embeddings = []
            
            for text in test_texts:
                embedding = await self.ollama.create_embedding(text)
                if not embedding:
                    raise ValueError(f"Empty embedding returned for text: {text}")
                embeddings.append(embedding)
            
            # Check all embeddings have same dimension
            dims = {len(e) for e in embeddings}
            if len(dims) != 1:
                raise ValueError(f"Inconsistent embedding dimensions: {dims}")
                
            self._embedding_dim = dims.pop()
            logger.info(f"Verified embedding dimension: {self._embedding_dim}")
            self._model_checked = True
            
        except Exception as e:
            logger.error(f"Embedding model verification failed: {str(e)}")
            raise

    async def _get_embedding_dimension(self) -> int:
        """Get the embedding dimension from the model"""
        if self._embedding_dim is None:
            await self._verify_embedding_model()
        return self._embedding_dim
    
    async def generate_embeddings(self, input: List[str]) -> List[List[float]]:
        """Generate embeddings with dimension validation"""
        if not input:
            return []
            
        embedding_dim = await self._get_embedding_dimension()
        embeddings = []
        
        for text in input:
            try:
                logger.debug(f"Generating embedding for text: {text[:100]}...")
                embedding = await self.ollama.create_embedding(text)
                
                if not embedding:
                    logger.warning(f"Empty embedding for text: {text[:50]}...")
                    embeddings.append([0.0] * embedding_dim)
                    continue
                    
                if hasattr(embedding, 'tolist'):
                    embedding = embedding.tolist()
                    
                # Validate dimension
                if len(embedding) != embedding_dim:
                    logger.warning(f"Embedding dimension mismatch: expected {embedding_dim}, got {len(embedding)}")
                    if len(embedding) > embedding_dim:
                        embedding = embedding[:embedding_dim]
                    else:
                        embedding.extend([0.0] * (embedding_dim - len(embedding)))
                
                # Normalize
                embedding_array = np.array(embedding)
                norm = np.linalg.norm(embedding_array)
                if norm > 0:
                    embedding = (embedding_array / norm).tolist()
                else:
                    logger.warning("Zero norm embedding detected")
                    
                embeddings.append(embedding)
                
            except Exception as e:
                logger.error(f"Embedding error: {str(e)}", exc_info=True)
                embeddings.append([0.0] * embedding_dim)
                
        return embeddings

class HybridRetriever:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words=list(_stop_words.ENGLISH_STOP_WORDS),
            ngram_range=(1, 2)
        )
        self.tfidf_matrix = None
        self.cross_encoder = None
        self._is_tfidf_trained = False
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize cross-encoder if enabled"""
        if settings.RERANKING_ENABLED:
            try:
                self.cross_encoder = CrossEncoder(settings.RERANKING_MODEL)
                logger.info(f"Initialized cross-encoder: {settings.RERANKING_MODEL}")
            except Exception as e:
                logger.error(f"Failed to initialize cross-encoder: {str(e)}")
                self.cross_encoder = None
    
    async def train_tfidf(self, corpus: List[str]):
        """Train TF-IDF vectorizer on a corpus of documents"""
        if not corpus:
            logger.warning("No documents provided for TF-IDF training")
            return
            
        try:
            logger.info(f"Training TF-IDF on {len(corpus)} documents...")
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
            self._is_tfidf_trained = True
            logger.info(f"TF-IDF vectorizer trained with {len(self.tfidf_vectorizer.get_feature_names_out())} features")
        except Exception as e:
            logger.error(f"Failed to train TF-IDF vectorizer: {str(e)}")
            self._is_tfidf_trained = False
    
    def get_sparse_embedding(self, text: str) -> Optional[List[float]]:
        """Generate sparse TF-IDF embedding with dimension validation"""
        if not self._is_tfidf_trained:
            return None
            
        try:
            vector = self.tfidf_vectorizer.transform([text])
            return vector.toarray()[0].tolist()
        except Exception as e:
            logger.error(f"Failed to generate sparse embedding: {str(e)}")
            return None

    async def rerank_results(
        self,
        query: str,
        documents: List[str],
        scores: List[float],
        top_k: int = 5
    ) -> Tuple[List[str], List[float]]:
        """Re-rank results using cross-encoder"""
        if not self.cross_encoder or not documents:
            return documents, scores
            
        if len(documents) != len(scores):
            logger.error(f"Mismatched documents ({len(documents)}) and scores ({len(scores)})")
            return documents, scores
            
        try:
            # Create query-document pairs for cross-encoder
            pairs = [(query, doc) for doc in documents]
            
            # Get scores from cross-encoder
            rerank_scores = self.cross_encoder.predict(pairs)
            
            # Combine with original scores
            combined_scores = [
                (settings.HYBRID_DENSE_WEIGHT * score) + 
                (settings.HYBRID_SPARSE_WEIGHT * rerank_score)
                for score, rerank_score in zip(scores, rerank_scores)
            ]
            
            # Sort documents by combined scores
            sorted_indices = np.argsort(combined_scores)[::-1]
            sorted_docs = [documents[i] for i in sorted_indices[:top_k]]
            sorted_scores = [combined_scores[i] for i in sorted_indices[:top_k]]
            
            return sorted_docs, sorted_scores
        except Exception as e:
            logger.error(f"Re-ranking failed: {str(e)}")
            return documents, scores

class QueryRewriter:
    @staticmethod
    async def expand_query(query: str, ollama_service: OllamaService) -> str:
        """Expand query using LLM to generate synonyms and related terms"""
        if not settings.QUERY_REWRITING_ENABLED:
            return query
            
        try:
            prompt = f"""Expand this search query with synonyms and related terms. 
Return only a comma-separated list of terms without any additional text.

Original query: {query}

Expanded terms:"""
            
            response = await ollama_service.chat(
                messages=[{"role": "user", "content": prompt}],
                model=settings.DEFAULT_OLLAMA_MODEL,
                temperature=0.7,
                max_tokens=100
            )
            
            expanded_terms = response.get("message", {}).get("content", "").strip()
            if expanded_terms:
                expanded_terms = re.sub(r'[^a-zA-Z0-9,\s]', '', expanded_terms)
                expanded_terms = ', '.join([term.strip() for term in expanded_terms.split(',') if term.strip()])
                return f"{query}, {expanded_terms}"
            return query
        except Exception as e:
            logger.error(f"Query expansion failed: {str(e)}")
            return query
    
    @staticmethod
    async def rewrite_query(query: str, ollama_service: OllamaService) -> str:
        """Rewrite query for better retrieval using LLM"""
        if not settings.QUERY_REWRITING_ENABLED:
            return query
            
        try:
            prompt = f"""Rewrite this search query to be more effective for document retrieval. 
Keep the original meaning but optimize for semantic search. 
Return only the rewritten query without any additional text.

Original query: {query}

Rewritten query:"""
            
            response = await ollama_service.chat(
                messages=[{"role": "user", "content": prompt}],
                model=settings.DEFAULT_OLLAMA_MODEL,
                temperature=0.3,
                max_tokens=100
            )
            
            rewritten = response.get("message", {}).get("content", "").strip()
            return rewritten if rewritten else query
        except Exception as e:
            logger.error(f"Query rewriting failed: {str(e)}")
            return query

class RAGService:
    def __init__(self):
        self.ollama = OllamaService()
        self.embedding_fn = OllamaEmbeddingFunction(self.ollama)
        self.hybrid_retriever = HybridRetriever(self.ollama)
        self.query_rewriter = QueryRewriter()
        self.collection_name = settings.QDRANT_COLLECTION_NAME
        self.client = None
        self._embedding_dim = None
        self._initialized = False
        self._lock = asyncio.Lock()

    async def initialize(self):
        """Initialize the Qdrant client and collection with proper dimension handling"""
        async with self._lock:
            if self._initialized:
                return

            try:
                self.client = await get_async_qdrant_client()
                
                # Verify embedding model first
                await self.embedding_fn._verify_embedding_model()
                self._embedding_dim = await self.embedding_fn._get_embedding_dimension()

                # Check if collection exists and has correct dimensions
                collection_exists = await self.client.collection_exists(self.collection_name)
                needs_recreation = False
                
                if collection_exists:
                    collection_info = await self.client.get_collection_info(self.collection_name)
                    if collection_info:
                        if collection_info.vector_size != self._embedding_dim:
                            logger.warning(
                                f"Collection dimension mismatch: "
                                f"expected {self._embedding_dim}, got {collection_info.vector_size}. "
                                "Recreating collection..."
                            )
                            needs_recreation = True
                    else:
                        logger.warning("Could not get collection info, assuming recreation needed")
                        needs_recreation = True

                if not collection_exists or needs_recreation:
                    await self.client.create_collection(
                        collection_name=self.collection_name,
                        vector_size=self._embedding_dim,
                        distance="Cosine",
                        recreate_if_exists=True
                    )
                    logger.info(f"Created collection {self.collection_name} with dimension {self._embedding_dim}")

                # Initialize TF-IDF with sample data
                sample_docs = [
                    "Artificial intelligence is transforming industries.",
                    "Machine learning models require large datasets.",
                    "Natural language processing enables text understanding.",
                    "Deep learning uses neural networks for pattern recognition."
                ]
                await self.hybrid_retriever.train_tfidf(sample_docs)

                self._initialized = True
                logger.info("RAGService initialized successfully")

            except Exception as e:
                logger.error(f"Initialization failed: {str(e)}", exc_info=True)
                raise

    async def _ensure_collection_exists(self):
        """Ensure collection exists with proper configuration"""
        if not self._initialized:
            await self.initialize()



    def _calculate_similarity_from_score(self, score: float) -> float:
        """Convert Qdrant cosine similarity score to similarity percentage"""
        return max(0.0, min(1.0, (score + 1) / 2)) 

    async def ingest_document(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        filename: str,
        content: bytes,
        chunk_size: int = None,
        chunk_overlap: int = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Ingest document into Qdrant with proper dimension handling"""
        await self._ensure_collection_exists()
        
        chunk_size = chunk_size or settings.DEFAULT_CHUNK_SIZE
        chunk_overlap = chunk_overlap or settings.DEFAULT_CHUNK_OVERLAP
        
        try:
            logger.info(f"Starting document ingestion for {filename}")
            text = process_document(filename, content)
            if not text:
                raise ValueError("No text extracted from document")
            
            # Generate document ID with hash for uniqueness
            doc_hash = hashlib.md5(text.encode()).hexdigest()[:8]
            doc_id = f"{user_id}_{agent_id}_{os.path.splitext(filename)[0]}_{doc_hash}"
            
            # Process metadata with agent_id
            processed_metadata = {
                "user_id": user_id,
                "agent_id": agent_id,
                "filename": filename,
                "doc_id": doc_id,
                "ingested_at": datetime.utcnow().isoformat(),
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap
            }
            if metadata:
                processed_metadata.update(metadata)
            
            # Chunk the document
            chunks = self._chunk_text_with_overlap(
                text, 
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
            
            if not chunks:
                raise ValueError("No valid chunks created from document")
            
            # Generate embeddings
            embeddings = await self.embedding_fn.generate_embeddings(chunks)
            
            # Verify embedding dimensions match collection
            collection_info = await self.client.get_collection_info(self.collection_name)
            if not collection_info:
                raise ValueError("Could not get collection information")
                
            for i, embedding in enumerate(embeddings):
                if len(embedding) != collection_info.vector_size:
                    raise ValueError(
                        f"Embedding dimension mismatch at chunk {i}: "
                        f"expected {collection_info.vector_size}, got {len(embedding)}"
                    )
            
            # Prepare points for upsert
            points = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_metadata = processed_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": i,
                    "text_length": len(chunk),
                    "created_at": datetime.utcnow().isoformat()
                })
                
                point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{doc_id}_{i}"))
                
                points.append(qdrant_models.PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "text": chunk,
                        **chunk_metadata
                    }
                ))
            
            # Upsert points
            success = await self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            if not success:
                raise Exception("Failed to upsert points to Qdrant")
            
            logger.info(f"Successfully ingested document {filename} with {len(chunks)} chunks")
            return doc_id
            
        except Exception as e:
            logger.error(f"Ingest failed: {str(e)}", exc_info=True)
            raise

    def _chunk_text_with_overlap(
        self, 
        text: str, 
        chunk_size: int, 
        chunk_overlap: int,
        separator: str = "\n\n"
    ) -> List[str]:
        """Improved text chunking with semantic boundaries"""
        if not text or not text.strip():
            return []
        
        if len(text) <= chunk_size:
            return [text.strip()]
        
        # First split by major sections
        sections = re.split(r'\n{2,}', text)
        chunks = []
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
                
            if len(section) <= chunk_size:
                chunks.append(section)
                continue
                
            # Then split by sentences
            sentences = re.split(r'(?<=[.!?])\s+', section)
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                if len(current_chunk) + len(sentence) + 1 > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk)
                        
                        # Keep overlap if specified
                        if chunk_overlap > 0:
                            overlap_start = max(0, len(current_chunk) - chunk_overlap)
                            overlap_text = current_chunk[overlap_start:]
                            current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                        else:
                            current_chunk = sentence
                    else:
                        # Sentence is too long, split by words
                        words = sentence.split()
                        current_word_chunk = []
                        
                        for word in words:
                            if len(" ".join(current_word_chunk + [word])) > chunk_size:
                                if current_word_chunk:
                                    chunks.append(" ".join(current_word_chunk))
                                    current_word_chunk = current_word_chunk[-chunk_overlap:] if chunk_overlap > 0 else []
                                current_word_chunk.append(word)
                            else:
                                current_word_chunk.append(word)
                                
                        if current_word_chunk:
                            current_chunk = " ".join(current_word_chunk)
                else:
                    current_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if current_chunk:
                chunks.append(current_chunk)
        
        # Post-processing to clean up chunks
        cleaned_chunks = []
        seen_hashes = set()
        
        for chunk in chunks:
            chunk = chunk.strip()
            if not chunk or len(chunk) < 50:  # Minimum chunk size
                continue
                
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash not in seen_hashes:
                cleaned_chunks.append(chunk)
                seen_hashes.add(chunk_hash)
        
        logger.info(f"Split text into {len(cleaned_chunks)} chunks with overlap")
        return cleaned_chunks

    async def query(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        query: str,
        max_results: int = 5,
        min_score: float = 0.3,
        filters: Optional[Dict[str, Any]] = None,
        rewrite_query: bool = True,
        use_reranking: bool = True,
        hybrid_search: bool = True
    ) -> Dict[str, Any]:
        """Enhanced query with hybrid search and query rewriting"""
        await self._ensure_collection_exists()
        
        try:
            # Validate input
            if not query or len(query.strip()) < 3:
                raise ValueError("Query must be at least 3 characters")
            
            max_results = min(max(1, max_results), 20)  # Clamp between 1-20
            min_score = max(0.0, min(1.0, min_score))  # Clamp between 0-1

            # Query processing
            processed_query = await self._process_query(query, rewrite_query)
            
            # Search execution with agent_id filter
            search_results = await self._execute_search(
                user_id, 
                agent_id,
                processed_query, 
                max_results * 3,  # Get more results for re-ranking
                min_score,
                filters,
                hybrid_search
            )
            
            if not search_results:
                return self._empty_response(query)
            
            # Re-ranking if enabled
            if use_reranking and settings.RERANKING_ENABLED:
                search_results = await self._rerank_results(processed_query, search_results, max_results)
            else:
                search_results = search_results[:max_results]
            
            # Response generation
            return await self._generate_response(query, processed_query, search_results, use_reranking, hybrid_search)
            
        except Exception as e:
            logger.error(f"RAG query failed: {str(e)}", exc_info=True)
            raise

    async def _process_query(self, query: str, rewrite: bool) -> str:
        """Process and optimize the query"""
        processed_query = query
        
        if rewrite and settings.QUERY_REWRITING_ENABLED:
            processed_query = await self.query_rewriter.rewrite_query(query, self.ollama)
            logger.info(f"Rewritten query: {processed_query}")
            
            if settings.QUERY_EXPANSION_ENABLED:
                expanded_query = await self.query_rewriter.expand_query(processed_query, self.ollama)
                if expanded_query != processed_query:
                    logger.info(f"Expanded query: {expanded_query}")
                    processed_query = expanded_query
        
        return processed_query

    async def _execute_search(
        self,
        user_id: str,
        agent_id: str,
        query: str,
        limit: int,
        min_score: float,
        filters: Optional[Dict[str, Any]],
        hybrid_search: bool
    ) -> List[qdrant_models.ScoredPoint]:
        """Execute the search with hybrid approach if enabled"""
        from app.utils.qdrant_async import create_user_filter
        
        # Get collection info for dimension verification
        collection_info = await self.client.get_collection_info(self.collection_name)
        if not collection_info:
            raise ValueError("Could not get collection information")

        # Generate dense embeddings
        query_embeddings = await self.embedding_fn.generate_embeddings([query])
        if not query_embeddings or not query_embeddings[0]:
            raise ValueError("Failed to generate query embedding")
        
        # Verify embedding dimension matches collection
        if len(query_embeddings[0]) != collection_info.vector_size:
            raise ValueError(
                f"Embedding dimension mismatch: query has {len(query_embeddings[0])}, "
                f"collection expects {collection_info.vector_size}"
            )

        # Add agent_id to filters
        if filters is None:
            filters = {}
        filters["agent_id"] = agent_id

        # Dense vector search
        dense_results = await self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embeddings[0],
            query_filter=create_user_filter(user_id, filters),
            limit=limit,
            score_threshold=min_score
        )
        
        # Early return if hybrid search is disabled
        if not hybrid_search or not settings.HYBRID_SEARCH_ENABLED:
            return dense_results
        
        # Sparse search (TF-IDF)
        sparse_embedding = self.hybrid_retriever.get_sparse_embedding(query)
        sparse_results = []
        if sparse_embedding:
            try:
                # Qdrant requires all vectors to have same dimension as collection
                # So we need to pad/truncate sparse embeddings to match
                sparse_embedding_adjusted = sparse_embedding.copy()
                if len(sparse_embedding_adjusted) > collection_info.vector_size:
                    sparse_embedding_adjusted = sparse_embedding_adjusted[:collection_info.vector_size]
                else:
                    sparse_embedding_adjusted.extend(
                        [0.0] * (collection_info.vector_size - len(sparse_embedding_adjusted))
                    )
                
                sparse_results = await self.client.search(
                    collection_name=self.collection_name,
                    query_vector=sparse_embedding_adjusted,
                    query_filter=create_user_filter(user_id, filters),
                    limit=limit,
                    score_threshold=min_score
                )
            except Exception as e:
                logger.error(f"Sparse search failed: {str(e)}")
                # Fall back to dense results only
                return dense_results
        
        return self._combine_results(dense_results, sparse_results)

    def _combine_results(
        self,
        dense_results: List[qdrant_models.ScoredPoint],
        sparse_results: List[qdrant_models.ScoredPoint]
    ) -> List[qdrant_models.ScoredPoint]:
        """Combine dense and sparse search results"""
        if not sparse_results or not settings.HYBRID_SEARCH_ENABLED:
            return dense_results
            
        # Create a map of unique documents
        results_map = {}
        
        # Process dense results
        for result in dense_results:
            doc_id = result.payload.get("doc_id", str(result.id))
            results_map[doc_id] = {
                "result": result,
                "dense_score": result.score,
                "sparse_score": 0.0
            }
        
        # Process sparse results
        for result in sparse_results:
            doc_id = result.payload.get("doc_id", str(result.id))
            if doc_id in results_map:
                results_map[doc_id]["sparse_score"] = result.score
            else:
                results_map[doc_id] = {
                    "result": result,
                    "dense_score": 0.0,
                    "sparse_score": result.score
                }
        
        # Calculate combined scores
        combined_results = []
        for doc_data in results_map.values():
            combined_score = (
                settings.HYBRID_DENSE_WEIGHT * doc_data["dense_score"] +
                settings.HYBRID_SPARSE_WEIGHT * doc_data["sparse_score"]
            )
            new_result = doc_data["result"]
            new_result.score = combined_score
            combined_results.append(new_result)
        
        # Sort by combined score
        combined_results.sort(key=lambda x: x.score, reverse=True)
        return combined_results

    async def _rerank_results(
        self,
        query: str,
        results: List[qdrant_models.ScoredPoint],
        top_k: int
    ) -> List[qdrant_models.ScoredPoint]:
        """Re-rank results using cross-encoder"""
        if not results or not settings.RERANKING_ENABLED:
            return results[:top_k]
            
        documents = [res.payload.get("text", "") for res in results]
        scores = [res.score for res in results]
        
        reranked_docs, reranked_scores = await self.hybrid_retriever.rerank_results(
            query,
            documents,
            scores,
            top_k
        )
        
        # Reconstruct results with new order
        reranked_results = []
        doc_set = set(reranked_docs)
        for doc, score in zip(reranked_docs, reranked_scores):
            for res in results:
                if res.payload.get("text", "") == doc and res not in reranked_results:
                    res.score = score  # Update score
                    reranked_results.append(res)
                    break
        
        return reranked_results[:top_k]

    async def _generate_response(
        self,
        original_query: str,
        processed_query: str,
        results: List[qdrant_models.ScoredPoint],
        use_reranking: bool,
        hybrid_search_used: bool
    ) -> Dict[str, Any]:
        """Generate final response with LLM"""
        if not results:
            return self._empty_response(original_query)
        
        # Extract documents and metadata
        documents = []
        metadatas = []
        for result in results:
            documents.append(result.payload.get("text", ""))
            metadatas.append(result.payload)
        
        # Build context
        context = "\n\n---\n\n".join([
            f"Source: {meta.get('filename', 'Unknown')} (Chunk {meta.get('chunk_index', 0) + 1})\n{doc}"
            for doc, meta in zip(documents, metadatas)
        ])
        
        # Generate answer with LLM
        system_prompt = f"""You are a helpful AI assistant. Use the following context to answer the user's question.
If you don't know the answer based on the provided context, say so honestly. Don't make up information.
Be comprehensive but concise in your response.

Context:
{context}

Instructions:
- Answer based only on the provided context
- If the context doesn't contain enough information, say so
- Cite relevant sources when appropriate
- Be helpful and informative"""

        llm_response = await self.ollama.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": original_query}  # Use original query here
            ]
        )
        
        # Prepare response
        sources = list(set(
            meta.get("filename", "Unknown") 
            for meta in metadatas 
            if meta and "filename" in meta
        ))
        
        return {
            "answer": llm_response.get("message", {}).get("content", "No response generated"),
            "documents": documents,
            "context": llm_response.get("context", []),
            "sources": sources,
            "debug_info": {
                "original_query": original_query,
                "processed_query": processed_query,
                "search_method": "hybrid" if hybrid_search_used else "dense",
                "reranking_applied": use_reranking,
                "total_results": len(results),
                "scores": [self._calculate_similarity_from_score(r.score) for r in results],
                "timestamp": datetime.utcnow().isoformat()
            }
        }

    def _empty_response(self, query: str) -> Dict[str, Any]:
        """Generate empty response structure"""
        return {
            "answer": f"No relevant documents found for your query: {query}",
            "documents": [],
            "context": [],
            "sources": [],
            "debug_info": {
                "original_query": query,
                "search_method": "none",
                "timestamp": datetime.utcnow().isoformat()
            }
        }

    async def list_documents(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        page: int = 1,
        per_page: int = 10
    ) -> List[Dict[str, Any]]:
        """List documents with pagination"""
        await self._ensure_collection_exists()
        
        try:
            if page < 1 or per_page < 1:
                raise ValueError("Page and per_page must be positive integers")

            from app.utils.qdrant_async import create_user_filter
            
            # Include agent_id in filter
            filters = {"agent_id": agent_id}
            
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id, filters),
                with_payload=True,
                limit=1000
            )
            
            if not records:
                return []

            unique_docs = {}
            for record in records:
                payload = record.payload
                if payload and "doc_id" in payload:
                    doc_id = payload["doc_id"]
                    if doc_id not in unique_docs:
                        unique_docs[doc_id] = {
                            "document_id": doc_id,
                            "filename": payload.get("filename", "unknown"),
                            "chunk_count": 0,
                            "total_text_length": 0,
                            "created_at": payload.get("created_at", 0)
                        }
                    
                    unique_docs[doc_id]["chunk_count"] += 1
                    unique_docs[doc_id]["total_text_length"] += len(payload.get("text", ""))

            doc_list = list(unique_docs.values())
            doc_list.sort(key=lambda x: x.get("created_at", 0), reverse=True)
            
            start = (page - 1) * per_page
            end = start + per_page
            return doc_list[start:end]
        except Exception as e:
            logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
            raise


    async def delete_document(
        self,
        db: AsyncSession,
        user_id: str,
        document_id: str
    ) -> bool:
        """Delete a document and all its chunks"""
        await self._ensure_collection_exists()
        
        try:
            from app.utils.qdrant_async import create_document_filter
            
            success = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=qdrant_models.FilterSelector(
                    filter=create_document_filter(user_id, document_id)
                )
            )
            
            logger.info(f"Successfully deleted document {document_id} for user {user_id}")
            return success
            
        except Exception as e:
            logger.error(f"Failed to delete document {document_id}: {str(e)}", exc_info=True)
            raise

    async def get_document_stats(self, db: AsyncSession, user_id: str) -> Dict[str, Any]:
        """Get statistics about user's documents"""
        await self._ensure_collection_exists()
        
        try:
            from app.utils.qdrant_async import create_user_filter
            
            total_chunks = await self.client.count_points(
                collection_name=self.collection_name,
                count_filter=create_user_filter(user_id)
            )
            
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id),
                with_payload=True,
                limit=1000
            )
            
            unique_docs = set()
            total_text_length = 0
            
            for record in records:
                payload = record.payload
                if payload:
                    unique_docs.add(payload.get("doc_id", ""))
                    total_text_length += len(payload.get("text", ""))
            
            return {
                "total_documents": len(unique_docs),
                "total_chunks": total_chunks,
                "total_text_length": total_text_length,
                "average_chunk_size": total_text_length // max(total_chunks, 1),
                "embedding_dimension": self._embedding_dim
            }
            
        except Exception as e:
            logger.error(f"Failed to get document stats: {str(e)}", exc_info=True)
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "total_text_length": 0,
                "average_chunk_size": 0,
                "embedding_dimension": self._embedding_dim or 384
            }


    async def _get_agent_collection_name(agent_id: str) -> str:
        return f"agent_{agent_id}_memory"

    async def store_agent_memory(
        self,
        agent_id: str,
        user_id: str,
        memory_text: str,
        memory_type: str = "conversation"
    ):
        """Store agent memory in Qdrant"""
        collection_name = await self._get_agent_collection_name(agent_id)
    
    # Create collection if not exists
        if not await self.client.collection_exists(collection_name):
            await self.client.create_collection(
                collection_name=collection_name,
                vector_size=self._embedding_dim,
                distance="Cosine"
            )
    
    # Store memory point
        embedding = await self.embedding_fn.generate_embeddings([memory_text])
        point_id = str(uuid.uuid4())
    
        await self.client.upsert(
            collection_name=collection_name,
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding[0],
                    payload={
                        "text": memory_text,
                        "type": memory_type,
                        "user_id": user_id,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                )
            ]
        )

    async def delete_agent_data(self, agent_id: str, user_id: str) -> bool:
        """Delete all data associated with an agent"""
        await self._ensure_collection_exists()
    
        try:
            from app.utils.qdrant_async import create_user_filter
        
        # Create filter for agent-specific data
            filters = {"agent_id": agent_id}
        
            success = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=qdrant_models.FilterSelector(
                    filter=create_user_filter(user_id, filters)
                )
            )
        
            if success:
                logger.info(f"Successfully deleted RAG data for agent {agent_id}")
            else:
                logger.warning(f"No RAG data found for agent {agent_id}")
            
            return success
        
        except Exception as e:
            logger.error(f"Failed to delete agent RAG data: {str(e)}")
        # Don't raise exception - allow agent deletion to continue
            return False


    async def close(self):
        """Clean up resources"""
        if self.client:
            await self.client.close()
        logger.info("RAG service closed successfully")
===== ./app/services/agent_service.py =====
# Updated agent_service.py
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.db_models import DBAgent
from app.models.agent_model import AgentCreate, AgentUpdate
from datetime import datetime
from sqlalchemy import select, update, delete
import httpx
import json
from app.dependencies import get_db

class AgentService:
    def __init__(self, db: AsyncSession = Depends(get_db)):
        self.db = db

    async def create_agent(self, owner_id: str, agent_data: AgentCreate) -> DBAgent:
        db_agent = DBAgent(
            owner_id=owner_id,
            name=agent_data.name,
            description=agent_data.description,
            model=agent_data.model,
            system_prompt=agent_data.system_prompt,
            is_public=agent_data.is_public,
            tools=agent_data.tools,
            agent_metadata=agent_data.metadata
        )
        self.db.add(db_agent)
        await self.db.commit()
        await self.db.refresh(db_agent)
        return db_agent

    async def get_agent(self, agent_id: str, owner_id: str) -> DBAgent:
        result = await self.db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        return result.scalars().first()

    async def list_agents(self, owner_id: str) -> list[DBAgent]:
        result = await self.db.execute(
            select(DBAgent).where(DBAgent.owner_id == owner_id)
        )
        return result.scalars().all()

    async def update_agent(self, agent_id: str, owner_id: str, update_data: AgentUpdate) -> DBAgent:
        update_dict = update_data.dict(exclude_unset=True)
        if not update_dict:
            return await self.get_agent(agent_id, owner_id)
            
        await self.db.execute(
            update(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
            .values(**update_dict)
        )
        await self.db.commit()
        return await self.get_agent(agent_id, owner_id)

    async def delete_agent(self, agent_id: str, owner_id: str) -> bool:
        result = await self.db.execute(
            delete(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        await self.db.commit()
        return result.rowcount > 0


# Standalone execute_agent function
async def execute_agent(agent_id: str, owner_id: str, input_data: dict):
    """
    Execute an agent with the given input data.
    This function should be implemented based on your specific agent execution logic.
    """
    from app.database import get_async_session
    
    # Get database session
    async with get_async_session() as db:
        agent_service = AgentService(db)
        
        # Get the agent
        agent = await agent_service.get_agent(agent_id, owner_id)
        if not agent:
            raise ValueError(f"Agent {agent_id} not found")
        
        # Extract message and parameters
        message = input_data.get("message", "")
        parameters = input_data.get("parameters", {})
        
        # Here you would implement your agent execution logic
        # This is a placeholder implementation
        try:
            # Example: Call to Ollama or your AI service
            response = await call_ai_service(
                model=agent.model,
                system_prompt=agent.system_prompt,
                message=message,
                parameters=parameters
            )
            
            return {
                "status": "success",
                "response": response,
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }


async def call_ai_service(model: str, system_prompt: str, message: str, parameters: dict):
    """
    Call your AI service (e.g., Ollama) to execute the agent.
    Replace this with your actual AI service integration.
    """
    # Example implementation for Ollama
    try:
        async with httpx.AsyncClient() as client:
            payload = {
                "model": model,
                "prompt": f"{system_prompt}\n\nUser: {message}",
                "stream": False,
                **parameters
            }
            
            response = await client.post(
                "http://localhost:11434/api/generate",  # Adjust URL as needed
                json=payload,
                timeout=30.0
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "No response generated")
            
    except Exception as e:
        raise Exception(f"AI service call failed: {str(e)}")
===== ./app/services/cache.py =====
import json
import logging
from typing import Any, Optional, Dict, List
from fastapi.encoders import jsonable_encoder
from redis.asyncio import Redis
from app.config import settings
import asyncio
import time

logger = logging.getLogger(__name__)

class CacheService:
    def __init__(self):
        self.redis: Optional[Redis] = None
        self.enabled = settings.CACHE_ENABLED
        self._connection_attempted = False
        self._connection_lock = asyncio.Lock()
        
    async def connect(self):
        """Initialize Redis connection with proper error handling"""
        if not self.enabled or self._connection_attempted:
            return
            
        async with self._connection_lock:
            if self._connection_attempted:
                return
                
            self._connection_attempted = True
            
            try:
                self.redis = Redis.from_url(
                    str(settings.REDIS_URL),
                    encoding="utf-8",
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30,
                    max_connections=20
                )
                
                # Test connection
                await self.redis.ping()
                logger.info("‚úÖ Connected to Redis successfully")
                self.enabled = True
                
            except Exception as e:
                logger.warning(f"‚ùå Redis connection failed, caching disabled: {str(e)}")
                self.enabled = False
                self.redis = None
    
    async def disconnect(self):
        """Clean disconnect from Redis"""
        if self.redis:
            try:
                await self.redis.close()
                logger.info("Redis connection closed gracefully")
            except Exception as e:
                logger.error(f"Error closing Redis connection: {str(e)}")
            finally:
                self.redis = None
                self._connection_attempted = False
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache with automatic JSON decoding"""
        if not self.enabled or not self.redis:
            return None
            
        try:
            data = await self.redis.get(key)
            if data is None:
                return None
                
            # Try to parse as JSON first
            try:
                return json.loads(data)
            except json.JSONDecodeError:
                # Return as string if not valid JSON
                return data
                
        except Exception as e:
            logger.warning(f"Cache get error for key '{key}': {str(e)}")
            return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in cache with automatic JSON encoding"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            ttl = ttl or getattr(settings, 'CACHE_TTL', 300)
            
            # Handle different value types
            if isinstance(value, (str, bytes)):
                serialized_value = value
            else:
                try:
                    # Use jsonable_encoder for complex objects
                    serializable_value = jsonable_encoder(value)
                    serialized_value = json.dumps(serializable_value, default=str, ensure_ascii=False)
                except (TypeError, ValueError) as e:
                    logger.warning(f"Failed to serialize value for key '{key}': {str(e)}")
                    return False
            
            await self.redis.set(key, serialized_value, ex=ttl)
            return True
            
        except Exception as e:
            logger.warning(f"Cache set error for key '{key}': {str(e)}")
            return False
    
    async def delete(self, key: str) -> bool:
        """Delete key from cache"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.delete(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache delete error for key '{key}': {str(e)}")
            return False
    
    async def exists(self, key: str) -> bool:
        """Check if key exists in cache"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.exists(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache exists error for key '{key}': {str(e)}")
            return False
    
    async def expire(self, key: str, ttl: int) -> bool:
        """Set expiration time for existing key"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.expire(key, ttl)
            return result
        except Exception as e:
            logger.warning(f"Cache expire error for key '{key}': {str(e)}")
            return False
    
    async def clear_pattern(self, pattern: str) -> int:
        """Clear all keys matching a pattern"""
        if not self.enabled or not self.redis:
            return 0
            
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                return await self.redis.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"Cache clear pattern error for pattern '{pattern}': {str(e)}")
            return 0
    
    async def get_ttl(self, key: str) -> Optional[int]:
        """Get TTL for a key"""
        if not self.enabled or not self.redis:
            return None
            
        try:
            ttl = await self.redis.ttl(key)
            return ttl if ttl > 0 else None
        except Exception as e:
            logger.warning(f"Cache TTL error for key '{key}': {str(e)}")
            return None
    
    # JSON-specific methods with better error handling
    async def set_json(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set JSON data with proper serialization"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            # Use FastAPI's jsonable_encoder for consistent serialization
            serializable_value = jsonable_encoder(value)
            json_string = json.dumps(serializable_value, default=str, ensure_ascii=False)
            
            ttl = ttl or getattr(settings, 'CACHE_TTL', 300)
            await self.redis.set(key, json_string, ex=ttl)
            return True
        except (TypeError, ValueError) as e:
            logger.warning(f"JSON serialization failed for key '{key}': {str(e)}")
            return False
        except Exception as e:
            logger.warning(f"Cache set JSON error for key '{key}': {str(e)}")
            return False
    
    async def get_json(self, key: str) -> Optional[Any]:
        """Get JSON data with proper deserialization"""
        if not self.enabled or not self.redis:
            return None
            
        try:
            data = await self.redis.get(key)
            if data is None:
                return None
            return json.loads(data)
        except json.JSONDecodeError as e:
            logger.warning(f"JSON deserialization failed for key '{key}': {str(e)}")
            return None
        except Exception as e:
            logger.warning(f"Cache get JSON error for key '{key}': {str(e)}")
            return None
    
    # Chat history specific methods
    async def get_chat_history(self, user_id: str, agent_id: str) -> Optional[List[Dict[str, Any]]]:
        """Get conversation history for user and agent"""
        key = f"conversation:{user_id}:{agent_id}"
        return await self.get_json(key) or []
    
    async def save_chat_history(
        self, 
        user_id: str, 
        agent_id: str, 
        history: List[Dict[str, Any]], 
        ttl: Optional[int] = None
    ) -> bool:
        """Save conversation history to cache"""
        key = f"conversation:{user_id}:{agent_id}"
        ttl = ttl or 86400  # 24 hours default
        return await self.set_json(key, history, ttl)
    
    async def clear_chat_history(self, user_id: str, agent_id: str) -> bool:
        """Clear conversation history for user and agent"""
        key = f"conversation:{user_id}:{agent_id}"
        return await self.delete(key)
    
    async def append_to_chat_history(
        self,
        user_id: str,
        agent_id: str,
        new_messages: List[Dict[str, Any]],
        max_history: int = 20
    ) -> bool:
        """Append messages to chat history with size limiting"""
        try:
            # Get existing history
            history = await self.get_chat_history(user_id, agent_id)
            
            # Append new messages
            history.extend(new_messages)
            
            # Keep only recent messages
            if len(history) > max_history:
                history = history[-max_history:]
            
            # Save updated history
            return await self.save_chat_history(user_id, agent_id, history)
            
        except Exception as e:
            logger.error(f"Error appending to chat history: {str(e)}")
            return False
    
    # Agent-specific caching
    async def cache_agent_response(
        self,
        agent_id: str,
        user_id: str,
        query_hash: str,
        response: str,
        ttl: int = 1800  # 30 minutes
    ) -> bool:
        """Cache agent response for similar queries"""
        key = f"agent_response:{agent_id}:{user_id}:{query_hash}"
        return await self.set(key, response, ttl)
    
    async def get_cached_agent_response(
        self,
        agent_id: str,
        user_id: str,
        query_hash: str
    ) -> Optional[str]:
        """Get cached agent response"""
        key = f"agent_response:{agent_id}:{user_id}:{query_hash}"
        return await self.get(key)
    
    # System-wide cache operations
    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on cache service"""
        if not self.enabled or not self.redis:
            return {
                "status": "disabled",
                "enabled": False,
                "redis_connected": False
            }
            
        try:
            # Test basic operations
            start_time = time.time()
            await self.redis.ping()
            ping_time = (time.time() - start_time) * 1000
            
            # Get some stats
            info = await self.redis.info()
            
            return {
                "status": "healthy",
                "enabled": True,
                "redis_connected": True,
                "ping_time_ms": round(ping_time, 2),
                "redis_version": info.get("redis_version", "unknown"),
                "connected_clients": info.get("connected_clients", 0),
                "used_memory_human": info.get("used_memory_human", "unknown")
            }
            
        except Exception as e:
            logger.error(f"Cache health check failed: {str(e)}")
            return {
                "status": "unhealthy",
                "enabled": self.enabled,
                "redis_connected": False,
                "error": str(e)
            }
    
    async def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        if not self.enabled or not self.redis:
            return {"cache_enabled": False}
            
        try:
            info = await self.redis.info()
            return {
                "cache_enabled": True,
                "total_connections_received": info.get("total_connections_received", 0),
                "total_commands_processed": info.get("total_commands_processed", 0),
                "keyspace_hits": info.get("keyspace_hits", 0),
                "keyspace_misses": info.get("keyspace_misses", 0),
                "hit_rate": (
                    info.get("keyspace_hits", 0) / 
                    max(info.get("keyspace_hits", 0) + info.get("keyspace_misses", 0), 1)
                ) * 100,
                "used_memory": info.get("used_memory", 0),
                "used_memory_human": info.get("used_memory_human", "0B")
            }
        except Exception as e:
            logger.error(f"Error getting cache stats: {str(e)}")
            return {"cache_enabled": True, "error": str(e)}

# Global cache service instance
cache_service = CacheService()

# Utility functions for common cache patterns
async def cache_with_key(key: str, fetch_func, ttl: int = 300):
    """Generic cache-aside pattern implementation"""
    # Try to get from cache first
    cached_value = await cache_service.get(key)
    if cached_value is not None:
        return cached_value
    
    # Fetch from source
    try:
        value = await fetch_func() if asyncio.iscoroutinefunction(fetch_func) else fetch_func()
        # Cache the result
        await cache_service.set(key, value, ttl)
        return value
    except Exception as e:
        logger.error(f"Error in cache_with_key for '{key}': {str(e)}")
        raise

def generate_cache_key(*parts: str) -> str:
    """Generate a standardized cache key from parts"""
    return ":".join(str(part) for part in parts if part)

===== ./app/services/__init__.py =====
# Empty file to make services a package
===== ./app/services/llm_service.py =====
# app/services/llm_service.py
import requests
import logging
from typing import Optional, Dict, Any, List
from app.config import settings
import asyncio
import aiohttp
import hashlib
import numpy as np
import json

logger = logging.getLogger(__name__)

class OllamaService:
    def __init__(self):
        self.base_url = str(settings.OLLAMA_URL).rstrip('/')
        self.default_model = settings.DEFAULT_OLLAMA_MODEL
        self.timeout = settings.OLLAMA_TIMEOUT
        # Use the main model for embeddings if no specific embedding model
        self.embedding_model = getattr(settings, 'EMBEDDING_MODEL', 'deepseek-r1:1.5b')

    async def _make_request_async(self, endpoint: str, payload: dict) -> dict:
        """Make async request to Ollama with proper error handling"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making async request to: {url}")
            logger.debug(f"Payload: {json.dumps(payload, indent=2)}")
        
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            async with aiohttp.ClientSession(
                timeout=timeout,
                connector=aiohttp.TCPConnector(limit=10)
            ) as session:
                try:
                    async with session.post(
                        url,
                        json=payload,
                        headers={
                            'Content-Type': 'application/json',
                            'Accept': 'application/json'
                        }
                    ) as response:
                        logger.debug(f"Response status: {response.status}")
                        
                        if response.status == 404:
                            # Check if it's really a 404 or a model not found issue
                            error_text = await response.text()
                            if "model" in error_text.lower() and "not found" in error_text.lower():
                                raise ValueError(f"Model '{payload.get('model')}' not found. Please pull the model first.")
                            else:
                                raise ValueError(f"Ollama endpoint not found at {url}. Is Ollama running and accessible?")
                        
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"HTTP {response.status}: {error_text}")
                            raise ValueError(f"HTTP {response.status}: {error_text}")
                        
                        response_text = await response.text()
                        logger.debug(f"Response text: {response_text[:500]}...")
                        
                        try:
                            return json.loads(response_text)
                        except json.JSONDecodeError as e:
                            logger.error(f"Failed to parse JSON response: {response_text}")
                            raise ValueError(f"Invalid JSON response from Ollama: {str(e)}")
                    
                except aiohttp.ClientConnectorError as e:
                    logger.error(f"Connection error: {str(e)}")
                    raise ConnectionError(f"Could not connect to Ollama at {url}. Please ensure Ollama is running and accessible.")
                except asyncio.TimeoutError:
                    logger.error(f"Request timeout after {self.timeout} seconds")
                    raise TimeoutError(f"Request to Ollama timed out after {self.timeout} seconds")
                
        except Exception as e:
            logger.error(f"Error calling Ollama async: {str(e)}")
            raise RuntimeError(f"Failed to communicate with Ollama: {str(e)}")

    def _make_request_sync_wrapper(self, endpoint: str, payload: dict) -> dict:
        """Synchronous wrapper for use with run_in_executor"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making sync request to: {url}")
            logger.debug(f"Payload: {json.dumps(payload, indent=2)}")
            
            response = requests.post(
                url,
                json=payload,
                headers={
                    'Content-Type': 'application/json',
                    'Accept': 'application/json'
                },
                timeout=self.timeout
            )
            
            logger.debug(f"Response status: {response.status_code}")
            logger.debug(f"Response text: {response.text[:500]}...")
            
            if response.status_code == 404:
                # Check if it's really a 404 or a model not found issue
                if "model" in response.text.lower() and "not found" in response.text.lower():
                    raise ValueError(f"Model '{payload.get('model')}' not found. Please pull the model first.")
                else:
                    raise ValueError(f"Ollama endpoint not found at {url}. Is Ollama running and accessible?")
            
            response.raise_for_status()
            
            try:
                return response.json()
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON response: {response.text}")
                raise ValueError(f"Invalid JSON response from Ollama: {str(e)}")
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Connection error: {str(e)}")
            raise ConnectionError(f"Could not connect to Ollama at {url}. Please ensure Ollama is running and accessible.")
        except requests.exceptions.Timeout:
            logger.error(f"Request timeout after {self.timeout} seconds")
            raise TimeoutError(f"Request to Ollama timed out after {self.timeout} seconds")
        except Exception as e:
            logger.error(f"Error calling Ollama sync: {str(e)}")
            raise RuntimeError(f"Failed to communicate with Ollama: {str(e)}")

    async def check_ollama_status(self) -> Dict[str, Any]:
        """Check if Ollama is running and accessible"""
        try:
            # Try to get the list of models first
            models = await self.list_models()
            
            # Check if our models are available
            model_names = [model.get('name', '') for model in models]
            
            return {
                "status": "healthy",
                "available_models": model_names,
                "default_model_available": self.default_model in model_names,
                "embedding_model_available": self.embedding_model in model_names,
                "base_url": self.base_url
            }
            
        except Exception as e:
            logger.error(f"Ollama health check failed: {str(e)}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "base_url": self.base_url
            }

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None,
        system: Optional[str] = None,
        template: Optional[str] = None,
        context: Optional[list] = None
    ) -> Dict[str, Any]:
        """Generate text using Ollama with proper payload structure"""
        payload = {
            "model": model or self.default_model,
            "prompt": prompt,
            "stream": False,
            "options": options or {},
        }
        
        # Add optional parameters only if they exist
        if system: 
            payload["system"] = system
        if template: 
            payload["template"] = template
        if context: 
            payload["context"] = context
        
        # First check if the model is available
        if not await self.check_model_availability(payload["model"]):
            logger.warning(f"Model {payload['model']} not available, attempting to pull...")
            success = await self.pull_model(payload["model"])
            if not success:
                raise ValueError(f"Model {payload['model']} is not available and could not be pulled")
        
        # Try async first, then fallback to sync
        try:
            logger.info(f"Attempting async generate request with model: {payload['model']}")
            return await self._make_request_async("/api/generate", payload)
        except Exception as e:
            logger.warning(f"Async generate request failed: {str(e)}")
            logger.info("Falling back to sync request")
            try:
                return await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self._make_request_sync_wrapper("/api/generate", payload)
                )
            except Exception as sync_e:
                logger.error(f"Both async and sync generate requests failed. Async: {str(e)}, Sync: {str(sync_e)}")
                raise RuntimeError(f"All generate request methods failed. Last error: {str(sync_e)}")

    async def chat(
        self,
        messages: list,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Chat with Ollama using proper message format"""
        payload = {
            "model": model or self.default_model,
            "messages": messages,
            "stream": False,
            "options": options or {},
        }
        
        # First check if the model is available
        if not await self.check_model_availability(payload["model"]):
            logger.warning(f"Model {payload['model']} not available, attempting to pull...")
            success = await self.pull_model(payload["model"])
            if not success:
                raise ValueError(f"Model {payload['model']} is not available and could not be pulled")
        
        # Try async first, then fallback to sync
        try:
            logger.info(f"Attempting async chat request with model: {payload['model']}")
            return await self._make_request_async("/api/chat", payload)
        except Exception as e:
            logger.warning(f"Async chat request failed: {str(e)}")
            logger.info("Falling back to sync request")
            try:
                return await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self._make_request_sync_wrapper("/api/chat", payload)
                )
            except Exception as sync_e:
                logger.error(f"Both async and sync chat requests failed. Async: {str(e)}, Sync: {str(sync_e)}")
                raise RuntimeError(f"All chat request methods failed. Last error: {str(sync_e)}")

    async def list_models(self) -> list:
        """List available models with improved error handling"""
        try:
            # Try async first
            try:
                url = f"{self.base_url}/api/tags"
                timeout = aiohttp.ClientTimeout(total=10)  # Shorter timeout for listing
                
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"Failed to list models: HTTP {response.status}: {error_text}")
                            raise ValueError(f"HTTP {response.status}: {error_text}")
                        
                        result = await response.json()
                        models = result.get("models", [])
                        logger.info(f"Found {len(models)} models")
                        return models
                        
            except Exception as async_error:
                logger.warning(f"Async model listing failed: {str(async_error)}")
                # Fallback to sync
                response = requests.get(
                    f"{self.base_url}/api/tags",
                    timeout=10,
                    headers={'Accept': 'application/json'}
                )
                response.raise_for_status()
                result = response.json()
                models = result.get("models", [])
                logger.info(f"Found {len(models)} models (sync)")
                return models
                
        except Exception as e:
            logger.error(f"Error listing models: {str(e)}")
            return []

    async def create_embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> List[float]:
        """Create embeddings with multiple fallback strategies"""
        target_model = model or self.embedding_model
        
        # Strategy 1: Try the embeddings endpoint
        try:
            return await self._try_embeddings_endpoint(text, target_model)
        except Exception as e:
            logger.warning(f"Embeddings endpoint failed: {str(e)}")
        
        # Strategy 2: Try using the main model for embeddings if it's different
        if target_model != self.default_model:
            try:
                logger.info(f"Trying main model {self.default_model} for embeddings")
                return await self._try_embeddings_endpoint(text, self.default_model)
            except Exception as e:
                logger.warning(f"Main model embedding failed: {str(e)}")
        
        # Strategy 3: Try generate endpoint with special prompt
        try:
            return await self._create_embedding_via_generate(text, target_model)
        except Exception as e:
            logger.warning(f"Generate-based embedding failed: {str(e)}")
        
        # Strategy 4: Fallback to deterministic hash-based embedding
        logger.warning("All embedding methods failed, using deterministic fallback")
        return self._create_deterministic_embedding(text)

    async def _try_embeddings_endpoint(self, text: str, model: str) -> List[float]:
        """Try the official embeddings endpoint with multiple payload formats"""
        
        # Try different payload formats
        payload_formats = [
            {"model": model, "prompt": text},
            {"model": model, "input": text},  
            {"name": model, "prompt": text},
            {"name": model, "input": text}
        ]
        
        endpoints = ["/api/embeddings", "/api/embed"]
        
        for endpoint in endpoints:
            for payload in payload_formats:
                try:
                    logger.debug(f"Trying {endpoint} with payload format: {list(payload.keys())}")
                    result = await self._make_request_async(endpoint, payload)
                    
                    # Handle different response formats
                    if 'embeddings' in result:
                        embeddings = result['embeddings']
                        if embeddings and len(embeddings) > 0:
                            return embeddings[0] if isinstance(embeddings[0], list) else embeddings
                    elif 'embedding' in result:
                        return result['embedding']
                    elif 'data' in result:
                        # OpenAI-style response
                        return result['data'][0]['embedding']
                    
                except Exception as e:
                    logger.debug(f"Failed {endpoint} with {list(payload.keys())}: {str(e)}")
                    continue
        
        raise ValueError("No valid embeddings endpoint found")

    async def _create_embedding_via_generate(self, text: str, model: str) -> List[float]:
        """Create embedding using generate endpoint with special prompt"""
        
        # Try a simple approach that might work with some models
        prompt = f"Embed: {text}"
        
        response = await self.generate(
            prompt=prompt,
            model=model,
            options={
                "temperature": 0.0,
                "num_predict": 1,  # Minimal generation
                "stop": ["\n", ".", "!"]
            }
        )
        
        # This is a fallback - extract features from the response
        response_text = response.get("response", "")
        
        # Create a more sophisticated embedding from the response
        return self._text_to_embedding(text + " " + response_text)

    def _create_deterministic_embedding(self, text: str) -> List[float]:
        """Create a deterministic embedding from text using multiple hash functions"""
        
        # Use multiple hash functions for better distribution
        hash_functions = [
            lambda x: hashlib.md5(x.encode()).hexdigest(),
            lambda x: hashlib.sha1(x.encode()).hexdigest(),
            lambda x: hashlib.sha256(x.encode()).hexdigest(),
        ]
        
        # Target dimension
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        embedding = []
        
        # Generate embedding using multiple hash functions
        for i, hash_func in enumerate(hash_functions):
            hash_hex = hash_func(f"{text}_{i}")
            
            # Convert hex to floats
            for j in range(0, len(hash_hex), 2):
                if len(embedding) >= target_dim:
                    break
                val = int(hash_hex[j:j+2], 16) / 255.0  # Normalize to 0-1
                embedding.append(val)
            
            if len(embedding) >= target_dim:
                break
        
        # Pad or truncate to desired dimension
        if len(embedding) < target_dim:
            embedding.extend([0.1] * (target_dim - len(embedding)))
        else:
            embedding = embedding[:target_dim]
        
        # Add some text-based features
        text_features = [
            len(text) / 1000.0,  # Text length feature
            text.count(' ') / len(text) if text else 0,  # Word density
            text.count('.') / len(text) if text else 0,  # Sentence density
        ]
        
        # Replace some values with text features
        for i, feature in enumerate(text_features):
            if i < len(embedding):
                embedding[i] = feature
        
        # Normalize the embedding
        embedding_array = np.array(embedding)
        norm = np.linalg.norm(embedding_array)
        if norm > 0:
            embedding = (embedding_array / norm).tolist()
        
        logger.info(f"Created deterministic embedding with dimension {len(embedding)}")
        return embedding

    def _text_to_embedding(self, text: str) -> List[float]:
        """Convert text to embedding using various text features"""
        
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        
        # Extract various text features
        features = []
        
        # Character-level features
        for i in range(min(len(text), 50)):
            features.append(ord(text[i]) / 255.0)
        
        # Word-level features
        words = text.lower().split()
        for word in words[:20]:  # First 20 words
            word_hash = hash(word) % 1000
            features.append(word_hash / 1000.0)
        
        # Statistical features
        features.extend([
            len(text) / 1000.0,
            len(words) / 100.0 if words else 0,
            text.count(' ') / len(text) if text else 0,
            text.count('.') / len(text) if text else 0,
            text.count(',') / len(text) if text else 0,
        ])
        
        # Pad or truncate
        if len(features) < target_dim:
            features.extend([0.1] * (target_dim - len(features)))
        else:
            features = features[:target_dim]
        
        # Normalize
        features_array = np.array(features)
        norm = np.linalg.norm(features_array)
        if norm > 0:
            features = (features_array / norm).tolist()
        
        return features

    async def check_model_availability(self, model_name: str) -> bool:
        """Check if a specific model is available"""
        try:
            models = await self.list_models()
            available_models = [model.get('name', '') for model in models]
            is_available = model_name in available_models
            logger.info(f"Model {model_name} availability: {is_available}")
            if not is_available:
                logger.info(f"Available models: {available_models}")
            return is_available
        except Exception as e:
            logger.error(f"Error checking model availability: {str(e)}")
            return False

    async def pull_model(self, model_name: str) -> bool:
        """Pull a model if it's not available"""
        try:
            payload = {"name": model_name}
            
            logger.info(f"Attempting to pull model: {model_name}")
            
            # Use longer timeout for model pulling
            try:
                timeout = aiohttp.ClientTimeout(total=300)  # 5 minutes
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.post(
                        f"{self.base_url}/api/pull",
                        json=payload
                    ) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"Failed to pull model: HTTP {response.status}: {error_text}")
                            return False
                        
                        logger.info(f"Successfully pulled model: {model_name}")
                        return True
            except Exception as async_error:
                logger.warning(f"Async pull failed: {str(async_error)}")
                # Fallback to sync
                response = requests.post(
                    f"{self.base_url}/api/pull",
                    json=payload,
                    timeout=300
                )
                if response.status_code != 200:
                    logger.error(f"Failed to pull model: HTTP {response.status_code}: {response.text}")
                    return False
                
                logger.info(f"Successfully pulled model: {model_name}")
                return True
                
        except Exception as e:
            logger.error(f"Error pulling model {model_name}: {str(e)}")
            return False

    async def ensure_embedding_model(self) -> bool:
        """Ensure embedding model is available"""
        try:
            # Check if current embedding model is available
            if await self.check_model_availability(self.embedding_model):
                logger.info(f"Embedding model {self.embedding_model} is available")
                return True
            
            # If not available and it's different from default model, try default model
            if self.embedding_model != self.default_model:
                if await self.check_model_availability(self.default_model):
                    logger.info(f"Using default model {self.default_model} for embeddings")
                    self.embedding_model = self.default_model
                    return True
            
            # Try to pull the embedding model
            logger.info(f"Embedding model {self.embedding_model} not found, attempting to pull...")
            success = await self.pull_model(self.embedding_model)
            
            if not success and self.embedding_model != self.default_model:
                logger.info(f"Failed to pull {self.embedding_model}, falling back to {self.default_model}")
                self.embedding_model = self.default_model
                return await self.check_model_availability(self.default_model)
            
            return success
            
        except Exception as e:
            logger.error(f"Error ensuring embedding model: {str(e)}")
            return False
===== ./app/services/ultra_fast_voice_service.py =====
# app/services/ultra_fast_voice_service.py - Optimized voice processing
import asyncio
import time
import logging
import base64
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class UltraFastVoiceService:
    def __init__(self):
        self.initialized = False
        self.processing_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "average_processing_time": 0.0
        }
        
    async def initialize(self):
        """Initialize the voice service"""
        self.initialized = True
        logger.info("Ultra Fast Voice Service initialized")

    async def initialize_session(self, session):
        """Initialize session-specific settings"""
        if not self.initialized:
            await self.initialize()
        
        logger.info(f"Voice service session initialized: {session.session_id}")

    async def process_voice_ultra_fast(self, session, audio_data: str, audio_format: str,
                                     voice_config: Dict, processing_options: Dict) -> Dict[str, Any]:
        """Ultra-fast voice processing pipeline"""
        start_time = time.time()
        
        try:
            self.processing_stats["total_requests"] += 1
            
            # Simulated ultra-fast processing
            # In production, this would integrate with actual STT/LLM/TTS services
            
            # Phase 1: Speech-to-Text (simulated)
            stt_start = time.time()
            transcription = "Hello, this is a test transcription"  # Placeholder
            confidence = 0.95
            stt_time = time.time() - stt_start
            
            # Phase 2: LLM Processing (simulated)
            llm_start = time.time()
            response_text = "Thank you for your message. This is a test response from the voice agent."
            llm_time = time.time() - llm_start
            
            # Phase 3: Text-to-Speech (simulated)
            tts_start = time.time()
            # In production, this would generate actual audio
            audio_base64 = base64.b64encode(b"fake_audio_data").decode()
            tts_time = time.time() - tts_start
            
            total_time = time.time() - start_time
            
            # Update session history
            session.add_to_history(transcription, response_text)
            
            # Update stats
            self.processing_stats["successful_requests"] += 1
            self.processing_stats["average_processing_time"] = (
                self.processing_stats["average_processing_time"] + total_time
            ) / 2
            
            result = {
                "transcription": transcription,
                "confidence": confidence,
                "response": {
                    "text": response_text,
                    "audio": audio_base64,
                    "format": "wav"
                },
                "processing_time": {
                    "total": round(total_time, 3),
                    "stt": round(stt_time, 3),
                    "llm": round(llm_time, 3),
                    "tts": round(tts_time, 3)
                },
                "sources": {
                    "rag": [],
                    "search": []
                },
                "performance_metrics": {
                    "targets_met": {
                        "stt": stt_time < 0.3,
                        "llm": llm_time < 0.5,
                        "tts": tts_time < 0.4,
                        "total": total_time < 1.2
                    },
                    "performance_rating": "excellent" if total_time < 0.8 else "good"
                }
            }
            
            logger.info(f"Voice processing completed in {total_time*1000:.0f}ms")
            return result
            
        except Exception as e:
            logger.error(f"Voice processing error: {str(e)}")
            return {
                "transcription": "",
                "confidence": 0.0,
                "response": {
                    "text": "I'm sorry, I encountered an error processing your request.",
                    "audio": None,
                    "format": "wav"
                },
                "processing_time": {
                    "total": time.time() - start_time,
                    "stt": 0,
                    "llm": 0,
                    "tts": 0
                },
                "error": str(e)
            }

    async def process_text_ultra_fast(self, session, text: str, processing_options: Dict) -> Dict[str, Any]:
        """Ultra-fast text processing"""
        start_time = time.time()
        
        try:
            # Simulated text processing
            response_text = f"Thank you for your message: '{text}'. This is a test response."
            
            processing_time = time.time() - start_time
            
            # Update session history
            session.add_to_history(text, response_text)
            
            return {
                "response": response_text,
                "processing_time": round(processing_time * 1000),  # Convert to ms
                "sources": {}
            }
            
        except Exception as e:
            logger.error(f"Text processing error: {str(e)}")
            return {
                "response": "I'm sorry, I encountered an error processing your request.",
                "processing_time": round((time.time() - start_time) * 1000),
                "error": str(e)
            }

    def get_stats(self) -> Dict[str, Any]:
        """Get service statistics"""
        return {
            "initialized": self.initialized,
            **self.processing_stats
        }

    async def cleanup(self):
        """Cleanup service resources"""
        logger.info("Ultra Fast Voice Service cleaned up")

===== ./app/services/connection_manager.py =====
# app/services/connection_manager.py - Enhanced with error recovery
import asyncio
import time
import logging
from typing import Dict, Any, Optional
from fastapi import WebSocket
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class Connection:
    connection_id: str
    websocket: WebSocket
    user_id: Optional[str]
    agent_id: Optional[str]
    created_at: float
    last_activity: float
    is_active: bool = True
    retry_count: int = 0
    error_count: int = 0

class ConnectionManager:
    def __init__(self):
        self.connections: Dict[str, Connection] = {}
        self.user_connections: Dict[str, set] = {}
        self.cleanup_task = None
        self.max_connections = 100
        self.connection_timeout = 300  # 5 minutes
        self.max_retries = 3
        
        logger.info("Enhanced Connection Manager initialized")

    async def add_connection(
        self, 
        connection_id: str, 
        websocket: WebSocket,
        user_id: Optional[str] = None,
        agent_id: Optional[str] = None
    ):
        """Add a new WebSocket connection with enhanced tracking"""
        try:
            # Check connection limits
            if len(self.connections) >= self.max_connections:
                logger.warning(f"Max connections reached: {len(self.connections)}")
                await self._cleanup_stale_connections()
                
                if len(self.connections) >= self.max_connections:
                    raise Exception("Server at maximum capacity")
            
            current_time = time.time()
            
            connection = Connection(
                connection_id=connection_id,
                websocket=websocket,
                user_id=user_id,
                agent_id=agent_id,
                created_at=current_time,
                last_activity=current_time
            )
            
            self.connections[connection_id] = connection
            
            # Track user connections
            if user_id:
                if user_id not in self.user_connections:
                    self.user_connections[user_id] = set()
                self.user_connections[user_id].add(connection_id)
            
            # Start cleanup task if not running
            if not self.cleanup_task:
                self.cleanup_task = asyncio.create_task(self._periodic_cleanup())
            
            logger.info(f"Connection added: {connection_id} (active: {len(self.connections)})")
            
        except Exception as e:
            logger.error(f"Failed to add connection {connection_id}: {str(e)}")
            raise

    async def remove_connection(self, connection_id: str):
        """Remove a WebSocket connection with proper cleanup"""
        try:
            if connection_id not in self.connections:
                logger.warning(f"Connection not found for removal: {connection_id}")
                return
            
            connection = self.connections[connection_id]
            
            # Mark as inactive
            connection.is_active = False
            
            # Remove from user tracking
            if connection.user_id and connection.user_id in self.user_connections:
                self.user_connections[connection.user_id].discard(connection_id)
                if not self.user_connections[connection.user_id]:
                    del self.user_connections[connection.user_id]
            
            # Close WebSocket if still open
            try:
                if connection.websocket:
                    await connection.websocket.close(code=1000, reason="Connection removed")
            except Exception as ws_error:
                logger.debug(f"WebSocket already closed for {connection_id}: {str(ws_error)}")
            
            # Remove from connections
            del self.connections[connection_id]
            
            logger.info(f"Connection removed: {connection_id} (active: {len(self.connections)})")
            
        except Exception as e:
            logger.error(f"Error removing connection {connection_id}: {str(e)}")

    async def get_connection(self, connection_id: str) -> Optional[Connection]:
        """Get connection with activity update"""
        connection = self.connections.get(connection_id)
        if connection and connection.is_active:
            connection.last_activity = time.time()
            return connection
        return None

    async def send_to_connection(
        self, 
        connection_id: str, 
        message: Dict[str, Any],
        retry_on_error: bool = True
    ) -> bool:
        """Send message to specific connection with error handling"""
        try:
            connection = await self.get_connection(connection_id)
            if not connection:
                logger.warning(f"Connection not found: {connection_id}")
                return False
            
            # Try to send message
            try:
                import json
                await connection.websocket.send_text(json.dumps(message))
                
                # Reset error count on successful send
                connection.error_count = 0
                return True
                
            except Exception as send_error:
                logger.error(f"Send failed for {connection_id}: {str(send_error)}")
                connection.error_count += 1
                
                # Remove connection if too many errors
                if connection.error_count >= 3:
                    logger.warning(f"Too many errors for {connection_id}, removing")
                    await self.remove_connection(connection_id)
                
                return False
                
        except Exception as e:
            logger.error(f"Error sending to connection {connection_id}: {str(e)}")
            return False

    async def broadcast_to_user(self, user_id: str, message: Dict[str, Any]) -> int:
        """Broadcast message to all user connections"""
        if user_id not in self.user_connections:
            return 0
        
        successful_sends = 0
        connection_ids = list(self.user_connections[user_id])
        
        for connection_id in connection_ids:
            success = await self.send_to_connection(connection_id, message)
            if success:
                successful_sends += 1
        
        return successful_sends

    async def _cleanup_stale_connections(self):
        """Clean up stale or inactive connections"""
        try:
            current_time = time.time()
            stale_connections = []
            
            for connection_id, connection in self.connections.items():
                # Check if connection is stale
                if (current_time - connection.last_activity) > self.connection_timeout:
                    stale_connections.append(connection_id)
                    continue
                
                # Check if WebSocket is still alive
                try:
                    await connection.websocket.ping()
                except Exception:
                    stale_connections.append(connection_id)
            
            # Remove stale connections
            for connection_id in stale_connections:
                await self.remove_connection(connection_id)
            
            if stale_connections:
                logger.info(f"Cleaned up {len(stale_connections)} stale connections")
                
        except Exception as e:
            logger.error(f"Cleanup error: {str(e)}")

    async def _periodic_cleanup(self):
        """Periodic cleanup task"""
        try:
            while True:
                await asyncio.sleep(60)  # Run every minute
                
                if not self.connections:
                    # No connections, stop cleanup task
                    self.cleanup_task = None
                    break
                
                await self._cleanup_stale_connections()
                
        except asyncio.CancelledError:
            logger.info("Cleanup task cancelled")
        except Exception as e:
            logger.error(f"Periodic cleanup error: {str(e)}")

    def get_connection_count(self) -> int:
        """Get total number of active connections"""
        return len([c for c in self.connections.values() if c.is_active])

    def get_user_connection_count(self, user_id: str) -> int:
        """Get number of connections for a specific user"""
        return len(self.user_connections.get(user_id, set()))

    async def get_stats(self) -> Dict[str, Any]:
        """Get connection statistics"""
        try:
            current_time = time.time()
            active_count = 0
            total_errors = 0
            avg_duration = 0
            
            for connection in self.connections.values():
                if connection.is_active:
                    active_count += 1
                    avg_duration += (current_time - connection.created_at)
                total_errors += connection.error_count
            
            if active_count > 0:
                avg_duration = avg_duration / active_count
            
            return {
                "total_connections": len(self.connections),
                "active_connections": active_count,
                "unique_users": len(self.user_connections),
                "total_errors": total_errors,
                "average_duration": round(avg_duration, 2),
                "max_connections": self.max_connections,
                "connection_timeout": self.connection_timeout
            }
            
        except Exception as e:
            logger.error(f"Error getting stats: {str(e)}")
            return {}

    async def cleanup(self):
        """Cleanup all connections and resources"""
        try:
            # Cancel cleanup task
            if self.cleanup_task and not self.cleanup_task.done():
                self.cleanup_task.cancel()
            
            # Close all connections
            connection_ids = list(self.connections.keys())
            for connection_id in connection_ids:
                await self.remove_connection(connection_id)
            
            # Clear data structures
            self.connections.clear()
            self.user_connections.clear()
            
            logger.info("Connection Manager cleanup completed")
            
        except Exception as e:
            logger.error(f"Connection Manager cleanup error: {str(e)}")

===== ./app/services/advanced_vad.py =====
# advanced_vad.py - Advanced Voice Activity Detection
import numpy as np
import time
import logging
from typing import Dict, Any, Tuple, Optional
from dataclasses import dataclass
import asyncio
from concurrent.futures import ThreadPoolExecutor

@dataclass
class VADConfig:
    sensitivity: float = 0.7
    min_speech_duration: int = 200  # ms
    min_silence_duration: int = 800  # ms
    max_recording_time: int = 15000  # ms
    energy_smoothing: float = 0.3
    noise_reduction: bool = True
    endpoint_detection: bool = True

class AdvancedVAD:
    """Advanced Voice Activity Detection with ultra-fast response"""
    
    def __init__(self, sensitivity: float = 0.7, min_speech_duration: int = 200, 
                 min_silence_duration: int = 800):
        self.config = VADConfig(
            sensitivity=sensitivity,
            min_speech_duration=min_speech_duration,
            min_silence_duration=min_silence_duration
        )
        
        self.logger = logging.getLogger(__name__)
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        # VAD state
        self.current_state = 'silence'  # 'silence', 'speech', 'endpoint'
        self.speech_start_time = 0
        self.silence_start_time = 0
        self.last_state_change = 0
        
        # Energy tracking
        self.energy_history = []
        self.background_noise = 0.01
        self.adaptive_threshold = {
            'speech': 0.03,
            'silence': 0.01
        }
        
        # Performance tracking
        self.processing_times = []
        self.detection_accuracy = 95.0
    
    async def process_audio_frame(self, audio_data: np.ndarray, timestamp: float = None) -> Dict[str, Any]:
        """Process audio frame and detect voice activity"""
        start_time = time.time()
        
        if timestamp is None:
            timestamp = time.time()
        
        try:
            # Calculate energy
            energy = await self.calculate_energy_async(audio_data)
            
            # Update energy history
            self.update_energy_history(energy)
            
            # Update adaptive thresholds
            self.update_adaptive_thresholds()
            
            # Perform VAD
            vad_result = self.detect_voice_activity(energy, timestamp)
            
            # Update performance metrics
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)
            
            return {
                'is_voice_active': vad_result['is_voice_active'],
                'energy': energy,
                'confidence': vad_result['confidence'],
                'state': self.current_state,
                'should_end_recording': vad_result['should_end_recording'],
                'timestamp': timestamp,
                'thresholds': self.adaptive_threshold.copy(),
                'background_noise': self.background_noise,
                'processing_time': processing_time
            }
            
        except Exception as e:
            self.logger.error(f"VAD processing error: {e}")
            return {
                'is_voice_active': False,
                'energy': 0.0,
                'confidence': 0.0,
                'state': 'error',
                'should_end_recording': False,
                'timestamp': timestamp,
                'processing_time': time.time() - start_time
            }
    
    async def calculate_energy_async(self, audio_data: np.ndarray) -> float:
        """Calculate audio energy asynchronously"""
        loop = asyncio.get_event_loop()
        
        def _calculate_energy():
            if len(audio_data) == 0:
                return 0.0
            
            # Calculate RMS energy
            rms = np.sqrt(np.mean(audio_data ** 2))
            
            # Normalize and clamp
            energy = max(0.0, min(1.0, rms * 10))
            
            return energy
        
        return await loop.run_in_executor(self.executor, _calculate_energy)
    
    def update_energy_history(self, energy: float):
        """Update energy history for adaptive processing"""
        self.energy_history.append(energy)
        
        # Keep history manageable
        if len(self.energy_history) > 100:
            self.energy_history.pop(0)
        
        # Update background noise estimate
        if len(self.energy_history) >= 10:
            sorted_history = sorted(self.energy_history)
            # Use 30th percentile as background noise
            noise_index = int(len(sorted_history) * 0.3)
            self.background_noise = sorted_history[noise_index]
    
    def update_adaptive_thresholds(self):
        """Update adaptive thresholds based on environment"""
        if len(self.energy_history) < 20:
            return
        
        recent_energy = self.energy_history[-20:]
        avg_energy = sum(recent_energy) / len(recent_energy)
        max_energy = max(recent_energy)
        
        # Calculate adaptive thresholds
        base_multiplier = 1 + self.config.sensitivity
        
        self.adaptive_threshold['speech'] = max(
            0.03,  # Minimum threshold
            (avg_energy + self.background_noise) * base_multiplier
        )
        
        self.adaptive_threshold['silence'] = max(
            0.01,  # Minimum threshold
            self.background_noise * 1.2
        )
        
        # Prevent thresholds from being too high
        self.adaptive_threshold['speech'] = min(
            self.adaptive_threshold['speech'],
            max_energy * 0.7
        )
    
    def detect_voice_activity(self, energy: float, timestamp: float) -> Dict[str, Any]:
        """Main voice activity detection logic"""
        time_since_last_change = timestamp - self.last_state_change
        is_voice_active = False
        confidence = 0.0
        should_end_recording = False
        
        # State machine for voice detection
        if self.current_state == 'silence':
            if energy > self.adaptive_threshold['speech']:
                if self.speech_start_time == 0:
                    self.speech_start_time = timestamp
                elif (timestamp - self.speech_start_time) * 1000 >= self.config.min_speech_duration:
                    # Transition to speech
                    self.current_state = 'speech'
                    self.last_state_change = timestamp
                    self.speech_start_time = 0
                    self.silence_start_time = 0
                    is_voice_active = True
                    confidence = self.calculate_confidence(energy, self.adaptive_threshold['speech'])
            else:
                self.speech_start_time = 0
        
        elif self.current_state == 'speech':
            if energy < self.adaptive_threshold['silence']:
                if self.silence_start_time == 0:
                    self.silence_start_time = timestamp
                elif (timestamp - self.silence_start_time) * 1000 >= self.config.min_silence_duration:
                    # Transition to silence
                    self.current_state = 'silence'
                    self.last_state_change = timestamp
                    self.silence_start_time = 0
                    self.speech_start_time = 0
                    is_voice_active = False
                    confidence = 0.0
                    
                    # Check for endpoint
                    if self.config.endpoint_detection:
                        should_end_recording = self.detect_endpoint(timestamp)
                else:
                    # Still in speech, brief dip
                    is_voice_active = True
                    confidence = self.calculate_confidence(energy, self.adaptive_threshold['silence'])
            else:
                # Continuing speech
                self.silence_start_time = 0
                is_voice_active = True
                confidence = self.calculate_confidence(energy, self.adaptive_threshold['speech'])
        
        return {
            'is_voice_active': is_voice_active,
            'confidence': confidence,
            'should_end_recording': should_end_recording
        }
    
    def calculate_confidence(self, energy: float, threshold: float) -> float:
        """Calculate confidence score for voice detection"""
        if energy <= threshold:
            return 0.0
        
        confidence = min(1.0, (energy - threshold) / threshold)
        return round(confidence, 2)
    
    def detect_endpoint(self, timestamp: float) -> bool:
        """Detect natural speech endpoints for faster response"""
        if len(self.energy_history) < 5:
            return False
        
        # Check for natural pauses
        recent_energy = self.energy_history[-5:]
        energy_trend = self.calculate_energy_trend(recent_energy)
        
        # Endpoint conditions
        conditions = [
            # Sufficient speech duration
            self.speech_start_time > 0 and 
            (timestamp - self.speech_start_time) * 1000 >= (self.config.min_speech_duration * 2),
            
            # Decreasing energy trend
            energy_trend < -0.1,
            
            # Below speech threshold
            recent_energy[-1] < self.adaptive_threshold['speech'] * 1.2,
            
            # Not at the very beginning
            len(self.energy_history) > 10
        ]
        
        return sum(conditions) >= 2
    
    def calculate_energy_trend(self, energy_array: list) -> float:
        """Calculate energy trend (positive = increasing, negative = decreasing)"""
        if len(energy_array) < 3:
            return 0.0
        
        trend = 0.0
        for i in range(1, len(energy_array)):
            trend += energy_array[i] - energy_array[i - 1]
        
        return trend / (len(energy_array) - 1)
    
    def reset_state(self):
        """Reset VAD state"""
        self.current_state = 'silence'
        self.speech_start_time = 0
        self.silence_start_time = 0
        self.last_state_change = 0
        self.energy_history = []
        self.background_noise = 0.01
        
        self.logger.debug("VAD state reset")
    
    def optimize_for_speed(self):
        """Optimize VAD for maximum speed"""
        self.config.min_silence_duration = 600
        self.config.min_speech_duration = 150
        self.config.sensitivity = 0.8
        self.config.endpoint_detection = True
        
        self.logger.info("VAD optimized for ultra-fast response")
    
    def optimize_for_accuracy(self):
        """Optimize VAD for maximum accuracy"""
        self.config.min_silence_duration = 1000
        self.config.min_speech_duration = 300
        self.config.sensitivity = 0.6
        self.config.endpoint_detection = False
        
        self.logger.info("VAD optimized for accuracy")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics"""
        if not self.processing_times:
            return {
                'average_processing_time': 0.0,
                'detection_accuracy': self.detection_accuracy,
                'total_frames': 0
            }
        
        avg_time = sum(self.processing_times) / len(self.processing_times)
        
        return {
            'average_processing_time': avg_time,
            'detection_accuracy': self.detection_accuracy,
            'total_frames': len(self.processing_times),
            'config': {
                'sensitivity': self.config.sensitivity,
                'min_speech_duration': self.config.min_speech_duration,
                'min_silence_duration': self.config.min_silence_duration
            },
            'current_state': self.current_state,
            'background_noise': self.background_noise,
            'adaptive_thresholds': self.adaptive_threshold.copy()
        }
    
    async def cleanup(self):
        """Cleanup resources"""
        try:
            self.executor.shutdown(wait=False)
            self.logger.info("VAD cleanup completed")
        except Exception as e:
            self.logger.error(f"VAD cleanup error: {e}")

===== ./app/services/mcp_client.py =====
# app/services/mcp_client.py
import httpx
import logging
from app.config import settings
from typing import Dict, Any
from fastapi import HTTPException, status

logger = logging.getLogger(__name__)

class MCPClient:
    def __init__(self):
        self.base_url = settings.MCP_SERVER_URL
        self.timeout = 30
    
    async def dispatch_task(self, agent_id: str, task_type: str, payload: Dict[str, Any]) -> str:
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/tasks",
                    json={
                        "agent_id": agent_id,
                        "task_type": task_type,
                        "payload": payload
                    }
                )
                response.raise_for_status()
                return response.json()["task_id"]
        except httpx.HTTPStatusError as e:
            logger.error(f"MCP server returned error: {e.response.text}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail="MCP service error"
            )
        except Exception as e:
            logger.error(f"Failed to dispatch MCP task: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="MCP service unavailable"
            )
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.base_url}/tasks/{task_id}")
                response.raise_for_status()
                return response.json()
        except httpx.HTTPStatusError as e:
            logger.error(f"MCP server returned error: {e.response.text}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail="MCP service error"
            )
        except Exception as e:
            logger.error(f"Failed to get MCP task status: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="MCP service unavailable"
            )
===== ./app/services/agent_execution_service.py =====
from fastapi import HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.db_models import DBAgent
from app.services.llm_service import OllamaService
from app.services.cache import cache_service
from app.services.multi_search_service import MultiSearchService, SearchResultAggregator
from app.models.agent_model import AgentPersonality
import json
import logging
import hashlib
import time
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

async def execute_agent(agent_id: str, user_id: str, input_data: dict, db: AsyncSession):
    """Enhanced agent execution with internet search capabilities"""
    
    start_time = time.time()
    
    try:
        # Get agent from database
        result = await db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == user_id
            )
        )
        agent = result.scalars().first()
        
        if not agent:
            raise ValueError(f"Agent {agent_id} not found")
        
        # Extract parameters
        parameters = input_data.get("parameters", {})
        user_input = input_data.get("input", "")
        
        # Determine if internet search is needed
        enable_search = parameters.get("enable_search", True)
        search_sources = parameters.get("search_sources", ["duckduckgo", "reddit"])
        
        search_context = ""
        search_results_info = {}
        
        if enable_search and await _should_search_internet(user_input):
            try:
                # Perform multi-source search
                search_service = MultiSearchService()
                aggregator = SearchResultAggregator()
                
                logger.info(f"Performing internet search for query: {user_input[:50]}...")
                
                search_results = await search_service.search_all_sources(
                    query=user_input,
                    sources=search_sources,
                    max_results_per_source=parameters.get("max_search_results", 3)
                )
                
                # Aggregate and rank results
                aggregated_results = aggregator.aggregate_results(
                    search_results, 
                    max_final_results=8
                )
                
                # Build search context
                search_context = _build_search_context(aggregated_results)
                
                search_results_info = {
                    "performed": True,
                    "sources_used": list(search_results.keys()),
                    "total_results": len(aggregated_results),
                    "top_sources": [r.source for r in aggregated_results[:3]]
                }
                
                logger.info(f"Search completed: {len(aggregated_results)} results from {len(search_results)} sources")
                
            except Exception as search_error:
                logger.error(f"Internet search failed: {str(search_error)}")
                search_context = ""
                search_results_info = {
                    "performed": False,
                    "error": str(search_error)
                }
        else:
            search_results_info = {"performed": False, "reason": "Search not needed or disabled"}
        
        # Get conversation history
        history_key = f"conversation:{user_id}:{agent_id}"
        conversation_history = []
        
        try:
            cached_history = await cache_service.get(history_key)
            if cached_history:
                conversation_history = json.loads(cached_history)
        except Exception as cache_error:
            logger.warning(f"Failed to retrieve conversation history: {str(cache_error)}")
        
        # Build enhanced system prompt
        enhanced_prompt = _build_enhanced_system_prompt(agent, search_context)
        
        # Build conversation with personality context
        personality = AgentPersonality(**agent.personality) if agent.personality else AgentPersonality()
        
        messages = [
            {"role": "system", "content": enhanced_prompt},
            *conversation_history[-10:],  # Keep last 10 messages
            {"role": "user", "content": user_input}
        ]
        
        # Execute with LLM
        llm_service = OllamaService()
        
        llm_options = {
            "temperature": parameters.get("temperature", personality.base_tone_temperature),
            "top_p": parameters.get("top_p", 0.9),
            "max_tokens": parameters.get("max_tokens", 2000),
        }
        
        response = await llm_service.chat(
            messages=messages,
            model=agent.model,
            options=llm_options
        )
        
        # Extract response content
        response_content = ""
        if response and "message" in response and "content" in response["message"]:
            response_content = response["message"]["content"]
        else:
            response_content = "I apologize, but I encountered an issue generating a response."
        
        # Update conversation history
        new_user_message = {"role": "user", "content": user_input}
        new_assistant_message = {"role": "assistant", "content": response_content}
        
        updated_history = conversation_history + [new_user_message, new_assistant_message]
        
        # Keep only recent messages (last 20)
        max_history = personality.memory.context_window if personality.memory else 20
        updated_history = updated_history[-max_history:]
        
        # Cache updated conversation history
        try:
            history_ttl = personality.memory.memory_refresh_interval if personality.memory else 3600
            await cache_service.set(
                history_key,
                json.dumps(updated_history, default=str, ensure_ascii=False),
                ttl=history_ttl
            )
        except Exception as cache_error:
            logger.warning(f"Failed to cache conversation history: {str(cache_error)}")
        
        execution_time = time.time() - start_time
        
        return {
            "response": response_content,
            "model": agent.model,
            "personality_applied": personality.dict() if personality else {},
            "context_length": len(updated_history),
            "execution_time_ms": round(execution_time * 1000, 2),
            "search_info": search_results_info,
            "success": True
        }
        
    except ValueError as ve:
        logger.error(f"Validation error: {str(ve)}")
        raise ve
    except Exception as e:
        logger.error(f"Agent execution failed: {str(e)}", exc_info=True)
        execution_time = time.time() - start_time
        
        return {
            "response": f"I apologize, but I encountered an error while processing your request: {str(e)}",
            "model": "error",
            "personality_applied": {},
            "context_length": 0,
            "execution_time_ms": round(execution_time * 1000, 2),
            "search_info": {"performed": False, "error": str(e)},
            "success": False,
            "error": str(e)
        }

async def _should_search_internet(query: str) -> bool:
    """Determine if a query should trigger internet search"""
    
    # Skip very short queries
    if len(query.strip()) < 3:
        return False
    
    # Keywords that suggest current information is needed
    current_info_keywords = [
        'news', 'recent', 'latest', 'current', 'today', 'yesterday', 
        'this week', 'this month', 'this year', 'now', 'currently',
        'trending', 'popular', 'new', 'update', 'breaking'
    ]
    
    # Question words that often need current info
    question_keywords = [
        'what is', 'what are', 'who is', 'where is', 'when is',
        'how to', 'why is', 'which', 'compare', 'best', 'top'
    ]
    
    # Technical topics that benefit from search
    tech_keywords = [
        'error', 'bug', 'issue', 'problem', 'solution', 'fix',
        'tutorial', 'guide', 'documentation', 'api', 'code'
    ]
    
    query_lower = query.lower()
    
    # Check for indicators that search would be helpful
    for keyword_list in [current_info_keywords, question_keywords, tech_keywords]:
        if any(keyword in query_lower for keyword in keyword_list):
            return True
    
    # Check for specific patterns
    if any(pattern in query_lower for pattern in ['?', 'how do', 'what does', 'explain']):
        return True
    
    # Default to search for queries longer than 10 words (likely complex questions)
    if len(query.split()) > 10:
        return True
    
    return False

def _build_enhanced_system_prompt(agent, search_context: str) -> str:
    """Build enhanced system prompt with search context"""
    
    base_prompt = agent.system_prompt or "You are a helpful AI assistant."
    
    if search_context:
        enhanced_prompt = f"""{base_prompt}

You have access to recent information from the internet to help answer the user's question. Use this information to provide accurate, up-to-date, and comprehensive responses.

RECENT INTERNET INFORMATION:
{search_context}

Instructions:
1. Use the recent internet information when relevant to the user's question
2. Combine your existing knowledge with this new information
3. If the internet information contradicts your knowledge, prefer the more recent information
4. Always be helpful and provide specific, actionable information when possible
5. If you cite specific information from the search results, mention the source naturally
6. Don't mention that you performed a search - just provide the information naturally"""
    else:
        enhanced_prompt = base_prompt
    
    return enhanced_prompt

def _build_search_context(results: List) -> str:
    """Build search context from aggregated results"""
    if not results:
        return ""
    
    context_parts = []
    
    for i, result in enumerate(results[:6]):  # Use top 6 results
        source_info = f"[Source: {result.source.title()}]"
        if result.metadata and result.metadata.get('subreddit'):
            source_info += f" (r/{result.metadata['subreddit']})"
        
        context_parts.append(
            f"{i+1}. **{result.title}** {source_info}\n"
            f"   {result.content[:400]}{'...' if len(result.content) > 400 else ''}\n"
        )
    
    return "\n".join(context_parts)

===== ./app/services/voice_service.py =====
# app/services/voice_service.py

import asyncio
import logging
import base64
import io
import tempfile
import os
import time
import hashlib
import wave
import re
import subprocess
import platform
import shutil
from typing import Optional, Dict, Any, List, Union
import numpy as np
from pathlib import Path

# Core dependencies
import torch
import torchaudio
from app.config import settings

logger = logging.getLogger(__name__)

class VoiceService:
    def __init__(self):
        self.whisper_model = None
        self.coqui_tts = None
        self._models_loaded = False
        self._load_lock = asyncio.Lock()
        self._model_cache = {}
        self._response_cache = {}
        
        # Audio processing settings
        self.target_sample_rate = 16000
        self.target_channels = 1
        
        # TTS Configuration - can be overridden by config.py
        self.tts_max_chunk_size = getattr(settings, 'TTS_MAX_CHUNK_SIZE', 400)
        self.tts_enable_chunking = getattr(settings, 'TTS_ENABLE_CHUNKING', True)
        self.tts_chunk_pause_duration = getattr(settings, 'TTS_CHUNK_PAUSE_DURATION', 0.3)
        
    async def _ensure_models_loaded(self):
        """Ensure voice models are loaded with proper error handling"""
        if self._models_loaded:
            return
            
        async with self._load_lock:
            if self._models_loaded:
                return
                
            try:
                logger.info("Loading voice models...")
                
                # Load Whisper for STT
                if settings.ENABLE_WHISPER:
                    await self._load_whisper_model()
                
                # Load TTS model
                if settings.ENABLE_TTS:
                    await self._load_tts_model()
                
                self._models_loaded = True
                logger.info("‚úÖ Voice models loaded successfully")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to load voice models: {str(e)}")
                # Allow service to work with fallbacks
                self._models_loaded = True
                
    async def _load_whisper_model(self):
        """Load Whisper model using settings from config.py"""
        try:
            import whisper
            
            loop = asyncio.get_event_loop()
            
            def load_model():
                model_name = settings.WHISPER_MODEL
                return whisper.load_model(model_name)
            
            self.whisper_model = await loop.run_in_executor(None, load_model)
            logger.info(f"‚úÖ Loaded Whisper model: {settings.WHISPER_MODEL}")
            
        except ImportError:
            logger.warning("Whisper not available. Install openai-whisper for STT functionality")
        except Exception as e:
            logger.error(f"Failed to load Whisper: {str(e)}")
            
    def _check_system_dependencies(self) -> Dict[str, bool]:
        """Check if required system dependencies are available"""
        try:
            dependencies = {
                "espeak": shutil.which("espeak") is not None,
                "espeak-ng": shutil.which("espeak-ng") is not None,
                "ffmpeg": shutil.which("ffmpeg") is not None
            }
            
            # Log dependency status
            for dep, available in dependencies.items():
                if available:
                    logger.info(f"‚úÖ Found {dep}")
                else:
                    logger.warning(f"‚ùå Missing {dep}")
            
            return dependencies
            
        except Exception as e:
            logger.error(f"Error checking system dependencies: {str(e)}")
            return {}
            
    async def _load_tts_model(self):
        """Load TTS model based on settings from config.py"""
        try:
            # Check system dependencies first
            deps = self._check_system_dependencies()
            if not any(deps.get(dep, False) for dep in ["espeak", "espeak-ng"]):
                logger.warning("No espeak backend found - Coqui TTS may fail. Install: sudo apt-get install espeak-ng")
            
            # Use TTS engine from settings
            if settings.TTS_ENGINE == "coqui":
                await self._load_coqui_tts_model()
            else:
                logger.warning(f"Unknown TTS engine: {settings.TTS_ENGINE}, using system fallback")
                
        except Exception as e:
            logger.error(f"TTS loading failed: {str(e)}")
            # Don't raise - allow fallback to system TTS
            
    async def _load_coqui_tts_model(self):
        """Load Coqui TTS model with FIXED multi-speaker support"""
        try:
            from TTS.api import TTS
            
            loop = asyncio.get_event_loop()
            
            def load_coqui():
                # FIXED: Better model selection prioritizing single-speaker models
                models_to_try = [
                    # Single-speaker models (more reliable)
                    "tts_models/en/ljspeech/tacotron2-DDC",    # Most reliable single-speaker
                    "tts_models/en/ljspeech/fast_pitch",       # Fast single-speaker
                    "tts_models/en/ljspeech/glow-tts",         # Good quality single-speaker
                    "tts_models/en/ljspeech/neural_hmm",       # Alternative single-speaker
                    # Multi-speaker models (with proper handling)
                    "tts_models/en/vctk/vits",                 # Multi-speaker (needs speaker)
                    settings.TTS_MODEL,                        # User configured (if different)
                ]
                
                # Remove duplicates while preserving order
                models_to_try = list(dict.fromkeys(models_to_try))
                
                last_error = None
                
                for model_name in models_to_try:
                    try:
                        logger.info(f"Trying to load Coqui TTS model: {model_name}")
                        
                        # Initialize the model
                        tts = TTS(model_name=model_name)
                        
                        # FIXED: Test with proper multi-speaker handling
                        test_result = self._validate_coqui_model_fixed(tts, model_name)
                        if test_result:
                            logger.info(f"‚úÖ Successfully loaded Coqui TTS model: {model_name}")
                            return tts
                        else:
                            logger.warning(f"Model {model_name} failed validation")
                            continue
                            
                    except Exception as e:
                        last_error = e
                        if "espeak" in str(e).lower():
                            logger.error(f"‚ùå Model {model_name} failed due to missing espeak: {str(e)}")
                        elif "speaker" in str(e).lower():
                            logger.error(f"‚ùå Model {model_name} failed due to speaker issue: {str(e)}")
                        else:
                            logger.warning(f"Failed to load model {model_name}: {str(e)}")
                        continue
                
                # Provide helpful error message based on last error
                if last_error:
                    if "espeak" in str(last_error).lower():
                        raise Exception(
                            "All Coqui TTS models failed due to missing espeak. "
                            "Install: sudo apt-get install espeak-ng espeak-ng-data"
                        )
                    else:
                        raise Exception(f"All Coqui TTS models failed. Last error: {str(last_error)}")
                else:
                    raise Exception("All Coqui TTS models failed to load")
            
            self.coqui_tts = await loop.run_in_executor(None, load_coqui)
            
        except ImportError:
            logger.error("Coqui TTS not available. Install with: pip install TTS")
            raise
        except Exception as e:
            logger.error(f"Coqui TTS loading failed: {str(e)}")
            raise

    def _validate_coqui_model_fixed(self, tts, model_name: str) -> bool:
        """FIXED: Validate Coqui TTS model with proper multi-speaker handling"""
        try:
            test_text = "Hello world"
            logger.info(f"Testing model {model_name} with text: '{test_text}'")
            
            # FIXED: Check if model is multi-speaker and handle accordingly
            is_multi_speaker = hasattr(tts, 'speakers') and tts.speakers is not None and len(tts.speakers) > 0
            
            if is_multi_speaker:
                logger.info(f"Model {model_name} is multi-speaker with {len(tts.speakers)} speakers")
                # Use first available speaker for multi-speaker models
                speaker = tts.speakers[0] if tts.speakers else None
                if speaker:
                    logger.info(f"Using speaker: {speaker}")
                    test_output = tts.tts(text=test_text, speaker=speaker)
                else:
                    logger.warning(f"Multi-speaker model {model_name} has no available speakers")
                    return False
            else:
                logger.info(f"Model {model_name} is single-speaker")
                test_output = tts.tts(text=test_text)
            
            logger.info(f"Model {model_name} test output type: {type(test_output)}, shape: {getattr(test_output, 'shape', 'N/A')}")
            
            # FIXED: Handle the specific case where Coqui returns list of float32 values
            if isinstance(test_output, list):
                if len(test_output) > 0:
                    # Check if it's a list of numpy scalars (float32)
                    if isinstance(test_output[0], (np.float32, np.float64, float)):
                        logger.info(f"‚úÖ Model {model_name} produced list of audio samples ({len(test_output)} samples)")
                        # Convert to proper numpy array
                        try:
                            audio_array = np.array(test_output, dtype=np.float32)
                            if audio_array.size > 0:
                                logger.info(f"‚úÖ Successfully converted to numpy array with shape: {audio_array.shape}")
                                return True
                            else:
                                logger.warning(f"Model {model_name} produced empty audio array")
                                return False
                        except Exception as conv_error:
                            logger.error(f"Failed to convert audio samples to array: {conv_error}")
                            return False
                    elif isinstance(test_output[0], np.ndarray):
                        logger.info(f"‚úÖ Model {model_name} produced valid list of audio arrays")
                        return True
                    elif isinstance(test_output[0], str):
                        logger.info(f"‚ö†Ô∏è Model {model_name} produced sentences - may need special handling but is valid")
                        return True
                    else:
                        logger.warning(f"Model {model_name} produced list with unknown content type: {type(test_output[0])}")
                        return False
                else:
                    logger.warning(f"Model {model_name} produced empty list")
                    return False
                    
            elif isinstance(test_output, np.ndarray):
                if test_output.size > 0 and test_output.ndim >= 1:
                    logger.info(f"‚úÖ Model {model_name} produced valid numpy array")
                    return True
                else:
                    logger.warning(f"Model {model_name} produced empty or invalid array")
                    return False
                    
            elif isinstance(test_output, (bytes, bytearray)):
                logger.info(f"‚úÖ Model {model_name} produced audio bytes")
                return True
                
            else:
                logger.warning(f"Model {model_name} produced unexpected output type: {type(test_output)}")
                return False
                
        except Exception as e:
            if "speaker" in str(e).lower():
                logger.error(f"Model {model_name} validation failed due to speaker issue: {str(e)}")
            elif "espeak" in str(e).lower():
                logger.error(f"Model {model_name} validation failed due to espeak: {str(e)}")
            else:
                logger.error(f"Model {model_name} validation failed: {str(e)}")
            return False

    async def speech_to_text(self, audio_data: bytes, format: str = "webm", user_id: str = None, language: str = "en", optimization_level: str = "balanced") -> Dict[str, Any]:
        """Convert speech to text using Whisper with settings from config.py"""
        try:
            # Initialize Whisper if not loaded
            if self.whisper_model is None:
                await self._ensure_models_loaded()
                
            if self.whisper_model is None:
                import whisper
                # Use model from settings as fallback
                self.whisper_model = whisper.load_model(settings.WHISPER_MODEL)
                
            # Save audio to temporary file
            with tempfile.NamedTemporaryFile(suffix=f".{format}", delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_path = temp_file.name
            
            try:
                # Transcribe with optimization settings
                transcribe_params = self._get_transcription_params(optimization_level, language)
                result = self.whisper_model.transcribe(temp_path, **transcribe_params)
                
                return {
                    "text": result["text"].strip(),
                    "confidence": self._calculate_confidence(result),
                    "language": result.get("language", language),
                    "processing_time": 0.5
                }
                
            finally:
                # Clean up temp file
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            logger.error(f"STT error: {str(e)}")
            return {
                "text": "",
                "confidence": 0.0,
                "error": str(e)
            }

    async def text_to_speech(self, text: str, user_id: str = None, agent_id: str = None, voice_config: Dict = None, optimization_level: str = "balanced") -> Dict[str, Any]:
        """Generate TTS using settings from config.py with comprehensive error handling and chunking support"""
        try:
            if not text or not text.strip():
                return {"audio_base64": "", "error": "Empty text"}
        
            # Clean text for TTS - properly handle HTML-encoded <think> tags
            clean_text = self._prepare_text_for_tts(text)
            logger.info(f"üîä Generating TTS for: {clean_text[:50]}... (Total length: {len(clean_text)} chars)")
        
            # Ensure models are loaded
            await self._ensure_models_loaded()
            
            # Try TTS generation with fallbacks
            return await self._generate_tts_with_fallbacks(clean_text, voice_config or {})
                
        except Exception as e:
            logger.error(f"‚ùå TTS Error: {str(e)}")
            return {
                "audio_base64": "",
                "error": str(e),
                "success": False
            }

    async def _generate_tts_with_fallbacks(self, text: str, voice_config: Dict) -> Dict[str, Any]:
        """Try multiple TTS methods based on settings with proper fallbacks"""
        
        # Method 1: Try Coqui TTS if configured and loaded
        if settings.TTS_ENGINE == "coqui" and self.coqui_tts:
            try:
                logger.info("Attempting Coqui TTS generation...")
                return await self._generate_coqui_tts_fixed(text, voice_config)
            except Exception as e:
                logger.warning(f"Coqui TTS failed: {str(e)}, trying system TTS...")
        else:
            logger.info("Coqui TTS not available, using system TTS fallback")
        
        # Method 2: Try system TTS as fallback
        try:
            return await self._generate_system_tts(text, voice_config)
        except Exception as e:
            logger.error(f"All TTS methods failed: {str(e)}")
            return {
                "audio_base64": "",
                "error": "TTS generation failed - all methods unavailable",
                "success": False
            }

    def _chunk_text_for_tts(self, text: str, max_chunk_size: int = None) -> List[str]:
        """Break long text into chunks that TTS can handle"""
        if max_chunk_size is None:
            max_chunk_size = self.tts_max_chunk_size
            
        if len(text) <= max_chunk_size:
            return [text]
        
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        current_chunk = ""
        
        for sentence in sentences:
            # If adding this sentence would exceed limit, save current chunk
            if len(current_chunk + " " + sentence) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += (" " + sentence if current_chunk else sentence)
        
        # Add the last chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    async def _generate_coqui_tts_fixed(self, text: str, voice_config: Dict) -> Dict[str, Any]:
        """Generate TTS using Coqui TTS with support for long text chunking"""
        try:
            loop = asyncio.get_event_loop()
            
            def generate_audio():
                # Process text but don't truncate
                processed_text = self._process_text_for_coqui(text)
                
                # Check if we need to chunk the text
                if self.tts_enable_chunking:
                    text_chunks = self._chunk_text_for_tts(processed_text, max_chunk_size=self.tts_max_chunk_size)
                else:
                    text_chunks = [processed_text]
                    
                logger.info(f"Processing {len(text_chunks)} text chunks for TTS")
                
                # Handle multi-speaker detection
                is_multi_speaker = hasattr(self.coqui_tts, 'speakers') and self.coqui_tts.speakers is not None and len(self.coqui_tts.speakers) > 0
                speaker = voice_config.get('speaker') or (self.coqui_tts.speakers[0] if is_multi_speaker else None)
                
                all_audio_data = []
                
                # Generate audio for each chunk
                for i, chunk in enumerate(text_chunks):
                    logger.info(f"Generating TTS for chunk {i+1}/{len(text_chunks)}: {chunk[:50]}...")
                    
                    if is_multi_speaker:
                        wav_data = self.coqui_tts.tts(text=chunk, speaker=speaker)
                    else:
                        wav_data = self.coqui_tts.tts(text=chunk)
                    
                    # Convert to numpy array (existing logic)
                    if isinstance(wav_data, list):
                        if isinstance(wav_data[0], (np.float32, np.float64, float)):
                            wav_data = np.array(wav_data, dtype=np.float32)
                        elif isinstance(wav_data[0], np.ndarray):
                            wav_data = np.concatenate([arr for arr in wav_data if isinstance(arr, np.ndarray) and arr.size > 0])
                    
                    if isinstance(wav_data, np.ndarray) and wav_data.size > 0:
                        all_audio_data.append(wav_data)
                        
                        # Add small pause between chunks (optional)
                        if i < len(text_chunks) - 1:
                            pause_samples = int(self.tts_chunk_pause_duration * 22050)  # configurable pause
                            pause = np.zeros(pause_samples, dtype=np.float32)
                            all_audio_data.append(pause)
                
                if not all_audio_data:
                    raise Exception("No audio data generated")
                
                # Combine all chunks
                final_audio = np.concatenate(all_audio_data)
                
                # Normalize and convert to 16-bit PCM
                if np.max(np.abs(final_audio)) > 0:
                    final_audio = final_audio / np.max(np.abs(final_audio)) * 0.95
                
                wav_data_int16 = (final_audio * 32767).astype(np.int16)
                
                # Create WAV file
                buffer = io.BytesIO()
                sample_rate = 22050
                
                with wave.open(buffer, 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(sample_rate)
                    wav_file.writeframes(wav_data_int16.tobytes())
                
                return buffer.getvalue()
            
            # Generate audio in thread pool
            audio_bytes = await loop.run_in_executor(None, generate_audio)
            audio_base64 = base64.b64encode(audio_bytes).decode()
            
            logger.info(f"‚úÖ Coqui TTS generated: {len(audio_base64)} chars for full text ({len(text)} input chars)")
            
            return {
                "audio_base64": audio_base64,
                "format": "wav",
                "sample_rate": 22050,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Coqui TTS generation error: {str(e)}")
            raise

    def _process_text_for_coqui(self, text: str) -> str:
        """Process text specifically for Coqui TTS without aggressive truncation"""
        # Replace problematic characters
        text = text.replace("'", "'").replace("'", "'")
        text = text.replace(""", '"').replace(""", '"')
        text = text.replace("‚Äì", "-").replace("‚Äî", "-")
        text = text.replace("‚Ä¶", "...")
        
        # Remove non-ASCII characters
        text = re.sub(r'[^\x00-\x7F]+', '', text)
        
        # Return full text for chunking - no truncation here
        return text.strip()

    async def _generate_system_tts(self, text: str, voice_config: Dict) -> Dict[str, Any]:
        """Fallback TTS using system commands with chunking support"""
        try:
            loop = asyncio.get_event_loop()
            
            def generate_system_audio():
                # If text is too long, chunk it
                if self.tts_enable_chunking and len(text) > self.tts_max_chunk_size:
                    text_chunks = self._chunk_text_for_tts(text)
                    all_audio_data = []
                    
                    for i, chunk in enumerate(text_chunks):
                        logger.info(f"Generating system TTS for chunk {i+1}/{len(text_chunks)}")
                        chunk_audio = self._generate_single_system_tts(chunk)
                        all_audio_data.append(chunk_audio)
                    
                    # Combine audio chunks
                    return self._combine_audio_chunks(all_audio_data)
                else:
                    return self._generate_single_system_tts(text)
            
            audio_base64 = await loop.run_in_executor(None, generate_system_audio)
            
            return {
                "audio_base64": audio_base64,
                "format": "wav",
                "success": True
            }
                
        except Exception as e:
            logger.error(f"System TTS error: {str(e)}")
            raise

    def _generate_single_system_tts(self, text: str) -> str:
        """Generate TTS for a single text chunk using system commands"""
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_path = temp_file.name
        
        try:
            system = platform.system().lower()
            
            if system == "darwin":  # macOS
                cmd = ["say", "-o", temp_path, "--data-format=LEI16@22050", text]
            elif system == "linux":
                cmd = ["espeak", "-w", temp_path, "-s", "150", text]
            else:  # Windows
                ps_script = f'''
                Add-Type -AssemblyName System.speech
                $speak = New-Object System.Speech.Synthesis.SpeechSynthesizer
                $speak.SetOutputToWaveFile("{temp_path}")
                $speak.Speak("{text}")
                $speak.Dispose()
                '''
                cmd = ["powershell", "-Command", ps_script]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode != 0:
                raise Exception(f"System TTS failed: {result.stderr}")
            
            # Read generated file
            with open(temp_path, "rb") as f:
                audio_data = f.read()
            
            return base64.b64encode(audio_data).decode()
            
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    def _combine_audio_chunks(self, audio_chunks: List[str]) -> str:
        """Combine multiple base64 audio chunks into one"""
        if not audio_chunks:
            raise Exception("No audio chunks to combine")
        
        if len(audio_chunks) == 1:
            return audio_chunks[0]
        
        # For simplicity, just return the first chunk
        # In a more sophisticated implementation, you would decode, combine, and re-encode
        logger.warning("Audio chunk combination not fully implemented - returning first chunk")
        return audio_chunks[0]

    def _prepare_text_for_tts(self, text: str) -> str:
        """Prepare text for optimal TTS processing without excessive truncation"""
        if not text or not text.strip():
            return ""
        
        # Remove HTML-encoded thinking tags first
        text = re.sub(r'&amp;amp;lt;think&amp;amp;gt;.*?&amp;amp;lt;/think&amp;amp;gt;', '', text, flags=re.DOTALL)
        text = re.sub(r'&amp;amp;lt;think&amp;amp;gt;.*$', '', text, flags=re.DOTALL)
        
        # Remove regular <think> tags
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'<think>.*$', '', text, flags=re.DOTALL)
        
        # Remove any remaining XML-like tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # Clean whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        if not text:
            return ""
        
        # Character replacements for better TTS
        replacements = {
            "'": "'", "'": "'", """: '"', """: '"',
            "‚Äì": "-", "‚Äî": "-", "‚Ä¶": "..."
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Clean markdown but keep the content
        text = re.sub(r'[*_`#]', '', text)
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # Keep link text
        text = re.sub(r'https?://[^\s]+', 'link', text)
        
        # Ensure proper punctuation
        if text and not text[-1] in '.!?':
            text += '.'
        
        return text.strip()

    # Additional utility methods
    def _get_transcription_params(self, optimization_level: str, language: Optional[str] = None) -> Dict[str, Any]:
        """Get transcription parameters based on optimization level"""
        base_params = {
            "fp16": torch.cuda.is_available(),
            "task": "transcribe",
            "condition_on_previous_text": False
        }
        
        if language:
            base_params["language"] = language
            
        if optimization_level == "fast":
            base_params.update({
                "temperature": 0.0,
                "no_speech_threshold": 0.7,
                "logprob_threshold": -1.0,
                "compression_ratio_threshold": 2.4
            })
        elif optimization_level == "quality":
            base_params.update({
                "temperature": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
                "no_speech_threshold": 0.4,
                "logprob_threshold": -1.0,
                "compression_ratio_threshold": 2.4
            })
        else:  # balanced
            base_params.update({
                "temperature": 0.0,
                "no_speech_threshold": 0.6,
                "logprob_threshold": -1.0,
                "compression_ratio_threshold": 2.4
            })
            
        return base_params

    def _calculate_confidence(self, whisper_result: Dict) -> float:
        """Calculate confidence score from Whisper result"""
        try:
            if "segments" in whisper_result and whisper_result["segments"]:
                segments = whisper_result["segments"]
                confidences = []
                
                for segment in segments:
                    if "avg_logprob" in segment:
                        # Convert log probability to confidence (0-1 scale)
                        logprob = segment["avg_logprob"]
                        confidence = min(1.0, max(0.0, np.exp(logprob)))
                        confidences.append(confidence)
                
                if confidences:
                    return sum(confidences) / len(confidences)
            
            # Fallback confidence based on text characteristics
            text = whisper_result.get("text", "").strip()
            if len(text) > 10:
                return 0.8
            elif len(text) > 0:
                return 0.6
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"Confidence calculation error: {str(e)}")
            return 0.7

    async def get_health_status(self) -> Dict[str, Any]:
        """Get health status of voice service using settings"""
        try:
            await self._ensure_models_loaded()
            
            deps = self._check_system_dependencies()
            
            return {
                "status": "healthy",
                "whisper_available": self.whisper_model is not None,
                "tts_available": self.coqui_tts is not None,
                "tts_engine": settings.TTS_ENGINE,
                "whisper_model": settings.WHISPER_MODEL if self.whisper_model else "none",
                "tts_model": settings.TTS_MODEL if settings.TTS_ENGINE == "coqui" else "system",
                "coqui_loaded": self.coqui_tts is not None,
                "coqui_is_multi_speaker": hasattr(self.coqui_tts, 'speakers') and self.coqui_tts.speakers is not None and len(self.coqui_tts.speakers) > 0 if self.coqui_tts else False,
                "available_speakers": self.coqui_tts.speakers if (self.coqui_tts and hasattr(self.coqui_tts, 'speakers') and self.coqui_tts.speakers) else [],
                "system_dependencies": deps,
                "models_loaded": self._models_loaded,
                "tts_chunking_enabled": self.tts_enable_chunking,
                "tts_max_chunk_size": self.tts_max_chunk_size,
                "settings_used": {
                    "enable_whisper": settings.ENABLE_WHISPER,
                    "enable_tts": settings.ENABLE_TTS,
                    "tts_engine": settings.TTS_ENGINE,
                    "whisper_model": settings.WHISPER_MODEL,
                    "tts_model": settings.TTS_MODEL
                }
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "models_loaded": False
            }

    async def clear_cache(self):
        """Clear all cached responses"""
        self._model_cache.clear()
        self._response_cache.clear()
        logger.info("Voice service cache cleared")

    async def cleanup(self):
        """Cleanup resources"""
        try:
            self._model_cache.clear()
            self._response_cache.clear()
            
            # Cleanup Coqui TTS if needed
            if hasattr(self, 'coqui_tts') and self.coqui_tts:
                try:
                    del self.coqui_tts
                    self.coqui_tts = None
                except:
                    pass
                    
            logger.info("Voice service cleanup completed")
            
        except Exception as e:
            logger.error(f"Voice service cleanup error: {str(e)}")

===== ./app/dependencies.py =====
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from app.utils.auth import verify_token, get_current_user
from app.database import AsyncSessionLocal
import logging

logger = logging.getLogger(__name__)

async def get_db() -> AsyncSession:
    """Database session dependency"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()

# Add this missing function
def get_db_session():
    """Returns the database session dependency"""
    return Depends(get_db)

# Authentication dependencies
def verify_token_dep():
    return Depends(verify_token)

def get_current_user_dep():
    return Depends(get_current_user)

===== ./app/init_db.py =====
import asyncio
import logging
from sqlalchemy import text
from app.database import engine, Base
from app.models.db_models import DBAgent, DBDocument, DBTrainingJob, DBConversation, DBMessage
from app.config import settings

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def create_schema():
    """Create the llm schema if it doesn't exist"""
    try:
        async with engine.begin() as conn:
            # Create schema
            await conn.execute(text("CREATE SCHEMA IF NOT EXISTS llm"))
            logger.info("Schema 'llm' created or already exists")
    except Exception as e:
        logger.error(f"Error creating schema: {str(e)}")
        raise

async def create_tables():
    """Create all tables in the correct order"""
    try:
        async with engine.begin() as conn:
            # Set search path to include llm schema
            await conn.execute(text("SET search_path TO llm, public"))
            
            # Create all tables
            await conn.run_sync(Base.metadata.create_all)
            logger.info("All tables created successfully")
    except Exception as e:
        logger.error(f"Error creating tables: {str(e)}")
        raise

async def verify_tables():
    """Verify that all required tables exist"""
    required_tables = [
        'agents', 'documents', 'training_jobs', 'conversations', 'messages'
    ]
    
    try:
        async with engine.begin() as conn:
            for table in required_tables:
                result = await conn.execute(text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'llm' 
                        AND table_name = '{table}'
                    )
                """))
                exists = result.scalar()
                if exists:
                    logger.info(f"‚úÖ Table 'llm.{table}' exists")
                else:
                    logger.error(f"‚ùå Table 'llm.{table}' does not exist")
    except Exception as e:
        logger.error(f"Error verifying tables: {str(e)}")
        raise

async def init_db():
    """Initialize the database with schema and tables"""
    try:
        logger.info("Starting database initialization...")
        
        # Step 1: Create schema
        await create_schema()
        
        # Step 2: Create tables
        await create_tables()
        
        # Step 3: Verify tables
        await verify_tables()
        
        logger.info("‚úÖ Database initialized successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Database initialization failed: {str(e)}")
        raise
    finally:
        # Close the engine
        await engine.dispose()

if __name__ == "__main__":
    asyncio.run(init_db())

