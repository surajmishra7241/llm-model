===== ./app/routers/auth.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from datetime import timedelta
from app.utils.auth import create_access_token
from app.config import settings

router = APIRouter(tags=["Authentication"])

# Mock user database - replace with real checks in production
fake_users_db = {
    "nodejs_service": {
        "username": "nodejs_service",
        "password": "nodejs_service_password",
        "id": "nodejs_service",  # Add this line
        "sub": "nodejs_service"   # Add this line for JWT compatibility
    }
}

@router.post("/token")
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    user = fake_users_db.get(form_data.username)
    if not user or user["password"] != form_data.password:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
        )
    
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=timedelta(minutes=settings.JWT_ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    return {"access_token": access_token, "token_type": "bearer"}
===== ./app/routers/execute.py =====
from fastapi import APIRouter, Depends
from app.dependencies import get_current_user
from app.services.agent_service import execute_agent

router = APIRouter(prefix="/execute", tags=["Execution"])

@router.post("/{agent_id}")
async def execute_agent_endpoint(
    agent_id: str,
    input_data: dict,
    user: dict = Depends(get_current_user)
):
    return await execute_agent(agent_id, user["sub"], input_data)
===== ./app/routers/__init__.py =====
# Empty file to make routers a package
===== ./app/routers/agents.py =====
# Updated agents.py
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse
from app.services.agent_service import AgentService
from app.dependencies import get_db, get_current_user
from typing import List
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse


router = APIRouter(prefix="/agents", tags=["agents"])

@router.post("", response_model=AgentResponse, status_code=status.HTTP_201_CREATED)
async def create_agent(
    agent_data: "AgentCreate",
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    try:
        return await service.create_agent(user["sub"], agent_data)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("", response_model=List[AgentResponse])
async def list_agents(
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    return await service.list_agents(user["sub"])

@router.get("/{agent_id}", response_model=AgentResponse)
async def get_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    agent = await service.get_agent(agent_id, user["sub"])
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    return agent

@router.put("/{agent_id}", response_model=AgentResponse)
async def update_agent(
    agent_id: str,
    agent_data: AgentUpdate,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    agent = await service.update_agent(agent_id, user["sub"], agent_data)
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    return agent

@router.delete("/{agent_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    if not await service.delete_agent(agent_id, user["sub"]):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
===== ./app/routers/voice.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from app.models.voice_model import VoiceResponse, TextToSpeechRequest
from app.services.voice_service import VoiceService
from app.dependencies import get_current_user
from fastapi.responses import StreamingResponse
import io

router = APIRouter()

@router.post("/stt", response_model=VoiceResponse)
async def speech_to_text(
    audio_file: UploadFile = File(...),
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert speech to text"""
    try:
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
        
        text = await voice_service.speech_to_text(audio_file)
        return VoiceResponse(text=text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tts")
async def text_to_speech(
    request: TextToSpeechRequest,
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert text to speech"""
    try:
        audio_data = await voice_service.text_to_speech(request.text)
        return StreamingResponse(
            io.BytesIO(audio_data),
            media_type="audio/wav"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/routers/chat.py =====
from fastapi import APIRouter, Depends, HTTPException
from app.models.response_schema import ChatRequest, ChatResponse
from app.services.llm_service import OllamaService
from app.dependencies import get_current_user

# Only one APIRouter definition, without a conflicting prefix
router = APIRouter(tags=["chat"])

@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(
    chat_request: ChatRequest,
    llm_service: OllamaService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Chat with an agent"""
    try:
        messages = [
            {"role": "system", "content": chat_request.system_prompt},
            {"role": "user", "content": chat_request.message}
        ]
        
        response = await llm_service.chat(
            messages=messages,
            model=chat_request.model,
            options=chat_request.options
        )
        
        return ChatResponse(
            message=response.get("message", {}).get("content", ""),
            model=chat_request.model,
            context=response.get("context", []),
            tokens_used=response.get("eval_count", 0)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

===== ./app/routers/rag.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, status
from typing import List, Optional
from app.models.response_schema import RAGResponse, DocumentResponse, RAGQueryRequest
from app.services.rag_service import RAGService
from app.dependencies import get_current_user, get_db
from sqlalchemy.ext.asyncio import AsyncSession
import logging

router = APIRouter(prefix="/rag", tags=["RAG Operations"])
logger = logging.getLogger(__name__)

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post(
    "/upload",
    response_model=DocumentResponse,
    status_code=status.HTTP_201_CREATED
)
async def upload_document(
    file: UploadFile = File(...),
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload a document for RAG processing"""
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No filename provided"
        )

    try:
        # Validate file type
        if not file.filename.lower().endswith(('.pdf', '.txt', '.md')):
            raise HTTPException(
                status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail="Only PDF, TXT, and MD files are supported"
            )

        contents = await file.read()
        
        # Extract user ID properly
        user_id = extract_user_id(user)
        
        doc_id = await rag_service.ingest_document(
            db=db,
            user_id=user_id,
            filename=file.filename,
            content=contents
        )

        return DocumentResponse(
            document_id=str(doc_id),
            filename=file.filename,
            status="success"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document upload failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process document"
        )

@router.post("/query", response_model=RAGResponse)
async def query_documents(
    request: RAGQueryRequest,  # Changed from individual parameters to request model
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Query documents using RAG"""
    if not request.query or len(request.query.strip()) < 3:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Query must be at least 3 characters"
        )

    try:
        user_id = extract_user_id(user)
        
        results = await rag_service.query(
            db=db,
            user_id=user_id,
            query=request.query,
            max_results=request.max_results
        )
        
        return RAGResponse(
            answer=results.get("answer", "No answer found"),
            documents=results.get("documents", []),
            context=results.get("context", []),
            sources=results.get("sources", [])
        )
    except Exception as e:
        logger.error(f"Query failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process query"
        )

@router.get("/documents", response_model=List[DocumentResponse])
async def list_documents(
    page: int = 1,
    per_page: int = 10,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List all documents for the user with pagination"""
    try:
        if page < 1 or per_page < 1:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Page and per_page must be positive integers"
            )

        user_id = extract_user_id(user)

        return await rag_service.list_documents(
            db=db,
            user_id=user_id,
            page=page,
            per_page=per_page
        )

    except Exception as e:
        logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve documents"
        )

@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Delete a specific document"""
    try:
        # Clean the document ID by removing any quotes or special characters
        document_id = document_id.strip('"\'')
        user_id = extract_user_id(user)
        
        logger.info(f"Attempting to delete document {document_id} for user {user_id}")
        
        success = await rag_service.delete_document(
            db=db,
            user_id=user_id,
            document_id=document_id
        )
        
        if not success:
            logger.warning(f"Document not found: {document_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or not owned by user"
            )
            
        logger.info(f"Successfully deleted document {document_id}")
        return {"status": "success", "deleted_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document deletion failed for {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete document: {str(e)}"
        )
===== ./app/routers/training.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks, status, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobCreate, 
    TrainingProgress,
    TrainingDataItem,
    TrainingDataType,
    is_supported_file,
    get_file_type,
    FileUploadInfo
)
from app.services.training_service import TrainingService
from app.dependencies import get_current_user
import logging
import uuid
import json
import os
from datetime import datetime

router = APIRouter()
logger = logging.getLogger(__name__)

# Global instance to maintain state across requests
_training_service_instance = TrainingService()

def get_training_service() -> TrainingService:
    """Get training service singleton instance"""
    return _training_service_instance

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post("", response_model=TrainingJobResponse)
async def create_training_job(
    training_data: TrainingJobCreate,
    background_tasks: BackgroundTasks,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a new training job with enhanced data support"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job for user_id: {user_id}, agent_id: {training_data.agent_id}")
        
        # Validate that at least some data is provided
        has_data = any([
            training_data.data_urls,
            training_data.training_data,
            training_data.text_data
        ])
        
        if not has_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one data source must be provided (data_urls, training_data, or text_data)"
            )
            
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=training_data.agent_id,
            data_urls=training_data.data_urls or [],
            training_data=training_data.training_data or [],
            text_data=training_data.text_data or [],
            config=training_data.config or {}
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created enhanced job: {job.id} for user: {user_id}, agent: {training_data.agent_id}")
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.post("/upload", response_model=TrainingJobResponse)
async def create_training_job_with_files(
    files: List[UploadFile],
    background_tasks: BackgroundTasks,
    agent_id: str = Form(...),
    text_data: Optional[str] = Form(None),  # JSON string of text array
    config: Optional[str] = Form("{}"),     # JSON string of config
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a training job with file uploads"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job with files for user_id: {user_id}, agent_id: {agent_id}")
        
        # Validate files
        if not files or len(files) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one file must be uploaded"
            )
        
        uploaded_files = []
        training_data_items = []
        
        for file in files:
            if not file.filename:
                continue
                
            if not is_supported_file(file.filename):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"File type not supported: {file.filename}"
                )
            
            # Generate unique file ID
            file_id = str(uuid.uuid4())
            file_content = await file.read()
            
            # Store file information
            file_info = FileUploadInfo(
                filename=file.filename,
                content_type=file.content_type or "application/octet-stream",
                size=len(file_content),
                file_id=file_id
            )
            uploaded_files.append(file_info)
            
            # Save file temporarily for processing
            temp_path = await training_service.save_uploaded_file(
                user_id, file_id, file.filename, file_content
            )
            
            # Create training data item
            data_item = TrainingDataItem(
                type=get_file_type(file.filename),
                content=temp_path,
                metadata={
                    "filename": file.filename,
                    "file_id": file_id,
                    "content_type": file.content_type,
                    "size": len(file_content)
                }
            )
            training_data_items.append(data_item)
        
        # Parse text data if provided
        parsed_text_data = []
        if text_data:
            try:
                parsed_text_data = json.loads(text_data)
                if not isinstance(parsed_text_data, list):
                    raise ValueError("text_data must be a JSON array")
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for text_data"
                )
        
        # Parse config
        parsed_config = {}
        if config:
            try:
                parsed_config = json.loads(config)
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for config"
                )
        
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=agent_id,
            data_urls=[],
            training_data=training_data_items,
            text_data=parsed_text_data,
            uploaded_files=uploaded_files,
            config=parsed_config
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created file-based job: {job.id} with {len(uploaded_files)} files")
        return job
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job with files: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("", response_model=List[TrainingJobResponse])
async def list_training_jobs(
    agent_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """List training jobs for an agent"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Listing jobs for user_id: {user_id}, agent_id: {agent_id}")
        
        jobs = await training_service.list_jobs(user_id, agent_id)
        logger.info(f"Found {len(jobs)} jobs for user: {user_id}, agent: {agent_id}")
        
        return jobs
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing training jobs: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )

@router.get("/{job_id}/progress", response_model=TrainingProgress)
async def get_training_progress(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get real-time training progress"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting progress for job: {job_id}, user: {user_id}")
        
        progress = await training_service.get_progress(user_id, job_id)
        if not progress:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or access denied"
            )
        
        return progress
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training progress: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.delete("/{job_id}")
async def cancel_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Cancel a training job"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Cancelling job: {job_id} for user: {user_id}")
        
        success = await training_service.cancel_job(user_id, job_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or cannot be cancelled"
            )
        
        return {"message": "Job cancelled successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/debug/all-jobs")
async def debug_all_jobs(
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Debug endpoint to see all jobs in memory"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Debug: Getting all jobs for user: {user_id}")
        
        # Access the jobs dictionary directly for debugging
        all_jobs = []
        for job_id, job in training_service.jobs.items():
            job_info = {
                "job_id": job_id,
                "user_id": job.user_id,
                "agent_id": job.agent_id,
                "status": job.status,
                "created_at": job.created_at,
                "data_sources": {
                    "urls": len(job.data_urls),
                    "training_data": len(job.training_data),
                    "text_data": len(job.text_data),
                    "uploaded_files": len(job.uploaded_files)
                }
            }
            all_jobs.append(job_info)
        
        # Filter for current user
        user_jobs = [job for job in all_jobs if job["user_id"] == user_id]
        
        return {
            "current_user_id": user_id,
            "total_jobs_in_system": len(all_jobs),
            "jobs_for_current_user": len(user_jobs),
            "all_jobs": all_jobs,
            "user_jobs": user_jobs
        }
    except Exception as e:
        logger.error(f"Debug endpoint error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{job_id}", response_model=TrainingJobResponse)
async def get_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get training job details"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting job: {job_id} for user: {user_id}")
        
        job = await training_service.get_job(user_id, job_id)
        if not job:
            logger.warning(f"Job not found: {job_id} for user: {user_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, 
                detail="Job not found"
            )
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )
===== ./app/routers/agent_interaction.py =====
# ./app/routers/agent_interaction.py
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from app.services.agent_interaction_service import AgentInteractionService
from app.dependencies import get_db, get_current_user
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
import logging

router = APIRouter(prefix="/interact", tags=["Agent Interaction"])
logger = logging.getLogger(__name__)

@router.post("/chat/{agent_id}")
async def chat_with_agent(
    agent_id: str,
    input_data: dict,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Chat with an agent using text"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            input_text=input_data.get("message"),
            db=db
        )
        return response
    except Exception as e:
        logger.error(f"Chat failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/voice/{agent_id}")
async def voice_interaction(
    agent_id: str,
    audio_file: UploadFile = File(...),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Interact with an agent using voice"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
            
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            audio_file=audio_file,
            db=db
        )
        
        # Convert response to speech
        speech_response = await service.text_to_speech(
            response["text_response"],
            response["emotional_state"]
        )
        
        return {
            "text": response["text_response"],
            "audio": speech_response,
            "emotion": response["emotional_state"]
        }
    except Exception as e:
        logger.error(f"Voice interaction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/middleware/logging_middleware.py =====
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response
import logging
from typing import Awaitable, Callable

logger = logging.getLogger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(
        self, 
        request: Request, 
        call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        # Log incoming request
        logger.info(f"Incoming request: {request.method} {request.url}")
        
        try:
            response = await call_next(request)
            # Log successful response
            logger.info(
                f"Request completed: {request.method} {request.url} "
                f"- Status: {response.status_code}"
            )
            return response
        except HTTPException as http_exc:
            # Log HTTP exceptions
            logger.warning(
                f"HTTP Exception: {request.method} {request.url} "
                f"- Status: {http_exc.status_code} - Detail: {http_exc.detail}"
            )
            raise
        except Exception as exc:
            # Log unexpected errors
            logger.error(
                f"Unexpected error: {request.method} {request.url} "
                f"- Error: {str(exc)}",
                exc_info=True
            )
            raise
===== ./app/middleware/rate_limiter.py =====
# middleware/rate_limiter.py

from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio
from collections import defaultdict
import time
import logging

logger = logging.getLogger(__name__)

class RateLimiterMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_requests=1000, time_window=60):
        super().__init__(app)
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_counts = defaultdict(lambda: {'count': 0, 'window_start': time.time()})
        self.lock = asyncio.Lock()

    async def dispatch(self, request: Request, call_next):
        client_ip = request.client.host if request.client else "unknown"
        
        async with self.lock:
            current_time = time.time()
            record = self.request_counts[client_ip]
            
            # Reset counter if time window has passed
            if current_time - record['window_start'] > self.time_window:
                record['count'] = 0
                record['window_start'] = current_time
            
            # Check rate limit
            if record['count'] >= self.max_requests:
                logger.warning(f"Rate limit exceeded for {client_ip}")
                return JSONResponse(
                    status_code=429,
                    content={"detail": "Too many requests"},
                    headers={"Retry-After": str(self.time_window)}
                )
            
            record['count'] += 1
        
        response = await call_next(request)
        return response
===== ./app/middleware/errorHandlingMiddleware.py =====
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import logging
import traceback
import time
from typing import Callable

logger = logging.getLogger(__name__)

class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        start_time = time.time()
        
        try:
            response = await call_next(request)
            return response
            
        except HTTPException as e:
            # Let FastAPI handle HTTP exceptions normally
            raise e
            
        except Exception as e:
            # Log the full error with traceback
            process_time = time.time() - start_time
            
            error_details = {
                "method": request.method,
                "url": str(request.url),
                "client": getattr(request.client, "host", "unknown") if request.client else "unknown",
                "process_time": f"{process_time:.3f}s",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            
            logger.error(
                f"Unhandled exception in {request.method} {request.url.path}: {str(e)}",
                extra=error_details
            )
            
            # Return a generic error response
            return JSONResponse(
                status_code=500,
                content={
                    "detail": "Internal server error",
                    "error_type": type(e).__name__,
                    "timestamp": time.time(),
                    "path": request.url.path
                }
            )
===== ./app/middleware/metrics_middleware.py =====
from prometheus_client import Counter, Histogram, make_asgi_app
from fastapi import Request, Response
from app.config import settings
import time

if settings.PROMETHEUS_ENABLED:
    REQUEST_COUNT = Counter(
        "http_requests_total",
        "Total HTTP Requests",
        ["method", "path", "status_code"]
    )
    
    REQUEST_LATENCY = Histogram(
        "http_request_duration_seconds",
        "HTTP Request Latency",
        ["method", "path"]
    )
    
    metrics_app = make_asgi_app()
    
    async def metrics_middleware(request: Request, call_next):
        if request.url.path == "/metrics":
            return await metrics_app(request.scope, request.receive, request.send)
            
        start_time = time.time()
        method = request.method
        path = request.url.path
        
        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception:
            status_code = 500
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_COUNT.labels(method, path, status_code).inc()
            REQUEST_LATENCY.labels(method, path).observe(duration)
            
        return response
else:
    async def metrics_middleware(request: Request, call_next):
        return await call_next(request)
===== ./app/config.py =====
# config.py

from pydantic_settings import BaseSettings
from typing import List, Optional, Dict, Any
from pydantic import Field, PostgresDsn, validator, RedisDsn, HttpUrl, conint
import logging
from urllib.parse import urlparse
import os

class Settings(BaseSettings):
    # App settings
    APP_VERSION: str = "1.0.0"
    APP_NAME: str = "AI Agent Platform"
    DEBUG: bool = False
    ENVIRONMENT: str = "development"
    
    # API settings
    API_PREFIX: str = "/api/v1"
    CORS_ORIGINS: List[str] = ["*"]
    
    # Database
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_SERVER: str = "localhost"
    POSTGRES_PORT: int = 5432
    POSTGRES_DB: str = "llm_agents"
    DATABASE_URL: Optional[PostgresDsn] = None
    
    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        return str(PostgresDsn.build(
            scheme="postgresql+asyncpg",
            username=values.get("POSTGRES_USER"),
            password=values.get("POSTGRES_PASSWORD"),
            host=values.get("POSTGRES_SERVER"),
            port=values.get("POSTGRES_PORT"),
            path=f"/{values.get('POSTGRES_DB') or ''}",
        ))
    
    # Redis
    REDIS_URL: RedisDsn = "redis://localhost:6379/0"
    CACHE_ENABLED: bool = True
    CACHE_TTL: int = 300
    
    # Ollama Configuration
    OLLAMA_URL: HttpUrl = "http://localhost:11434"
    DEFAULT_OLLAMA_MODEL: str = "deepseek-r1:1.5b"
    OLLAMA_TIMEOUT: conint(gt=0) = 60  # Increased timeout
    
    # Qdrant Configuration
    QDRANT_URL: HttpUrl = "http://localhost:6333"
    QDRANT_API_KEY: Optional[str] = None
    QDRANT_MAX_WORKERS: conint(gt=0) = 8
    QDRANT_COLLECTION_NAME: str = "documents"
    QDRANT_BATCH_SIZE: conint(gt=0) = 100
    QDRANT_TIMEOUT: conint(gt=0) = 30
    
    # Embedding Model Configuration
    # Use the same model as default for embeddings initially
    EMBEDDING_MODEL: str = "deepseek-r1:1.5b"  # Changed to use main model
    EXPECTED_EMBEDDING_DIMENSION: Optional[conint(gt=0)] = None
    EMBEDDING_FALLBACK_DIMENSION: conint(gt=0) = 768
    
    # Alternative embedding models to try (in order of preference)
    EMBEDDING_MODEL_ALTERNATIVES: List[str] = [
        "deepseek-r1:1.5b",
        "nomic-embed-text",
        "all-minilm",
        "mxbai-embed-large"
    ]

    # Voice settings
    ENABLE_WHISPER: bool = True
    WHISPER_MODEL: str = "base"  # or "small", "medium", "large"
    ENABLE_TTS: bool = True
    TTS_ENGINE: str = "coqui"  # or "pyttsx3"
    TTS_MODEL: str = "tts_models/en/ljspeech/glow-tts"
    TTS_VOICE: str = "female"
    
    # Document Processing
    DEFAULT_CHUNK_SIZE: conint(gt=0) = 1000
    DEFAULT_CHUNK_OVERLAP: conint(ge=0) = 200
    MAX_DOCUMENT_SIZE_MB: conint(gt=0) = 50
    SUPPORTED_FILE_TYPES: List[str] = [".pdf", ".txt", ".docx", ".pptx", ".xlsx"]
    
    # RAG Settings
    RAG_DEFAULT_MAX_RESULTS: conint(gt=0) = 5
    RAG_DEFAULT_MIN_SCORE: float = Field(ge=0, le=1, default=0.3)
    RAG_QUERY_TIMEOUT: conint(gt=0) = 30
    
    # Auth
    JWT_SECRET_KEY: str = "your-secret-key-here"
    JWT_ALGORITHM: str = "HS256"
    JWT_ACCESS_TOKEN_EXPIRE_MINUTES: int = 11440  # 24 hours
    
    # Monitoring
    PROMETHEUS_ENABLED: bool = False
    PROMETHEUS_PORT: conint(gt=0, le=65535) = 8001
    
    # Logging
    LOG_LEVEL: str = Field(pattern="^(DEBUG|INFO|WARNING|ERROR|CRITICAL)$", default="INFO")
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # Embedding fallback strategy
    USE_DETERMINISTIC_FALLBACK: bool = True
    FALLBACK_EMBEDDING_STRATEGY: str = Field(
        pattern="^(deterministic|hash|features|skip)$", 
        default="deterministic"
    )
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True
        extra = "ignore"

# Create settings instance
settings = Settings()

def validate_config():
    """Validate and log configuration"""
    logger = logging.getLogger(__name__)
    
    try:
        # Validate URLs
        parsed_ollama = urlparse(str(settings.OLLAMA_URL))
        if not all([parsed_ollama.scheme, parsed_ollama.netloc]):
            raise ValueError("Invalid OLLAMA_URL format")
            
        parsed_qdrant = urlparse(str(settings.QDRANT_URL))
        if not all([parsed_qdrant.scheme, parsed_qdrant.netloc]):
            raise ValueError("Invalid QDRANT_URL format")
        
        # Log important config
        logger.info("=== Application Configuration ===")
        logger.info(f"Environment: {settings.ENVIRONMENT}")
        logger.info(f"Debug Mode: {settings.DEBUG}")
        logger.info("")
        logger.info("=== Ollama Configuration ===")
        logger.info(f"Ollama URL: {settings.OLLAMA_URL}")
        logger.info(f"Default Model: {settings.DEFAULT_OLLAMA_MODEL}")
        logger.info(f"Embedding Model: {settings.EMBEDDING_MODEL}")
        logger.info(f"Timeout: {settings.OLLAMA_TIMEOUT}s")
        logger.info(f"Alternative Models: {settings.EMBEDDING_MODEL_ALTERNATIVES}")
        logger.info("")
        logger.info("=== Vector Database Configuration ===")
        logger.info(f"Qdrant URL: {settings.QDRANT_URL}")
        logger.info(f"Collection Name: {settings.QDRANT_COLLECTION_NAME}")
        logger.info(f"Expected Dimension: {settings.EXPECTED_EMBEDDING_DIMENSION or 'Auto-detect'}")
        logger.info(f"Fallback Dimension: {settings.EMBEDDING_FALLBACK_DIMENSION}")
        logger.info("")
        logger.info("=== Document Processing ===")
        logger.info(f"Chunk Size: {settings.DEFAULT_CHUNK_SIZE}")
        logger.info(f"Chunk Overlap: {settings.DEFAULT_CHUNK_OVERLAP}")
        logger.info(f"Max File Size: {settings.MAX_DOCUMENT_SIZE_MB}MB")
        logger.info(f"Supported Types: {settings.SUPPORTED_FILE_TYPES}")
        logger.info("")
        logger.info("=== RAG Configuration ===")
        logger.info(f"Max Results: {settings.RAG_DEFAULT_MAX_RESULTS}")
        logger.info(f"Min Score Threshold: {settings.RAG_DEFAULT_MIN_SCORE}")
        logger.info(f"Query Timeout: {settings.RAG_QUERY_TIMEOUT}s")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {str(e)}")
        raise

# Validate on import (only in production-like environments)
if "pytest" not in os.sys.modules and __name__ != "__main__":
    try:
        validate_config()
    except Exception as e:
        print(f"Warning: Configuration validation failed: {e}")
        # Don't raise in case this is being imported during setup
===== ./app/database.py =====
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.pool import QueuePool
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Get the database URL as string and ensure it uses asyncpg
database_url = str(settings.DATABASE_URL).replace(
    "postgresql://", 
    "postgresql+asyncpg://"
)

engine = create_async_engine(
    database_url,  # Use the modified URL string
    echo=settings.DEBUG,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    connect_args={
        "server_settings": {
            "application_name": settings.APP_NAME,
            "search_path": "llm,public"
        }
    }
)

AsyncSessionLocal = sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False
)

Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception as e:
            logger.error(f"Database error: {str(e)}")
            await session.rollback()
            raise
        finally:
            await session.close()
===== ./app/__init__.py =====
# Initialize application package
from .config import settings

__all__ = ["settings"]
===== ./app/utils/auth.py =====
from datetime import datetime, timedelta
from jose import JWTError, jwt
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from app.config import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/auth/token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode, 
        settings.JWT_SECRET_KEY, 
        algorithm=settings.JWT_ALGORITHM
    )
    return encoded_jwt

# In utils/auth.py
async def verify_token(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            token, 
            settings.JWT_SECRET_KEY, 
            algorithms=[settings.JWT_ALGORITHM]
        )
        # Ensure sub (subject) is present
        if "sub" not in payload:
            raise credentials_exception
        return payload
    except JWTError:
        raise credentials_exception

# Add this function
def get_current_user(token_payload: dict = Depends(verify_token)):
    """
    Dependency that can be used to get the current user from the token
    """
    return token_payload
===== ./app/utils/logging.py =====
import logging
from logging.config import dictConfig
from app.config import settings
import sys

def configure_logging():
    logging_config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": settings.LOG_FORMAT,
                "datefmt": "%Y-%m-%d %H:%M:%S"
            },
        },
        "handlers": {
            "console": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.StreamHandler",
                "stream": sys.stdout,
            },
            "file": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "app.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "encoding": "utf8"
            },
        },
        "loggers": {
            "app": {
                "handlers": ["console", "file"],
                "level": settings.LOG_LEVEL,
                "propagate": False
            },
            "sqlalchemy": {
                "handlers": ["console"],
                "level": "WARNING",
                "propagate": False
            },
        }
    }
    
    dictConfig(logging_config)
    logger = logging.getLogger("app")
    logger.info("Logging configured successfully")
    return logger

logger = configure_logging()
===== ./app/utils/cache_decorator.py =====
from typing import Any, Optional, Callable, Awaitable
from functools import wraps
from app.services.cache import cache_service
import json
import hashlib
import inspect

def cached(key_pattern: str, ttl: Optional[int] = None):
    """Decorator for caching async function results with smart key generation"""
    def decorator(func: Callable[..., Awaitable[Any]]):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not cache_service.enabled:
                return await func(*args, **kwargs)
            
            try:
                # Get function signature to map args to parameter names
                sig = inspect.signature(func)
                bound_args = sig.bind(*args, **kwargs)
                bound_args.apply_defaults()
                
                # Create a dictionary of all parameters
                params = dict(bound_args.arguments)
                
                # Generate query_hash if needed and not provided
                if 'query_hash' in key_pattern and 'query_hash' not in params:
                    query = params.get('query', '')
                    if query:
                        params['query_hash'] = hashlib.md5(query.encode()).hexdigest()[:8]
                    else:
                        params['query_hash'] = 'no_query'
                
                # Format the cache key with available parameters
                try:
                    cache_key = key_pattern.format(**params)
                except KeyError as e:
                    # If formatting fails, create a simple fallback key
                    func_name = func.__name__
                    args_hash = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()[:8]
                    cache_key = f"{func_name}:{args_hash}"
                
                # Try to get from cache
                cached_data = await cache_service.get(cache_key)
                if cached_data is not None:
                    return cached_data
                
                # Execute function and cache result
                result = await func(*args, **kwargs)
                await cache_service.set(cache_key, result, ttl)
                return result
                
            except Exception as e:
                # If caching fails, just execute the function
                return await func(*args, **kwargs)
                
        return wrapper
    return decorator
===== ./app/utils/__init__.py =====
# Empty file to make utils a package
===== ./app/utils/chroma_async.py =====
# utils/chroma_async.py
import chromadb
import logging
from typing import List, Dict, Any, Optional, Callable, AsyncIterator
from concurrent.futures import ThreadPoolExecutor
import asyncio
from functools import partial
from contextlib import asynccontextmanager
import time
import threading
from collections import defaultdict
from app.config import settings

logger = logging.getLogger(__name__)

class AsyncChromaClient:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, path: str, embedding_function: Optional[Callable] = None, max_workers: int = 4):
        if not hasattr(self, '_initialized'):
            self.path = path
            self.embedding_function = embedding_function
            self.executor = ThreadPoolExecutor(
                max_workers=max_workers, 
                thread_name_prefix="chroma",
            )
            self._local = threading.local()
            self.collections = defaultdict(dict)
            self._initialized = True
            self._closed = False
    
    async def initialize(self):
        """Initialize the client in a thread-safe way"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            loop = asyncio.get_running_loop()
            self._local.client = await loop.run_in_executor(
                self.executor,
                partial(
                    chromadb.PersistentClient,
                    path=self.path,
                )
            )
            logger.info("AsyncChromaDB client initialized for thread %s", threading.current_thread().name)
    
    @asynccontextmanager
    async def get_collection(self, name: str, embedding_function=None, metadata=None) -> AsyncIterator[chromadb.Collection]:
        """Async context manager for collection access with connection pooling"""
        if self._closed:
            raise RuntimeError("Client is closed")
            
        await self.initialize()
        
        thread_id = threading.current_thread().ident
        collection_key = f"{name}_{thread_id}"
        
        if collection_key not in self.collections[thread_id]:
            loop = asyncio.get_running_loop()
            
            # Fix: Don't pass metadata parameter if it's None or empty
            collection_kwargs = {
                'name': name,
                'embedding_function': embedding_function or self.embedding_function,
            }
            
            # Only add metadata if it's not None and not empty
            if metadata:
                collection_kwargs['metadata'] = metadata
                
            collection = await loop.run_in_executor(
                self.executor,
                partial(
                    self._local.client.get_or_create_collection,
                    **collection_kwargs
                )
            )
            self.collections[thread_id][collection_key] = collection
        
        try:
            yield self.collections[thread_id][collection_key]
        except Exception as e:
            logger.error(f"Error in collection context for {name}: {str(e)}")
            raise
        finally:
            # Clean up if this is the last reference
            pass
    
    async def add_documents(
        self, 
        collection_name: str, 
        documents: List[str], 
        metadatas: List[Dict], 
        ids: List[str],
        embeddings: Optional[List[List[float]]] = None,
        batch_size: int = 100
    ):
        """Async document addition with improved batching and error handling"""
        if not documents:
            return
            
        async with self.get_collection(collection_name) as collection:
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i + batch_size]
                batch_metas = metadatas[i:i + batch_size] if metadatas else None
                batch_ids = ids[i:i + batch_size] if ids else None
                batch_embeds = embeddings[i:i + batch_size] if embeddings else None
                
                loop = asyncio.get_running_loop()
                
                try:
                    await loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.add,
                            documents=batch_docs,
                            metadatas=batch_metas,
                            ids=batch_ids,
                            embeddings=batch_embeds
                        )
                    )
                    
                    # Small delay between batches to prevent overwhelming the system
                    if i + batch_size < len(documents):
                        await asyncio.sleep(0.05)
                        
                except Exception as e:
                    logger.error(f"Failed to add batch {i//batch_size + 1}: {str(e)}")
                    raise
    
    async def query_collection(
        self, 
        collection_name: str, 
        query_texts: Optional[List[str]] = None, 
        query_embeddings: Optional[List[List[float]]] = None,
        n_results: int = 5, 
        where: Optional[Dict] = None, 
        where_document: Optional[Dict] = None,
        include: Optional[List[str]] = None,
        timeout: float = 30.0
    ) -> Dict[str, Any]:
        """Async query with improved error handling and timeout"""
        if not query_texts and not query_embeddings:
            raise ValueError("Either query_texts or query_embeddings must be provided")
            
        async with self.get_collection(collection_name) as collection:
            loop = asyncio.get_running_loop()
            
            try:
                return await asyncio.wait_for(
                    loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.query,
                            query_texts=query_texts,
                            query_embeddings=query_embeddings,
                            n_results=n_results,
                            where=where,
                            where_document=where_document,
                            include=include or ["documents", "distances", "metadatas"]
                        )
                    ),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.error(f"Query timeout for collection {collection_name}")
                raise
            except Exception as e:
                logger.error(f"Query failed for collection {collection_name}: {str(e)}")
                raise
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        try:
            # Clear collections cache
            self.collections.clear()
            
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
                logger.info("AsyncChromaDB client closed")
                
        except Exception as e:
            logger.error(f"Error closing AsyncChromaDB client: {str(e)}")
        finally:
            self._instance = None

# Global instance with lazy initialization
_async_chroma_client = None
_async_chroma_lock = asyncio.Lock()

async def get_async_chroma_client() -> AsyncChromaClient:
    global _async_chroma_client
    async with _async_chroma_lock:
        if _async_chroma_client is None or _async_chroma_client._closed:
            _async_chroma_client = AsyncChromaClient(
                path=settings.CHROMA_PATH,
                embedding_function=None,  # Will be set per request
                max_workers=settings.CHROMA_MAX_WORKERS
            )
        return _async_chroma_client
===== ./app/utils/file_processing_training.py =====
import os
import logging
import requests
from typing import Optional, List
from io import BytesIO
import tempfile
from pathlib import Path

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup
import json
import csv

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> Optional[str]:
    """Extract text from PDF content"""
    try:
        pdf_file = BytesIO(content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        return None

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        from app.models.training_model import SUPPORTED_EXTENSIONS
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }
===== ./app/utils/file_processing.py =====
import os
import logging
import requests
from typing import Optional, List, Tuple, Dict, Any
from io import BytesIO
import tempfile
from pathlib import Path
import fitz  # PyMuPDF
import re
import json
import csv

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = os.path.splitext(filename.lower())[1]
    return ext in SUPPORTED_EXTENSIONS

def get_file_type(filename: str) -> str:
    """Determine the file type based on extension"""
    ext = os.path.splitext(filename.lower())[1]
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return 'image'
    elif ext in {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}:
        return 'audio'
    elif ext in {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'}:
        return 'video'
    else:
        return 'document'

def clean_text(text: str) -> str:
    """Clean text by removing excessive whitespace"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> str:
    """Extract text from PDF bytes using PyMuPDF"""
    try:
        with fitz.open(stream=content, filetype="pdf") as doc:
            text = "\n".join([page.get_text() for page in doc])
        return clean_text(text)
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {e}")
        return ""

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }

    
===== ./app/utils/health_check.py =====
from fastapi import APIRouter, status
from fastapi.responses import JSONResponse
from app.config import settings
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/health", tags=["Health Check"])
async def health_check():
    """Basic health check endpoint"""
    try:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "healthy",
                "version": settings.APP_VERSION,
                "environment": settings.ENVIRONMENT
            }
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"status": "unhealthy", "error": str(e)}
        )
===== ./app/utils/helpers.py =====
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Convert Pydantic PostgresDsn to string and ensure proper format
def get_database_url() -> str:
    url = str(settings.DATABASE_URL)
    # Ensure there's no double slash before the database name
    if '//' in url.split('@')[-1]:
        parts = url.split('@')
        parts[-1] = parts[-1].replace('//', '/')
        url = '@'.join(parts)
    return url

# Create engine with properly formatted URL string
engine = create_async_engine(
    get_database_url(),
    future=True,
    echo=True
)

AsyncSessionLocal = sessionmaker(
    engine, 
    class_=AsyncSession,
    expire_on_commit=False
)

Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as db:
        yield db
===== ./app/utils/qdrant_async.py =====
import logging
import asyncio
import threading
from typing import List, Dict, Any, Optional, Tuple, Union
from contextlib import asynccontextmanager
from functools import partial
from concurrent.futures import ThreadPoolExecutor
import uuid
import time
from dataclasses import dataclass

from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse, ResponseHandlingException
from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from app.config import settings # Ensure this import path is correct based on your project structure

logger = logging.getLogger(__name__)

class QdrantConnectionError(Exception):
    """Custom exception for Qdrant connection issues"""
    pass

class QdrantOperationError(Exception):
    """Custom exception for Qdrant operation failures"""
    pass

@dataclass
class CollectionInfo:
    name: str
    status: str
    vector_size: int
    distance: str
    points_count: int

class AsyncQdrantClient:
    """Thread-safe async wrapper for Qdrant client with improved error handling"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, url: str = None, api_key: str = None, max_workers: int = None):
        if self._initialized:
            return
            
        self.url = url or str(settings.QDRANT_URL)
        self.api_key = api_key or settings.QDRANT_API_KEY
        self.max_workers = max_workers or settings.QDRANT_MAX_WORKERS
        self.timeout = settings.QDRANT_TIMEOUT
        self.batch_size = settings.QDRANT_BATCH_SIZE
        
        self.executor = ThreadPoolExecutor(
            max_workers=self.max_workers, 
            thread_name_prefix="qdrant_worker"
        )
        self._local = threading.local()
        self._initialized = True
        self._closed = False
        self._connection_verified = False
        
        logger.info(f"AsyncQdrantClient initialized with URL: {self.url}")
    
    def _get_client(self) -> QdrantClient:
        """Get thread-local client instance"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            try:
                self._local.client = QdrantClient(
                    url=self.url,
                    api_key=self.api_key,
                    prefer_grpc=False, # Use HTTP API
                    timeout=self.timeout,
                    grpc_options={ # These are ignored if prefer_grpc is False, but kept for completeness
                        "grpc.keepalive_time_ms": 30000,
                        "grpc.max_receive_message_length": 100 * 1024 * 1024  # 100MB
                    }
                )
                logger.debug(f"Created new Qdrant client for thread {threading.current_thread().name}")
            except Exception as e:
                logger.error(f"Failed to create Qdrant client: {str(e)}", exc_info=True)
                raise QdrantConnectionError(f"Cannot connect to Qdrant at {self.url}: {str(e)}")
        
        return self._local.client
    
    async def _verify_connection(self) -> bool:
        """Verify Qdrant connection and service availability"""
        if self._connection_verified:
            return True
            
        try:
            loop = asyncio.get_running_loop()
            
            def _check_connection():
                client = self._get_client()
                try:
                    # First try to get collections as basic health check
                    collections = client.get_collections()
                    logger.info(f"Qdrant connection verified - {len(collections.collections)} collections found")
                    return True
                except Exception as e:
                    logger.error(f"Qdrant connection check failed: {str(e)}")
                    raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
            
            result = await loop.run_in_executor(self.executor, _check_connection)
            self._connection_verified = result
            logger.info("Qdrant connection verified successfully")
            return result
            
        except Exception as e:
            logger.error(f"Qdrant connection verification failed: {str(e)}", exc_info=True)
            raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
    
    async def initialize(self):
        """Initialize and verify the client connection"""
        if self._closed:
            raise QdrantConnectionError("Client has been closed")
            
        await self._verify_connection()
        logger.info("AsyncQdrant client initialized and verified")
    
    async def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine",
        recreate_if_exists: bool = False
    ) -> bool:
        """Create a new collection with specified parameters"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _create_collection():
                client = self._get_client()
                
                # Check if collection exists
                try:
                    collections = client.get_collections()
                    existing_names = [col.name for col in collections.collections]
                    
                    if collection_name in existing_names:
                        if recreate_if_exists:
                            logger.info(f"Recreating existing collection: {collection_name}")
                            client.delete_collection(collection_name)
                            # Give a small moment for deletion to propagate if recreating
                            time.sleep(0.1) 
                        else:
                            logger.info(f"Collection {collection_name} already exists")
                            return True
                except Exception as e:
                    logger.warning(f"Error checking existing collections: {str(e)}")
                    # If checking collections fails, and we are not forcing recreate, raise
                    if not recreate_if_exists:
                        raise QdrantOperationError(f"Could not verify collection existence: {str(e)}")
            
                # Create collection with proper configuration
                distance_enum = Distance[distance.upper()] if hasattr(Distance, distance.upper()) else Distance.COSINE
                
                result = client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=distance_enum
                    )
                )
                
                logger.info(f"Created collection {collection_name} with {vector_size}D vectors")
                return True
            
            return await loop.run_in_executor(self.executor, _create_collection)
            
        except Exception as e:
            logger.error(f"Failed to create collection {collection_name}: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Collection creation failed: {str(e)}")
    
    async def collection_exists(self, collection_name: str) -> bool:
        """Check if collection exists"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _check_collection():
                client = self._get_client()
                try:
                    collections = client.get_collections()
                    return collection_name in [col.name for col in collections.collections]
                except Exception as e:
                    logger.warning(f"Error checking collection existence: {str(e)}")
                    return False
            
            return await loop.run_in_executor(self.executor, _check_collection)
            
        except Exception as e:
            logger.error(f"Error checking collection existence: {str(e)}", exc_info=True)
            return False
    
    async def get_collection_info(self, collection_name: str) -> Optional[CollectionInfo]:
        """Get collection information"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _get_info():
                client = self._get_client()
                try:
                    info = client.get_collection(collection_name)
                    return CollectionInfo(
                        name=collection_name,
                        status=info.status.value if hasattr(info.status, 'value') else str(info.status),
                        vector_size=info.config.params.vectors.size,
                        distance=info.config.params.vectors.distance.value,
                        points_count=info.points_count
                    )
                except Exception as e:
                    logger.error(f"Error getting collection info: {str(e)}")
                    return None
            
            return await loop.run_in_executor(self.executor, _get_info)
            
        except Exception as e:
            logger.error(f"Failed to get collection info: {str(e)}", exc_info=True)
            return None
    
    async def upsert(
        self,
        collection_name: str,
        points: List[PointStruct],
        batch_size: int = None
    ) -> bool:
        """Upsert points into collection with batching"""
        await self.initialize()
        
        if not points:
            logger.warning("No points to upsert")
            return True
        
        batch_size = batch_size or self.batch_size
        
        try:
            loop = asyncio.get_running_loop()
            
            def _upsert_batch(batch_points):
                client = self._get_client()
                
                # Validate points before upsert
                for point in batch_points:
                    if not isinstance(point.vector, list) or len(point.vector) == 0:
                        raise ValueError(f"Invalid vector for point {point.id}")
                    if not isinstance(point.payload, dict):
                        raise ValueError(f"Invalid payload for point {point.id}")
                
                result = client.upsert(
                    collection_name=collection_name,
                    points=batch_points,
                    wait=True
                )
                return result
            
            # Process in batches
            total_points = len(points)
            logger.info(f"Upserting {total_points} points in batches of {batch_size}")
            
            success_count = 0
            for i in range(0, total_points, batch_size):
                batch = points[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_points + batch_size - 1) // batch_size
                
                logger.debug(f"Processing batch {batch_num}/{total_batches} ({len(batch)} points)")
                
                try:
                    result = await loop.run_in_executor(self.executor, _upsert_batch, batch)
                    if result and result.status == models.UpdateStatus.COMPLETED:
                        success_count += len(batch)
                    else:
                        logger.error(f"Batch {batch_num} upsert failed with status: {result.status if result else 'N/A'}")
                        raise QdrantOperationError(f"Batch {batch_num} upsert failed")
                    
                    # Small delay between batches to avoid overwhelming Qdrant
                    if batch_num < total_batches:
                        await asyncio.sleep(0.01)
                except Exception as e:
                    logger.error(f"Failed to upsert batch {batch_num}: {str(e)}")
                    raise
            
            logger.info(f"Successfully upserted {success_count}/{total_points} points to {collection_name}")
            return success_count == total_points
            
        except Exception as e:
            logger.error(f"Failed to upsert points: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Upsert operation failed: {str(e)}")
    
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        query_filter: Optional[Filter] = None,
        limit: int = 5,
        with_vectors: bool = False,
        with_payload: bool = True,
        score_threshold: Optional[float] = None
    ) -> List[models.ScoredPoint]:
        """Search collection with query vector"""
        await self.initialize()
        
        if not query_vector:
            raise ValueError("Query vector cannot be empty")
        
        try:
            loop = asyncio.get_running_loop()
            
            def _search():
                client = self._get_client()
                
                results = client.search(
                    collection_name=collection_name,
                    query_vector=query_vector,
                    query_filter=query_filter,
                    limit=max(1, min(limit, 1000)),
                    with_vectors=with_vectors,
                    with_payload=with_payload,
                    score_threshold=score_threshold
                )
                return results
            
            results = await loop.run_in_executor(self.executor, _search)
            
            logger.debug(f"Search returned {len(results)} results for collection {collection_name}")
            return results
            
        except ValueError as ve:
            logger.error(f"Validation error in search: {str(ve)}")
            raise
        except Exception as e:
            logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Search operation failed: {str(e)}")
    
    async def scroll(
        self,
        collection_name: str,
        scroll_filter: Optional[Filter] = None,
        limit: int = 10,
        offset: Optional[str] = None,
        with_vectors: bool = False,
        with_payload: bool = True
    ) -> Tuple[List[models.Record], Optional[str]]:
        """Scroll through collection records"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _scroll():
                client = self._get_client()
                records, next_page_offset = client.scroll(
                    collection_name=collection_name,
                    scroll_filter=scroll_filter,
                    limit=max(1, min(limit, 1000)),
                    offset=offset,
                    with_vectors=with_vectors,
                    with_payload=with_payload
                )
                return records, next_page_offset
            
            records, next_offset = await loop.run_in_executor(self.executor, _scroll)
            logger.debug(f"Scroll returned {len(records)} records from {collection_name}")
            return records, next_offset
            
        except Exception as e:
            logger.error(f"Scroll failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Scroll operation failed: {str(e)}")
    
    async def delete(
        self,
        collection_name: str,
        points_selector: models.PointsSelector
    ) -> bool:
        """Delete points from collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _delete():
                client = self._get_client()
                result = client.delete(
                    collection_name=collection_name,
                    points_selector=points_selector,
                    wait=True
                )
                return result
            
            result = await loop.run_in_executor(self.executor, _delete)
            logger.info(f"Delete operation completed for collection {collection_name}")
            return bool(result and result.status == models.UpdateStatus.COMPLETED)
            
        except Exception as e:
            logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Delete operation failed: {str(e)}")
    
    async def count_points(
        self,
        collection_name: str,
        count_filter: Optional[Filter] = None
    ) -> int:
        """Count points in collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _count():
                client = self._get_client()
                result = client.count(
                    collection_name=collection_name,
                    count_filter=count_filter,
                    exact=True
                )
                return result.count
            
            count = await loop.run_in_executor(self.executor, _count)
            return count
            
        except Exception as e:
            logger.error(f"Count failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Count operation failed: {str(e)}")
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        try:
            await self.initialize()
            
            loop = asyncio.get_running_loop()
            
            def _health_check():
                client = self._get_client()
                start_time = time.time()
                
                try:
                    collections = client.get_collections()
                    
                    end_time = time.time()
                    
                    return {
                        "status": "healthy",
                        "url": self.url,
                        "response_time_ms": round((end_time - start_time) * 1000, 2),
                        "collections_count": len(collections.collections),
                        "timestamp": time.time()
                    }
                except Exception as e:
                    return {
                        "status": "unhealthy",
                        "url": self.url,
                        "error": str(e),
                        "timestamp": time.time()
                    }
            
            return await loop.run_in_executor(self.executor, _health_check)
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "url": self.url,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        
        try:
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True, timeout=30)
                logger.info("AsyncQdrant executor shutdown completed")
                
            if hasattr(self._local, 'client') and self._local.client is not None:
                try:
                    self._local.client.close()
                    logger.info("Main thread Qdrant client closed.")
                except Exception as e:
                    logger.warning(f"Error closing main thread client: {str(e)}")
        except Exception as e:
            logger.error(f"Error closing AsyncQdrant client: {str(e)}", exc_info=True)
        finally:
            # Reset the singleton instance to allow re-initialization if needed
            AsyncQdrantClient._instance = None 
            logger.info("AsyncQdrant client closed")

# Global instance management
_async_qdrant_client: Optional[AsyncQdrantClient] = None
_async_qdrant_lock = asyncio.Lock()

async def get_async_qdrant_client() -> AsyncQdrantClient:
    """Get or create an async Qdrant client instance"""
    global _async_qdrant_client
    
    async with _async_qdrant_lock:
        if _async_qdrant_client is None or _async_qdrant_client._closed:
            _async_qdrant_client = AsyncQdrantClient()
            await _async_qdrant_client.initialize()
        return _async_qdrant_client

@asynccontextmanager
async def qdrant_client_context():
    """Context manager for Qdrant client"""
    client = await get_async_qdrant_client()
    try:
        yield client
    finally:
        # In a global singleton scenario, we generally don't close the client here
        # as it's meant to be reused across requests.
        # Closing should be handled at application shutdown.
        pass

# Utility functions
def create_user_filter(user_id: str) -> Filter:
    """Create a filter for user documents"""
    return Filter(
        must=[
            FieldCondition(
                key="user_id",
                match=MatchValue(value=user_id)
            )
        ]
    )

def create_document_filter(user_id: str, doc_id: str) -> Filter:
    """Create a filter for specific document"""
    return Filter(
        must=[
            FieldCondition(
                key="user_id",
                match=MatchValue(value=user_id)
            ),
            FieldCondition(
                key="doc_id",
                match=MatchValue(value=doc_id)
            )
        ]
    )

def validate_vector(vector: List[float], expected_dim: int) -> bool:
    """Validate vector dimensions and values"""
    if not isinstance(vector, list):
        return False
    if len(vector) != expected_dim:
        return False
    if not all(isinstance(x, (int, float)) for x in vector):
        return False
    return True
===== ./app/models/agent_model.py =====
# ./app/models/agent_model.py
from datetime import datetime  # Add this import at the top
from enum import Enum
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any

class PersonalityTrait(str, Enum):
    FRIENDLY = "friendly"
    PROFESSIONAL = "professional"
    WITTY = "witty"
    EMPATHETIC = "empathetic"
    ENTHUSIASTIC = "enthusiastic"

class EmotionalAwarenessConfig(BaseModel):
    detect_emotion: bool = True
    adjust_tone: bool = True
    empathy_level: float = Field(0.8, ge=0, le=1)
    max_emotional_response_time: float = Field(1.5, gt=0)

class MemoryConfig(BaseModel):
    context_window: int = Field(10, gt=0)
    long_term_memory: bool = False
    memory_refresh_interval: int = Field(300, gt=0)  # seconds

class AgentPersonality(BaseModel):
    traits: List[PersonalityTrait] = [PersonalityTrait.FRIENDLY]
    base_tone: str = "helpful and knowledgeable"
    emotional_awareness: EmotionalAwarenessConfig = EmotionalAwarenessConfig()
    memory: MemoryConfig = MemoryConfig()

class AgentBase(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: str = Field(..., description="Ollama model to use")
    system_prompt: str = Field(..., description="System prompt defining behavior")
    is_public: bool = Field(False)
    tools: List[str] = Field([])
    personality: AgentPersonality = Field(default_factory=AgentPersonality)
    metadata: Dict[str, Any] = Field({}, alias="agent_metadata")

class AgentCreate(AgentBase):
    pass

class AgentUpdate(BaseModel):
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    is_public: Optional[bool] = None
    tools: Optional[List[str]] = None
    personality: Optional[AgentPersonality] = None
    metadata: Optional[Dict[str, Any]] = Field(None, alias="agent_metadata")

class AgentResponse(AgentBase):
    id: str
    owner_id: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True
        populate_by_name = True
===== ./app/models/voice_model.py =====
from pydantic import BaseModel
from typing import Optional

class VoiceResponse(BaseModel):
    text: str

class TextToSpeechRequest(BaseModel):
    text: str
    voice_model: Optional[str] = None
===== ./app/models/__init__.py =====
# Empty file to make models a package
===== ./app/models/db_models.py =====
from sqlalchemy import Column, String, Boolean, DateTime
from sqlalchemy.dialects.postgresql import JSONB, ARRAY
from app.database import Base
from datetime import datetime
import uuid

class DBAgent(Base):
    __tablename__ = "agents"

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    owner_id = Column(String, nullable=False, index=True)
    name = Column(String(100), nullable=False)
    description = Column(String(500))
    model = Column(String, nullable=False)
    system_prompt = Column(String, nullable=False)
    is_public = Column(Boolean, default=False, index=True)
    tools = Column(ARRAY(String))
    # Rename 'metadata' to something else like 'agent_metadata'
    agent_metadata = Column(JSONB, name="metadata")  # The 'name' parameter keeps the column name as 'metadata' in DB
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    __table_args__ = (
        {'schema': 'llm'},
    )
===== ./app/models/training_model.py =====
from enum import Enum
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, validator
import mimetypes

class TrainingJobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PROCESSING_FILES = "processing_files"
    EXTRACTING_CONTENT = "extracting_content"

class TrainingDataType(str, Enum):
    URL = "url"
    TEXT = "text"
    FILE = "file"
    IMAGE = "image"

class TrainingDataItem(BaseModel):
    type: TrainingDataType
    content: str  # URL, text content, or file path
    metadata: Optional[Dict[str, Any]] = {}
    
    @validator('content')
    def validate_content(cls, v, values):
        data_type = values.get('type')
        if data_type == TrainingDataType.URL:
            # Basic URL validation
            if not v.startswith(('http://', 'https://')):
                raise ValueError('URL must start with http:// or https://')
        elif data_type == TrainingDataType.TEXT:
            if len(v.strip()) < 10:
                raise ValueError('Text content must be at least 10 characters')
        return v

class TrainingJobCreate(BaseModel):
    agent_id: str
    # Backward compatibility - keep data_urls for existing API
    data_urls: Optional[List[str]] = None
    # New enhanced data structure
    training_data: Optional[List[TrainingDataItem]] = None
    # Direct text input
    text_data: Optional[List[str]] = None
    # Training configuration
    config: Optional[Dict[str, Any]] = {}
    
    @validator('training_data', 'data_urls', 'text_data')
    def validate_data_sources(cls, v, values):
        # Ensure at least one data source is provided
        data_urls = values.get('data_urls')
        text_data = values.get('text_data')
        
        if not any([v, data_urls, text_data]):
            raise ValueError('At least one data source must be provided')
        return v

class FileUploadInfo(BaseModel):
    filename: str
    content_type: str
    size: int
    file_id: str

class TrainingJobResponse(BaseModel):
    id: str
    user_id: str
    agent_id: str
    data_urls: List[str] = []
    training_data: List[TrainingDataItem] = []
    text_data: List[str] = []
    uploaded_files: List[FileUploadInfo] = []
    status: TrainingJobStatus
    created_at: datetime
    updated_at: datetime
    progress: int
    current_step: Optional[str] = None
    total_steps: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0

class TrainingProgress(BaseModel):
    job_id: str
    status: TrainingJobStatus
    progress: int
    current_step: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0
    message: Optional[str] = None

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def get_file_type(filename: str) -> TrainingDataType:
    """Determine the training data type based on file extension"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    ext = f'.{ext}'
    
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return TrainingDataType.IMAGE
    else:
        return TrainingDataType.FILE

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    return f'.{ext}' in SUPPORTED_EXTENSIONS
===== ./app/models/response_schema.py =====
from typing import List, Optional, Dict, Any
from pydantic import BaseModel

class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    options: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    message: str
    model: str
    context: List[Any]
    tokens_used: int

class RAGResponse(BaseModel):
    answer: str
    documents: List[str]
    context: List[Any]
    sources: List[str] = []

class DocumentResponse(BaseModel):
    document_id: str
    filename: str
    status: str = "success" 

class RAGQueryRequest(BaseModel):
    query: str
    max_results: Optional[int] = 5
===== ./app/scripts/fix_dimension_mismatch.py =====
# scripts/fix_dimension_mismatch.py
"""
Script to fix ChromaDB embedding dimension mismatch issues.
Run this script to reset your ChromaDB collection when you encounter dimension errors.
"""

import asyncio
import logging
import sys
import os
from pathlib import Path

# Add the parent directory to the path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.rag_service import RAGService
from app.services.llm_service import OllamaService
from app.config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def check_embedding_model():
    """Check what embedding dimension your current model produces"""
    try:
        ollama = OllamaService()
        test_text = "This is a test sentence to check embedding dimensions."
        
        logger.info("Testing embedding model...")
        embedding = await ollama.create_embedding(test_text)
        
        if embedding:
            logger.info(f"✅ Embedding model working")
            logger.info(f"✅ Current embedding dimension: {len(embedding)}")
            logger.info(f"✅ Model: {settings.EMBEDDING_MODEL}")
            return len(embedding)
        else:
            logger.error("❌ Failed to get embedding from model")
            return None
            
    except Exception as e:
        logger.error(f"❌ Error testing embedding model: {str(e)}")
        return None

async def reset_chroma_collection():
    """Reset the ChromaDB collection to fix dimension issues"""
    try:
        logger.info("Initializing RAG service...")
        rag_service = RAGService()
        
        logger.info("Resetting ChromaDB collection...")
        await rag_service.reset_collection()
        
        logger.info("✅ Collection reset successfully!")
        
        # Clean up
        await rag_service.close()
        
    except Exception as e:
        logger.error(f"❌ Error resetting collection: {str(e)}")
        raise

async def verify_setup():
    """Verify that everything is working correctly"""
    try:
        logger.info("Verifying setup...")
        
        # Check embedding model
        embedding_dim = await check_embedding_model()
        if not embedding_dim:
            return False
            
        # Test RAG service initialization
        rag_service = RAGService()
        await rag_service.initialize()
        
        logger.info("✅ Setup verification completed successfully!")
        
        # Clean up
        await rag_service.close()
        return True
        
    except Exception as e:
        logger.error(f"❌ Setup verification failed: {str(e)}")
        return False

async def main():
    """Main function to fix dimension mismatch issues"""
    print("🔧 ChromaDB Dimension Fix Tool")
    print("=" * 40)
    
    # Step 1: Check current embedding model
    print("\n📊 Step 1: Checking embedding model...")
    embedding_dim = await check_embedding_model()
    
    if not embedding_dim:
        print("❌ Cannot proceed without working embedding model")
        print("Please check your Ollama installation and model configuration")
        return
    
    # Step 2: Show current configuration
    print(f"\n⚙️  Current Configuration:")
    print(f"   - Embedding Model: {settings.EMBEDDING_MODEL}")
    print(f"   - Embedding Dimension: {embedding_dim}")
    print(f"   - ChromaDB Path: {settings.CHROMA_PATH}")
    print(f"   - Collection Name: {settings.CHROMA_COLLECTION_NAME}")
    
    # Step 3: Ask user if they want to reset
    print(f"\n🔄 The embedding dimension mismatch error occurs when:")
    print(f"   - Your collection expects one dimension (e.g., 384)")
    print(f"   - Your embedding model produces another (e.g., 1536)")
    print(f"   - Current model produces: {embedding_dim} dimensions")
    
    response = input(f"\n❓ Do you want to reset the ChromaDB collection? (y/N): ").lower()
    
    if response in ['y', 'yes']:
        print(f"\n🗑️  Step 3: Resetting ChromaDB collection...")
        await reset_chroma_collection()
        
        print(f"\n✅ Step 4: Verifying setup...")
        success = await verify_setup()
        
        if success:
            print(f"\n🎉 All done! Your ChromaDB is now ready to use.")
            print(f"   - Collection reset successfully")
            print(f"   - Embedding dimension: {embedding_dim}")
            print(f"   - You can now upload documents without dimension errors")
        else:
            print(f"\n❌ Verification failed. Please check the logs for errors.")
    else:
        print(f"\n📝 To fix this manually:")
        print(f"   1. Delete the ChromaDB directory: {settings.CHROMA_PATH}")
        print(f"   2. Or use a different embedding model with 384 dimensions")
        print(f"   3. Or modify your code to handle {embedding_dim} dimensions")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n\n👋 Exiting...")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        sys.exit(1)
===== ./app/main.py =====
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.routers import agents, auth, chat, rag, training, voice, agent_interaction  # Add agent_interaction here
from app.middleware.logging_middleware import LoggingMiddleware
from app.middleware.metrics_middleware import metrics_middleware
from app.utils.health_check import router as health_router
from app.utils.logging import logger
from app.services.cache import cache_service
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await cache_service.connect()
    yield
    # Shutdown
    await cache_service.disconnect()

app = FastAPI(lifespan=lifespan)
import uvicorn

def create_app() -> FastAPI:
    app = FastAPI(
        title=settings.APP_NAME,
        description="Multi-tenant AI agent platform with Ollama integration",
        version="1.0.0",
        docs_url="/docs" if settings.DEBUG else None,
        redoc_url="/redoc" if settings.DEBUG else None
    )
    
    # Middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    app.add_middleware(LoggingMiddleware)
    app.middleware("http")(metrics_middleware)
    
    # Routers
    app.include_router(health_router)
    app.include_router(auth.router, prefix=settings.API_PREFIX)
    app.include_router(agents.router, prefix=settings.API_PREFIX)
    app.include_router(chat.router, prefix=settings.API_PREFIX)
    app.include_router(rag.router, prefix=settings.API_PREFIX)
    app.include_router(training.router, prefix=f"{settings.API_PREFIX}/training")
    app.include_router(voice.router, prefix=f"{settings.API_PREFIX}/voice")
    app.include_router(agent_interaction.router, prefix=settings.API_PREFIX)
    
    @app.on_event("startup")
    async def startup():
        logger.info("Starting application...")
        await cache_service.connect()
        
    @app.on_event("shutdown")
    async def shutdown():
        logger.info("Shutting down application...")
        await cache_service.disconnect()
    
    return app

app = create_app()

if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG,
        log_config=None,
        access_log=False
    )
===== ./app/services/agent_interaction_service.py =====
# ./app/services/agent_interaction_service.py
import logging
from typing import Dict, Optional, List, AsyncIterator, Any  # Added Any here
from fastapi import UploadFile
from app.services.llm_service import OllamaService
from app.services.voice_service import VoiceService
from app.services.rag_service import RAGService
from app.models.agent_model import AgentPersonality
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio

logger = logging.getLogger(__name__)

class EmotionalAnalyzer:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        
    async def analyze_emotion(self, text: str) -> Dict[str, float]:
        """Analyze emotional tone from text"""
        prompt = """Analyze the emotional tone of this text. Respond ONLY with a JSON object containing 
        emotion scores between 0-1 for: happiness, sadness, anger, fear, surprise, neutral.

        Text: {text}""".format(text=text)
        
        try:
            response = await self.ollama.generate(
                prompt=prompt,
                model="llama3",
                options={"temperature": 0.2}
            )
            
            # Parse the JSON response
            import json
            return json.loads(response["response"])
        except Exception as e:
            logger.error(f"Emotion analysis failed: {str(e)}")
            return {
                "happiness": 0.5,
                "sadness": 0.0,
                "anger": 0.0,
                "fear": 0.0,
                "surprise": 0.0,
                "neutral": 0.5
            }

class AgentInteractionService:
    def __init__(self):
        self.ollama = OllamaService()
        self.voice = VoiceService()
        self.rag = RAGService()
        self.emotion_analyzer = EmotionalAnalyzer(self.ollama)
        self.conversation_memory = {}
        
    async def initialize(self):
        """Initialize all required services"""
        await self.rag.initialize()
        
    async def process_input(
        self,
        agent_id: str,
        user_id: str,
        input_text: Optional[str] = None,
        audio_file: Optional[UploadFile] = None,
        db: Optional[AsyncSession] = None
    ) -> Dict[str, Any]:
        """Process user input (text or voice) and generate response"""
        # Convert audio to text if provided
        if audio_file:
            input_text = await self.voice.speech_to_text(audio_file)
            
        if not input_text:
            raise ValueError("No input text provided")
            
        # Get or create conversation memory
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = {
                "history": [],
                "last_updated": datetime.now(),
                "emotional_state": {}
            }
            
        # Analyze emotion
        emotion_scores = await self.emotion_analyzer.analyze_emotion(input_text)
        self.conversation_memory[user_id]["emotional_state"] = emotion_scores
        
        # Retrieve relevant context
        rag_results = await self.rag.query(
            db=db,
            user_id=user_id,
            query=input_text
        )
        
        # Generate response considering emotion and context
        response = await self._generate_response(
            agent_id=agent_id,
            user_id=user_id,
            input_text=input_text,
            emotion_scores=emotion_scores,
            context=rag_results.get("documents", [])
        )
        
        # Update conversation history
        self.conversation_memory[user_id]["history"].append({
            "input": input_text,
            "response": response,
            "timestamp": datetime.now()
        })
        
        return {
            "text_response": response,
            "emotional_state": emotion_scores,
            "context_used": rag_results.get("sources", [])
        }
        
    async def _generate_response(
        self,
        agent_id: str,
        user_id: str,
        input_text: str,
        emotion_scores: Dict[str, float],
        context: List[str]
    ) -> str:
        """Generate response considering emotional state and context"""
        # Get dominant emotion
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        # Build system prompt based on emotion and context
        system_prompt = self._build_system_prompt(
            agent_id=agent_id,
            dominant_emotion=dominant_emotion,
            context=context
        )
        
        # Generate response
        response = await self.ollama.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": input_text}
            ]
        )
        
        return response.get("message", {}).get("content", "")
        
    def _build_system_prompt(
        self,
        agent_id: str,
        dominant_emotion: str,
        context: List[str]
    ) -> str:
        """Build dynamic system prompt based on context and emotion"""
        # TODO: Fetch agent personality from database
        personality = AgentPersonality()  # Default personality
        
        # Base prompt
        prompt_lines = [
            f"You are {agent_id}, a highly intelligent AI assistant.",
            f"Your personality traits: {', '.join([t.value for t in personality.traits])}.",
            f"Current user emotional state: {dominant_emotion}. Adjust your tone accordingly.",
            "",
            "Context from knowledge base:",
            "\n".join(context) if context else "No relevant context found",
            "",
            "Guidelines:",
            f"- Be {personality.base_tone}",
            "- Acknowledge user's emotional state if strong",
            "- Use context when relevant but don't force it",
            "- Keep responses concise but thorough when needed",
            "- Maintain natural conversation flow"
        ]
        
        # Emotion-specific adjustments
        if dominant_emotion == "sadness":
            prompt_lines.append("- Show extra empathy and support")
        elif dominant_emotion == "anger":
            prompt_lines.append("- Remain calm and solution-focused")
        elif dominant_emotion == "happiness":
            prompt_lines.append("- Match the positive energy but stay professional")
            
        return "\n".join(prompt_lines)
        
    async def text_to_speech(
        self,
        text: str,
        emotional_state: Optional[Dict[str, float]] = None
    ) -> bytes:
        """Convert text to speech with emotional inflection"""
        # TODO: Adjust TTS parameters based on emotional state
        return await self.voice.text_to_speech(text)
        
    async def get_conversation_history(self, user_id: str) -> List[Dict[str, Any]]:
        """Get conversation history for a user"""
        return self.conversation_memory.get(user_id, {}).get("history", [])
        
    async def clear_memory(self, user_id: str):
        """Clear conversation memory for a user"""
        if user_id in self.conversation_memory:
            del self.conversation_memory[user_id]
===== ./app/services/training_service.py =====
import os
import asyncio
import aiofiles
import uuid
import logging
import requests
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import tempfile
import shutil
from pathlib import Path

from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobStatus,
    TrainingDataItem,
    TrainingDataType,
    TrainingProgress,
    FileUploadInfo
)
from app.utils.file_processing import (
    process_document,
    process_image,
    extract_text_from_url,
    chunk_text
)

logger = logging.getLogger(__name__)

class TrainingService:
    def __init__(self):
        self.jobs = {}
        self.executor = ThreadPoolExecutor(max_workers=10)
        # Create temporary directory for file uploads
        self.temp_dir = Path(tempfile.gettempdir()) / "training_uploads"  
        self.temp_dir.mkdir(exist_ok=True)
        
        # File storage with user isolation
        self.user_files = {}  # user_id -> {file_id: file_path}

    async def create_job(
        self,
        user_id: str,
        agent_id: str,
        data_urls: List[str] = None,
        training_data: List[TrainingDataItem] = None,
        text_data: List[str] = None,
        uploaded_files: List[FileUploadInfo] = None,
        config: Dict[str, Any] = None
    ) -> TrainingJobResponse:
        """Create a new enhanced training job"""
        job_id = str(uuid.uuid4())
        now = datetime.utcnow()
        
        # Convert legacy data_urls to training_data format for backward compatibility
        enhanced_training_data = training_data or []
        if data_urls:
            for url in data_urls:
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.URL,
                        content=url,
                        metadata={"source": "legacy_data_urls"}
                    )
                )
        
        # Add text data as training items
        if text_data:
            for i, text in enumerate(text_data):
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.TEXT,
                        content=text,
                        metadata={"index": i, "source": "text_input"}
                    )
                )
        
        job = TrainingJobResponse(
            id=job_id,
            user_id=user_id,
            agent_id=agent_id,
            data_urls=data_urls or [],
            training_data=enhanced_training_data,
            text_data=text_data or [],
            uploaded_files=uploaded_files or [],
            status=TrainingJobStatus.PENDING,
            created_at=now,
            updated_at=now,
            progress=0,
            current_step="Initializing",
            total_steps=self._calculate_total_steps(enhanced_training_data),
            result=None,
            processed_items=0,
            total_items=len(enhanced_training_data)
        )
        
        self.jobs[job_id] = job
        logger.info(f"Created job {job_id} with {len(enhanced_training_data)} training items")
        return job

    def _calculate_total_steps(self, training_data: List[TrainingDataItem]) -> int:
        """Calculate total processing steps based on data types"""
        steps = 1  # Initialization
        
        for item in training_data:
            if item.type == TrainingDataType.URL:
                steps += 2  # Download + Process
            elif item.type == TrainingDataType.FILE:
                steps += 2  # Read + Process
            elif item.type == TrainingDataType.IMAGE:
                steps += 2  # Read + OCR/Analysis
            else:  # TEXT
                steps += 1  # Process
        
        steps += 2  # Finalization + Model training
        return steps

    async def save_uploaded_file(
        self,
        user_id: str,
        file_id: str,
        filename: str,
        content: bytes
    ) -> str:
        """Save uploaded file to temporary storage"""
        try:
            # Create user-specific directory
            user_dir = self.temp_dir / user_id
            user_dir.mkdir(exist_ok=True)
            
            # Save file with unique name
            file_path = user_dir / f"{file_id}_{filename}"
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # Track file for cleanup
            if user_id not in self.user_files:
                self.user_files[user_id] = {}
            self.user_files[user_id][file_id] = str(file_path)
            
            logger.info(f"Saved file {filename} for user {user_id} as {file_id}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"Error saving file: {str(e)}")
            raise

    async def list_jobs(self, user_id: str, agent_id: str) -> List[TrainingJobResponse]:
        """List training jobs for a user and agent"""
        user_jobs = []
        for job in self.jobs.values():
            if job.user_id == user_id and job.agent_id == agent_id:
                user_jobs.append(job)
        
        # Sort by creation date, newest first
        user_jobs.sort(key=lambda x: x.created_at, reverse=True)
        return user_jobs

    async def get_job(self, user_id: str, job_id: str) -> Optional[TrainingJobResponse]:
        """Get a specific job for a user"""
        job = self.jobs.get(job_id)
        if job and job.user_id == user_id:
            return job
        return None

    async def get_progress(self, user_id: str, job_id: str) -> Optional[TrainingProgress]:
        """Get training progress for a job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return None
        
        return TrainingProgress(
            job_id=job.id,
            status=job.status,
            progress=job.progress,
            current_step=job.current_step,
            processed_items=job.processed_items,
            total_items=job.total_items,
            message=job.error_message if job.status == TrainingJobStatus.FAILED else None
        )

    async def cancel_job(self, user_id: str, job_id: str) -> bool:
        """Cancel a training job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return False
        
        if job.status in [TrainingJobStatus.PENDING, TrainingJobStatus.PROCESSING_FILES, 
                         TrainingJobStatus.EXTRACTING_CONTENT, TrainingJobStatus.RUNNING]:
            job.status = TrainingJobStatus.FAILED
            job.error_message = "Job cancelled by user"
            job.updated_at = datetime.utcnow()
            logger.info(f"Cancelled job {job_id} for user {user_id}")
            return True
        
        return False

    async def run_training(self, job_id: str, user_id: str):
        """Run enhanced training process in background"""
        if job_id not in self.jobs:
            logger.error(f"Job {job_id} not found")
            return
        
        job = self.jobs[job_id]
        if job.user_id != user_id:
            logger.error(f"User {user_id} not authorized for job {job_id}")
            return
        
        try:
            await self._execute_training_pipeline(job)
        except Exception as e:
            logger.error(f"Training failed for job {job_id}: {str(e)}")
            job.status = TrainingJobStatus.FAILED
            job.error_message = str(e)
            job.updated_at = datetime.utcnow()
        finally:
            # Cleanup uploaded files
            await self._cleanup_user_files(user_id)

    async def _cleanup_user_files(self, user_id: str):
        """Clean up uploaded files for a user"""
        try:
            if user_id in self.user_files:
                for file_id, file_path in self.user_files[user_id].items():
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                            logger.info(f"Cleaned up file: {file_path}")
                    except Exception as e:
                        logger.error(f"Error cleaning up file {file_path}: {str(e)}")
                
                # Remove user directory if empty
                user_dir = self.temp_dir / user_id
                if user_dir.exists() and not any(user_dir.iterdir()):
                    user_dir.rmdir()
                
                # Clear from tracking
                del self.user_files[user_id]
                
        except Exception as e:
            logger.error(f"Error during cleanup for user {user_id}: {str(e)}")

    async def _execute_training_pipeline(self, job: TrainingJobResponse):
        """Execute the complete training pipeline"""
        logger.info(f"Starting training pipeline for job {job.id}")
        
        job.status = TrainingJobStatus.PROCESSING_FILES
        job.current_step = "Processing input data"
        job.updated_at = datetime.utcnow()
        
        processed_content = []
        current_step = 0
        
        # Process each training data item
        for i, item in enumerate(job.training_data):
            try:
                job.current_step = f"Processing item {i+1}/{len(job.training_data)}: {item.type.value}"
                job.processed_items = i
                job.progress = int((current_step / job.total_steps) * 100)
                job.updated_at = datetime.utcnow()
                
                content = await self._process_training_item(item)
                if content:
                    processed_content.extend(content)
                
                current_step += 2 if item.type != TrainingDataType.TEXT else 1
                
            except Exception as e:
                logger.error(f"Error processing item {i}: {str(e)}")
                # Continue with other items instead of failing completely
                continue
        
        # Update progress
        job.status = TrainingJobStatus.EXTRACTING_CONTENT
        job.current_step = "Extracting and chunking content"
        job.progress = 80
        job.updated_at = datetime.utcnow()
        
        # Chunk all processed content
        all_chunks = []
        for content in processed_content:
            chunks = chunk_text(content)
            all_chunks.extend(chunks)
        
        current_step += 1
        
        # Simulate model training
        job.status = TrainingJobStatus.RUNNING
        job.current_step = "Training model"
        job.progress = 90
        job.updated_at = datetime.utcnow()
        
        # Simulate training time based on content size
        training_time = min(len(all_chunks) * 0.1, 10)  # Max 10 seconds
        await asyncio.sleep(training_time)
        
        # Complete the job
        job.status = TrainingJobStatus.COMPLETED
        job.current_step = "Training completed"
        job.progress = 100
        job.processed_items = len(job.training_data)
        job.result = {
            "total_content_items": len(processed_content),
            "total_chunks": len(all_chunks),
            "average_chunk_size": sum(len(chunk) for chunk in all_chunks) / len(all_chunks) if all_chunks else 0,
            "training_time_seconds": training_time,
            "data_types_processed": list(set(item.type.value for item in job.training_data)),
            "accuracy": 0.95,  # Simulated
            "loss": 0.15       # Simulated
        }
        job.updated_at = datetime.utcnow()
        
        logger.info(f"Training completed for job {job.id} with {len(all_chunks)} chunks")

    async def _process_training_item(self, item: TrainingDataItem) -> List[str]:
        """Process a single training data item"""
        try:
            if item.type == TrainingDataType.URL:
                return await self._process_url(item.content)
            elif item.type == TrainingDataType.FILE:
                return await self._process_file(item.content, item.metadata)
            elif item.type == TrainingDataType.IMAGE:
                return await self._process_image_file(item.content, item.metadata)
            elif item.type == TrainingDataType.TEXT:
                return [item.content]
            else:
                logger.warning(f"Unknown data type: {item.type}")
                return []
                
        except Exception as e:
            logger.error(f"Error processing {item.type} item: {str(e)}")
            return []

    async def _process_url(self, url: str) -> List[str]:
        """Process content from URL"""
        try:
            # Run in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor,
                extract_text_from_url,
                url
            )
            return [content] if content else []
        except Exception as e:
            logger.error(f"Error processing URL {url}: {str(e)}")
            return []

    async def _process_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process uploaded file"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read file content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process document
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_document,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {str(e)}")
            return []

    async def _process_image_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process image file with OCR"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"Image file not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read image content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process image
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_image,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing image {file_path}: {str(e)}")
            return []
===== ./app/services/rag_service.py =====
# service/rag_service.py
import logging
import os
import asyncio
import uuid
import numpy as np
from typing import List, Dict, Any, Optional
from app.config import settings
from app.utils.file_processing import process_document
from app.services.llm_service import OllamaService
from sqlalchemy.ext.asyncio import AsyncSession
from app.utils.qdrant_async import get_async_qdrant_client
from qdrant_client.http import models as qdrant_models
from qdrant_client.http.models import Distance, VectorParams

logger = logging.getLogger(__name__)

def chunk_text_with_overlap(
    text: str, 
    chunk_size: int = 1000, 
    chunk_overlap: int = 200,
    separator: str = "\n\n"
) -> List[str]:
    """
    Split text into chunks with overlap to maintain context.
    
    Args:
        text: The input text to chunk
        chunk_size: Maximum size of each chunk in characters
        chunk_overlap: Number of characters to overlap between chunks
        separator: Primary separator to use for splitting (paragraphs by default)
    
    Returns:
        List of text chunks with overlap
    """
    if not text or not text.strip():
        return []
    
    # If text is smaller than chunk_size, return as single chunk
    if len(text) <= chunk_size:
        return [text.strip()]
    
    chunks = []
    
    # First, try to split by separator (paragraphs)
    paragraphs = text.split(separator)
    
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
            
        # If adding this paragraph would exceed chunk_size
        if len(current_chunk) + len(paragraph) + len(separator) > chunk_size:
            if current_chunk:
                chunks.append(current_chunk.strip())
                
                # Start new chunk with overlap from previous chunk
                if chunk_overlap > 0 and len(current_chunk) > chunk_overlap:
                    # Take last chunk_overlap characters from current chunk
                    overlap_text = current_chunk[-chunk_overlap:].strip()
                    # Find a good breaking point (sentence or word boundary)
                    sentences = overlap_text.split('. ')
                    if len(sentences) > 1:
                        overlap_text = '. '.join(sentences[1:])
                    current_chunk = overlap_text + separator + paragraph
                else:
                    current_chunk = paragraph
            else:
                # Single paragraph is larger than chunk_size, need to split it
                if len(paragraph) > chunk_size:
                    # Split large paragraph by sentences
                    sentences = paragraph.split('. ')
                    temp_chunk = ""
                    
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if not sentence:
                            continue
                            
                        if not sentence.endswith('.') and sentence != sentences[-1]:
                            sentence += '.'
                            
                        if len(temp_chunk) + len(sentence) + 1 > chunk_size:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                                
                                # Add overlap
                                if chunk_overlap > 0 and len(temp_chunk) > chunk_overlap:
                                    overlap_text = temp_chunk[-chunk_overlap:].strip()
                                    temp_chunk = overlap_text + " " + sentence
                                else:
                                    temp_chunk = sentence
                            else:
                                # Single sentence is too large, split by words
                                words = sentence.split()
                                word_chunk = ""
                                
                                for word in words:
                                    if len(word_chunk) + len(word) + 1 > chunk_size:
                                        if word_chunk:
                                            chunks.append(word_chunk.strip())
                                            
                                            # Add overlap
                                            if chunk_overlap > 0:
                                                overlap_words = word_chunk.split()[-chunk_overlap//10:]  # Rough estimate
                                                word_chunk = " ".join(overlap_words) + " " + word
                                            else:
                                                word_chunk = word
                                        else:
                                            # Single word is too large, just add it
                                            chunks.append(word)
                                            word_chunk = ""
                                    else:
                                        word_chunk = word_chunk + " " + word if word_chunk else word
                                
                                if word_chunk:
                                    temp_chunk = word_chunk
                                else:
                                    temp_chunk = ""
                        else:
                            temp_chunk = temp_chunk + " " + sentence if temp_chunk else sentence
                    
                    if temp_chunk:
                        current_chunk = temp_chunk
                else:
                    current_chunk = paragraph
        else:
            current_chunk = current_chunk + separator + paragraph if current_chunk else paragraph
    
    # Add the last chunk
    if current_chunk and current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # Clean up chunks - remove empty ones and duplicates
    cleaned_chunks = []
    seen_chunks = set()
    
    for chunk in chunks:
        chunk = chunk.strip()
        if chunk and chunk not in seen_chunks and len(chunk) > 50:  # Minimum chunk size
            cleaned_chunks.append(chunk)
            seen_chunks.add(chunk)
    
    logger.info(f"Split text into {len(cleaned_chunks)} chunks with overlap")
    return cleaned_chunks

class OllamaEmbeddingFunction:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self._embedding_dim = None
    
    async def _get_embedding_dimension(self) -> int:
        """Get the embedding dimension from the model"""
        if self._embedding_dim is None:
            try:
                # Test with a small text to get the dimension
                test_embedding = await self.ollama.create_embedding("test")
                self._embedding_dim = len(test_embedding) if test_embedding else 384
                logger.info(f"Detected embedding dimension: {self._embedding_dim}")
            except Exception as e:
                logger.error(f"Failed to detect embedding dimension: {str(e)}")
                self._embedding_dim = 384  # Default fallback
        return self._embedding_dim
    
    async def generate_embeddings(self, input: List[str]) -> List[List[float]]:
        embeddings = []
        embedding_dim = await self._get_embedding_dimension()
        
        for text in input:
            try:
                logger.debug(f"Generating embedding for text: {text[:100]}...")
                embedding = await self.ollama.create_embedding(text)
                
                if not embedding:
                    logger.warning(f"Empty embedding for text: {text[:50]}...")
                    embeddings.append([0.0] * embedding_dim)
                    continue
                    
                # Convert to list if it's a numpy array
                if hasattr(embedding, 'tolist'):
                    embedding = embedding.tolist()
                    
                # Ensure correct dimension
                if len(embedding) != embedding_dim:
                    logger.warning(f"Embedding dimension mismatch: expected {embedding_dim}, got {len(embedding)}")
                    if len(embedding) > embedding_dim:
                        embedding = embedding[:embedding_dim]
                    else:
                        embedding.extend([0.0] * (embedding_dim - len(embedding)))
                
                # Debug the raw embedding values
                logger.debug(f"Raw embedding values (first 5): {embedding[:5]}")
                
                # Normalize
                embedding_array = np.array(embedding)
                norm = np.linalg.norm(embedding_array)
                if norm > 0:
                    embedding = (embedding_array / norm).tolist()
                    logger.debug(f"Normalized embedding (first 5): {embedding[:5]}")
                else:
                    logger.warning("Zero norm embedding detected")
                    
                embeddings.append(embedding)
                
            except Exception as e:
                logger.error(f"Embedding error: {str(e)}", exc_info=True)
                embeddings.append([0.0] * embedding_dim)
                
        return embeddings

class RAGService:
    def __init__(self):
        self.ollama = OllamaService()
        self.embedding_fn = OllamaEmbeddingFunction(self.ollama)
        self.collection_name = "documents"
        self.client = None
        self._embedding_dim = None

    async def initialize(self):
        """Initialize the Qdrant client and collection"""
        if self.client is None:
            self.client = await get_async_qdrant_client()
            self._embedding_dim = await self.embedding_fn._get_embedding_dimension()

        # Check if collection exists and recreate if needed
        try:
            collection_exists = await self.client.collection_exists(self.collection_name)
            
            if collection_exists:
                # Get collection info to check dimensions
                collection_info = await self.client.get_collection_info(self.collection_name)
                if collection_info and collection_info.vector_size != self._embedding_dim:
                    logger.warning(f"Collection dimension mismatch: expected {self._embedding_dim}, got {collection_info.vector_size}. Recreating...")
                    collection_exists = False  # Force recreation
                else:
                    logger.info(f"Collection {self.collection_name} already exists with correct dimensions")
                    return
        except Exception as e:
            logger.info(f"Collection check failed (may not exist): {str(e)}")
            collection_exists = False

        # Create collection with proper parameters
        try:
            await self.client.create_collection(
                collection_name=self.collection_name,
                vector_size=self._embedding_dim,
                distance="Cosine",
                recreate_if_exists=True
            )
            logger.info(f"Created collection {self.collection_name} with dimension {self._embedding_dim}")
        except Exception as e:
            logger.error(f"Failed to create collection: {str(e)}")
            raise

    async def _ensure_collection_exists(self):
        """Ensure collection exists with proper configuration"""
        try:
            # Check if client is initialized
            if self.client is None:
                await self.initialize()
                return
                
            # Verify collection exists
            collection_exists = await self.client.collection_exists(self.collection_name)
            if not collection_exists:
                await self.initialize()
                
        except Exception as e:
            logger.info(f"Collection verification failed, initializing: {str(e)}")
            await self.initialize()

    async def ingest_document(
        self,
        db: AsyncSession,
        user_id: str,
        filename: str,
        content: bytes,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> str:
        """Ingest document into Qdrant"""
        await self._ensure_collection_exists()
        
        try:
            logger.info(f"Starting document ingestion for {filename}")
            text = process_document(filename, content)
            if not text:
                raise ValueError("No text extracted")
            
            logger.info(f"Extracted text length: {len(text)}")
            
            chunks = chunk_text_with_overlap(
                text, 
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
            
            if not chunks:
                raise ValueError("No chunks created")
            
            logger.info(f"Created {len(chunks)} chunks")
            
            doc_id = f"{user_id}_{os.path.splitext(filename)[0]}_{uuid.uuid4().hex[:8]}"
            
            # Generate embeddings asynchronously
            logger.info("Generating embeddings...")
            embeddings = await self.embedding_fn.generate_embeddings(chunks)
            logger.info(f"Generated {len(embeddings)} embeddings")
            
            # Prepare points for upsert
            points = [
                qdrant_models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        "user_id": user_id,
                        "filename": filename,
                        "chunk_index": i,
                        "doc_id": doc_id,
                        "text": chunk,
                        "chunk_size": len(chunk),
                        "created_at": asyncio.get_event_loop().time()
                    }
                )
                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))
            ]
            
            # Upsert points
            success = await self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            if not success:
                raise Exception("Failed to upsert points to Qdrant")
            
            logger.info(f"Successfully ingested document {filename} with {len(chunks)} chunks")
            return doc_id
            
        except Exception as e:
            logger.error(f"Ingest failed: {str(e)}", exc_info=True)
            raise

    def _calculate_similarity_from_score(self, score: float) -> float:
        """Convert Qdrant cosine similarity score to similarity percentage"""
        # Qdrant cosine similarity returns values between -1 and 1
        # Convert to 0-1 range for easier interpretation
        return (score + 1) / 2
            
    async def query(
        self,
        db: AsyncSession,
        user_id: str,
        query: str,
        max_results: int = 5,
        min_score: float = 0.3  # Qdrant's cosine similarity threshold
    ) -> Dict[str, Any]:
        """Query documents using RAG with Qdrant"""
        await self._ensure_collection_exists()
        
        try:
            if not query or len(query.strip()) < 3:
                raise ValueError("Query must be at least 3 characters")

            logger.info(f"Processing query: {query}")
            
            # Generate query embedding
            logger.info("Generating query embedding...")
            query_embeddings = await self.embedding_fn.generate_embeddings([query])
            
            if not query_embeddings or len(query_embeddings[0]) == 0:
                raise ValueError("Failed to generate query embedding")
            
            logger.info(f"Query embedding dimension: {len(query_embeddings[0])}")

            # Import the filter creation function
            from app.utils.qdrant_async import create_user_filter
            
            # Search Qdrant
            search_results = await self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embeddings[0],
                query_filter=create_user_filter(user_id),
                limit=max_results * 2,  # Get more results to filter
                score_threshold=min_score
            )
            
            if not search_results:
                return {
                    "answer": "No relevant documents found for your query",
                    "documents": [],
                    "context": [],
                    "sources": []
                }
            
            # Process results
            filtered_docs = []
            filtered_metas = []
            similarity_scores = []
            
            for result in search_results:
                similarity = self._calculate_similarity_from_score(result.score)
                similarity_scores.append(similarity)
                
                if similarity >= min_score:
                    filtered_docs.append(result.payload.get("text", ""))
                    filtered_metas.append(result.payload)
            
            logger.info(f"Filtered {len(filtered_docs)} documents from {len(search_results)}")
            
            if not filtered_docs:
                max_similarity = max(similarity_scores) if similarity_scores else 0.0
                return {
                    "answer": f"No relevant documents found above similarity threshold {min_score}. Maximum similarity found: {max_similarity:.4f}",
                    "documents": [],
                    "context": [],
                    "sources": []
                }
            
            # Build context
            context = "\n\n---\n\n".join([
                f"Source: {res.get('filename', 'Unknown')} (Chunk {res.get('chunk_index', 0) + 1})\n{doc}"
                for doc, res in zip(filtered_docs, filtered_metas)
            ])
            
            system_prompt = f"""You are a helpful AI assistant. Use the following context to answer the user's question.
If you don't know the answer based on the provided context, say so honestly. Don't make up information.
Be comprehensive but concise in your response.

Context:
{context}

Instructions:
- Answer based only on the provided context
- If the context doesn't contain enough information, say so
- Cite relevant sources when appropriate
- Be helpful and informative"""

            # Get answer from LLM
            logger.info("Generating LLM response...")
            llm_response = await self.ollama.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ]
            )
            
            return {
                "answer": llm_response.get("message", {}).get("content", "No response generated"),
                "documents": filtered_docs,
                "context": llm_response.get("context", []),
                "sources": list(set(
                    meta.get("filename", "Unknown") 
                    for meta in filtered_metas 
                    if meta and "filename" in meta
                )),
                "debug_info": {
                    "similarity_scores": similarity_scores[:5],
                    "threshold_used": min_score,
                    "total_chunks_found": len(search_results),
                    "chunks_after_filtering": len(filtered_docs),
                    "query_embedding_dim": len(query_embeddings[0])
                }
            }
        except Exception as e:
            logger.error(f"RAG query failed: {str(e)}", exc_info=True)
            raise

    async def list_documents(
        self,
        db: AsyncSession,
        user_id: str,
        page: int = 1,
        per_page: int = 10
    ) -> List[Dict[str, Any]]:
        """List documents with pagination"""
        await self._ensure_collection_exists()
        
        try:
            if page < 1 or per_page < 1:
                raise ValueError("Page and per_page must be positive integers")

            # Import the filter creation function
            from app.utils.qdrant_async import create_user_filter
            
            # Scroll through user's documents
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id),
                with_payload=True,
                limit=1000  # Large number to get all docs
            )
            
            if not records:
                return []

            # Get unique documents with stats
            unique_docs = {}
            for record in records:
                payload = record.payload
                if payload and "doc_id" in payload:
                    doc_id = payload["doc_id"]
                    if doc_id not in unique_docs:
                        unique_docs[doc_id] = {
                            "document_id": doc_id,
                            "filename": payload.get("filename", "unknown"),
                            "chunk_count": 0,
                            "total_text_length": 0,
                            "created_at": payload.get("created_at", 0)
                        }
                    
                    unique_docs[doc_id]["chunk_count"] += 1
                    unique_docs[doc_id]["total_text_length"] += len(payload.get("text", ""))

            # Convert to list and sort by creation time
            doc_list = list(unique_docs.values())
            doc_list.sort(key=lambda x: x.get("created_at", 0), reverse=True)
            
            # Apply pagination
            start = (page - 1) * per_page
            end = start + per_page
            return doc_list[start:end]
        except Exception as e:
            logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
            raise

    async def delete_document(
        self,
        db: AsyncSession,
        user_id: str,
        document_id: str
    ) -> bool:
        """Delete a document and all its chunks"""
        await self._ensure_collection_exists()
        
        try:
            # Import the filter creation function
            from app.utils.qdrant_async import create_document_filter
            
            # Delete points matching the document_id and user_id
            success = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=qdrant_models.FilterSelector(
                    filter=create_document_filter(user_id, document_id)
                )
            )
            
            logger.info(f"Successfully deleted document {document_id} for user {user_id}")
            return success
            
        except Exception as e:
            logger.error(f"Failed to delete document {document_id}: {str(e)}", exc_info=True)
            raise

    async def get_document_stats(self, db: AsyncSession, user_id: str) -> Dict[str, Any]:
        """Get statistics about user's documents"""
        await self._ensure_collection_exists()
        
        try:
            # Import the filter creation function
            from app.utils.qdrant_async import create_user_filter
            
            # Count total points for user
            total_chunks = await self.client.count_points(
                collection_name=self.collection_name,
                count_filter=create_user_filter(user_id)
            )
            
            # Get unique documents
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id),
                with_payload=True,
                limit=1000
            )
            
            unique_docs = set()
            total_text_length = 0
            
            for record in records:
                payload = record.payload
                if payload:
                    unique_docs.add(payload.get("doc_id", ""))
                    total_text_length += len(payload.get("text", ""))
            
            return {
                "total_documents": len(unique_docs),
                "total_chunks": total_chunks,
                "total_text_length": total_text_length,
                "average_chunk_size": total_text_length // max(total_chunks, 1),
                "embedding_dimension": self._embedding_dim
            }
            
        except Exception as e:
            logger.error(f"Failed to get document stats: {str(e)}", exc_info=True)
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "total_text_length": 0,
                "average_chunk_size": 0,
                "embedding_dimension": self._embedding_dim or 384
            }

    async def reset_collection(self):
        """Reset the collection - useful for debugging"""
        await self.initialize()
        try:
            # Import the filter creation function
            from app.utils.qdrant_async import create_user_filter
            
            # Delete all points in the collection by creating an empty filter
            success = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=qdrant_models.FilterSelector(
                    filter=qdrant_models.Filter(must=[])
                )
            )
            logger.info(f"Collection {self.collection_name} reset successfully")
        except Exception as e:
            logger.error(f"Failed to reset collection: {str(e)}")
            raise

    async def close(self):
        """Clean up resources"""
        if self.client:
            await self.client.close()
            logger.info("RAG service closed successfully")
===== ./app/services/agent_service.py =====
# Updated agent_service.py
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.db_models import DBAgent
from app.models.agent_model import AgentCreate, AgentUpdate
from datetime import datetime
from sqlalchemy import select, update, delete

class AgentService:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def create_agent(self, owner_id: str, agent_data: AgentCreate) -> DBAgent:
        db_agent = DBAgent(
            owner_id=owner_id,
            name=agent_data.name,
            description=agent_data.description,
            model=agent_data.model,
            system_prompt=agent_data.system_prompt,
            is_public=agent_data.is_public,
            tools=agent_data.tools,
            agent_metadata=agent_data.metadata
        )
        self.db.add(db_agent)
        await self.db.commit()
        await self.db.refresh(db_agent)
        return db_agent

    async def get_agent(self, agent_id: str, owner_id: str) -> DBAgent:
        result = await self.db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        return result.scalars().first()

    async def list_agents(self, owner_id: str) -> list[DBAgent]:
        result = await self.db.execute(
            select(DBAgent).where(DBAgent.owner_id == owner_id)
        )
        return result.scalars().all()

    async def update_agent(self, agent_id: str, owner_id: str, update_data: AgentUpdate) -> DBAgent:
        update_dict = update_data.dict(exclude_unset=True)
        if not update_dict:
            return await self.get_agent(agent_id, owner_id)
            
        await self.db.execute(
            update(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
            .values(**update_dict)
        )
        await self.db.commit()
        return await self.get_agent(agent_id, owner_id)

    async def delete_agent(self, agent_id: str, owner_id: str) -> bool:
        result = await self.db.execute(
            delete(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        await self.db.commit()
        return result.rowcount > 0
===== ./app/services/cache.py =====
from redis.asyncio import Redis
from app.config import settings
import logging
from typing import Any, Optional
import json

logger = logging.getLogger(__name__)

class CacheService:
    def __init__(self):
        self.redis = None
        self.enabled = settings.CACHE_ENABLED
        self._connection_attempted = False
        
    async def connect(self):
        if self.enabled and not self._connection_attempted:
            self._connection_attempted = True
            try:
                self.redis = Redis.from_url(
                    str(settings.REDIS_URL),
                    encoding="utf-8",
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
                await self.redis.ping()
                logger.info("Connected to Redis successfully")
                self.enabled = True
            except Exception as e:
                logger.warning(f"Redis connection failed, caching disabled: {str(e)}")
                self.enabled = False
                self.redis = None
    
    async def disconnect(self):
        if self.redis:
            try:
                await self.redis.close()
                logger.info("Redis connection closed")
            except Exception as e:
                logger.error(f"Error closing Redis connection: {str(e)}")
    
    async def get(self, key: str) -> Optional[Any]:
        if not self.enabled or not self.redis:
            return None
            
        try:
            data = await self.redis.get(key)
            if data is None:
                return None
                
            try:
                return json.loads(data)
            except json.JSONDecodeError:
                return data
        except Exception as e:
            logger.warning(f"Cache get error for key '{key}': {str(e)}")
            return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            ttl = ttl or getattr(settings, 'CACHE_TTL', 300)  # Default 5 minutes
            
            if isinstance(value, (str, bytes)):
                serialized_value = value
            else:
                try:
                    serialized_value = json.dumps(value, default=str)
                except (TypeError, ValueError) as e:
                    logger.warning(f"Failed to serialize value for key '{key}': {str(e)}")
                    return False
            
            await self.redis.set(key, serialized_value, ex=ttl)
            return True
        except Exception as e:
            logger.warning(f"Cache set error for key '{key}': {str(e)}")
            return False
    
    async def delete(self, key: str) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.delete(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache delete error for key '{key}': {str(e)}")
            return False
    
    async def exists(self, key: str) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.exists(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache exists error for key '{key}': {str(e)}")
            return False
    
    async def clear_pattern(self, pattern: str) -> int:
        """Clear all keys matching a pattern"""
        if not self.enabled or not self.redis:
            return 0
            
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                return await self.redis.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"Cache clear pattern error for pattern '{pattern}': {str(e)}")
            return 0
    
    async def health_check(self) -> bool:
        """Check if Redis is healthy"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            await self.redis.ping()
            return True
        except Exception as e:
            logger.warning(f"Redis health check failed: {str(e)}")
            return False

# Global instance
cache_service = CacheService()

===== ./app/services/__init__.py =====
# Empty file to make services a package
===== ./app/services/llm_service.py =====
# app/services/llm_service.py
import requests
import logging
from typing import Optional, Dict, Any, List
from app.config import settings
import asyncio
import aiohttp
import hashlib
import numpy as np

logger = logging.getLogger(__name__)

class OllamaService:
    def __init__(self):
        self.base_url = str(settings.OLLAMA_URL).rstrip('/')
        self.default_model = settings.DEFAULT_OLLAMA_MODEL
        self.timeout = settings.OLLAMA_TIMEOUT
        # Use the main model for embeddings if no specific embedding model
        self.embedding_model = getattr(settings, 'EMBEDDING_MODEL', 'deepseek-r1:1.5b')

    async def _make_request_async(self, endpoint: str, payload: dict) -> dict:
        """Make async HTTP request to Ollama"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making request to: {url}")
            logger.debug(f"Payload: {payload}")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                ) as response:
                    response_text = await response.text()
                    logger.debug(f"Response status: {response.status}")
                    logger.debug(f"Response: {response_text[:500]}...")
                    
                    if response.status == 405:
                        logger.error(f"Method not allowed for {url}. Available methods might be different.")
                        raise requests.exceptions.HTTPError(f"405 Client Error: Method Not Allowed for url: {url}")
                    
                    response.raise_for_status()
                    return await response.json()
                    
        except aiohttp.ClientError as e:
            logger.error(f"HTTP Client Error calling Ollama: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Error calling Ollama: {str(e)}")
            raise

    async def _make_request(self, endpoint: str, payload: dict) -> dict:
        """Fallback sync request method"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making sync request to: {url}")
            
            response = requests.post(
                url,
                json=payload,
                timeout=self.timeout
            )
            
            logger.debug(f"Response status: {response.status_code}")
            logger.debug(f"Response: {response.text[:500]}...")
            
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            logger.error(f"Error calling Ollama: {str(e)}")
            raise

    async def check_ollama_status(self) -> Dict[str, Any]:
        """Check if Ollama is running and accessible"""
        try:
            # Try to get the list of models first
            models = await self.list_models()
            
            # Check if our models are available
            model_names = [model.get('name', '') for model in models]
            
            return {
                "status": "healthy",
                "available_models": model_names,
                "default_model_available": self.default_model in model_names,
                "embedding_model_available": self.embedding_model in model_names,
                "base_url": self.base_url
            }
            
        except Exception as e:
            logger.error(f"Ollama health check failed: {str(e)}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "base_url": self.base_url
            }

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None,
        system: Optional[str] = None,
        template: Optional[str] = None,
        context: Optional[list] = None
    ) -> Dict[str, Any]:
        payload = {
            "model": model or self.default_model,
            "prompt": prompt,
            "stream": False,
            "options": options or {},
        }
        if system: 
            payload["system"] = system
        if template: 
            payload["template"] = template
        if context: 
            payload["context"] = context
        
        try:
            return await self._make_request_async("/api/generate", payload)
        except Exception as e:
            logger.warning(f"Async request failed, trying sync: {str(e)}")
            return await self._make_request("/api/generate", payload)

    async def chat(
        self,
        messages: list,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        payload = {
            "model": model or self.default_model,
            "messages": messages,
            "stream": False,
            "options": options or {},
        }
        
        try:
            return await self._make_request_async("/api/chat", payload)
        except Exception as e:
            logger.warning(f"Async request failed, trying sync: {str(e)}")
            return await self._make_request("/api/chat", payload)

    async def list_models(self) -> list:
        """List available models"""
        try:
            # Try async first
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"{self.base_url}/api/tags",
                        timeout=aiohttp.ClientTimeout(total=self.timeout)
                    ) as response:
                        response.raise_for_status()
                        result = await response.json()
                        return result.get("models", [])
            except:
                # Fallback to sync
                response = requests.get(
                    f"{self.base_url}/api/tags",
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json().get("models", [])
                
        except Exception as e:
            logger.error(f"Error listing models: {str(e)}")
            return []

    async def create_embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> List[float]:
        """Create embeddings with multiple fallback strategies"""
        target_model = model or self.embedding_model
        
        # Strategy 1: Try the embeddings endpoint
        try:
            return await self._try_embeddings_endpoint(text, target_model)
        except Exception as e:
            logger.warning(f"Embeddings endpoint failed: {str(e)}")
        
        # Strategy 2: Try using the main model for embeddings if it's different
        if target_model != self.default_model:
            try:
                logger.info(f"Trying main model {self.default_model} for embeddings")
                return await self._try_embeddings_endpoint(text, self.default_model)
            except Exception as e:
                logger.warning(f"Main model embedding failed: {str(e)}")
        
        # Strategy 3: Try generate endpoint with special prompt
        try:
            return await self._create_embedding_via_generate(text, target_model)
        except Exception as e:
            logger.warning(f"Generate-based embedding failed: {str(e)}")
        
        # Strategy 4: Fallback to deterministic hash-based embedding
        logger.warning("All embedding methods failed, using deterministic fallback")
        return self._create_deterministic_embedding(text)

    async def _try_embeddings_endpoint(self, text: str, model: str) -> List[float]:
        """Try the official embeddings endpoint"""
        payload = {
            "model": model,
            "prompt": text  # Some versions use 'prompt' instead of 'input'
        }
        
        try:
            # Try with 'prompt' first
            result = await self._make_request_async("/api/embeddings", payload)
        except:
            # Try with 'input' 
            payload["input"] = payload.pop("prompt")
            try:
                result = await self._make_request_async("/api/embeddings", payload)
            except:
                # Try the /api/embed endpoint
                result = await self._make_request_async("/api/embed", payload)
        
        # Handle different response formats
        if 'embeddings' in result:
            embeddings = result['embeddings']
            if embeddings and len(embeddings) > 0:
                return embeddings[0] if isinstance(embeddings[0], list) else embeddings
        elif 'embedding' in result:
            return result['embedding']
        elif 'data' in result:
            # OpenAI-style response
            return result['data'][0]['embedding']
        
        raise ValueError("No embeddings found in response")

    async def _create_embedding_via_generate(self, text: str, model: str) -> List[float]:
        """Create embedding using generate endpoint with special prompt"""
        
        # Try a simple approach that might work with some models
        prompt = f"Embed: {text}"
        
        response = await self.generate(
            prompt=prompt,
            model=model,
            options={
                "temperature": 0.0,
                "num_predict": 1,  # Minimal generation
                "stop": ["\n", ".", "!"]
            }
        )
        
        # This is a fallback - extract features from the response
        response_text = response.get("response", "")
        
        # Create a more sophisticated embedding from the response
        return self._text_to_embedding(text + " " + response_text)

    def _create_deterministic_embedding(self, text: str) -> List[float]:
        """Create a deterministic embedding from text using multiple hash functions"""
        
        # Use multiple hash functions for better distribution
        hash_functions = [
            lambda x: hashlib.md5(x.encode()).hexdigest(),
            lambda x: hashlib.sha1(x.encode()).hexdigest(),
            lambda x: hashlib.sha256(x.encode()).hexdigest(),
        ]
        
        # Target dimension
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        embedding = []
        
        # Generate embedding using multiple hash functions
        for i, hash_func in enumerate(hash_functions):
            hash_hex = hash_func(f"{text}_{i}")
            
            # Convert hex to floats
            for j in range(0, len(hash_hex), 2):
                if len(embedding) >= target_dim:
                    break
                val = int(hash_hex[j:j+2], 16) / 255.0  # Normalize to 0-1
                embedding.append(val)
            
            if len(embedding) >= target_dim:
                break
        
        # Pad or truncate to desired dimension
        if len(embedding) < target_dim:
            embedding.extend([0.1] * (target_dim - len(embedding)))
        else:
            embedding = embedding[:target_dim]
        
        # Add some text-based features
        text_features = [
            len(text) / 1000.0,  # Text length feature
            text.count(' ') / len(text) if text else 0,  # Word density
            text.count('.') / len(text) if text else 0,  # Sentence density
        ]
        
        # Replace some values with text features
        for i, feature in enumerate(text_features):
            if i < len(embedding):
                embedding[i] = feature
        
        # Normalize the embedding
        embedding_array = np.array(embedding)
        norm = np.linalg.norm(embedding_array)
        if norm > 0:
            embedding = (embedding_array / norm).tolist()
        
        logger.info(f"Created deterministic embedding with dimension {len(embedding)}")
        return embedding

    def _text_to_embedding(self, text: str) -> List[float]:
        """Convert text to embedding using various text features"""
        
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        
        # Extract various text features
        features = []
        
        # Character-level features
        for i in range(min(len(text), 50)):
            features.append(ord(text[i]) / 255.0)
        
        # Word-level features
        words = text.lower().split()
        for word in words[:20]:  # First 20 words
            word_hash = hash(word) % 1000
            features.append(word_hash / 1000.0)
        
        # Statistical features
        features.extend([
            len(text) / 1000.0,
            len(words) / 100.0 if words else 0,
            text.count(' ') / len(text) if text else 0,
            text.count('.') / len(text) if text else 0,
            text.count(',') / len(text) if text else 0,
        ])
        
        # Pad or truncate
        if len(features) < target_dim:
            features.extend([0.1] * (target_dim - len(features)))
        else:
            features = features[:target_dim]
        
        # Normalize
        features_array = np.array(features)
        norm = np.linalg.norm(features_array)
        if norm > 0:
            features = (features_array / norm).tolist()
        
        return features

    async def check_model_availability(self, model_name: str) -> bool:
        """Check if a specific model is available"""
        try:
            models = await self.list_models()
            available_models = [model.get('name', '') for model in models]
            is_available = model_name in available_models
            logger.info(f"Model {model_name} availability: {is_available}")
            logger.info(f"Available models: {available_models}")
            return is_available
        except Exception as e:
            logger.error(f"Error checking model availability: {str(e)}")
            return False

    async def pull_model(self, model_name: str) -> bool:
        """Pull a model if it's not available"""
        try:
            payload = {"name": model_name}
            
            # Use longer timeout for model pulling
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.base_url}/api/pull",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=300)  # 5 minutes
                    ) as response:
                        response.raise_for_status()
                        logger.info(f"Successfully pulled model: {model_name}")
                        return True
            except:
                # Fallback to sync
                response = requests.post(
                    f"{self.base_url}/api/pull",
                    json=payload,
                    timeout=300
                )
                response.raise_for_status()
                logger.info(f"Successfully pulled model: {model_name}")
                return True
                
        except Exception as e:
            logger.error(f"Error pulling model {model_name}: {str(e)}")
            return False

    async def ensure_embedding_model(self) -> bool:
        """Ensure embedding model is available"""
        try:
            # Check if current embedding model is available
            if await self.check_model_availability(self.embedding_model):
                logger.info(f"Embedding model {self.embedding_model} is available")
                return True
            
            # If not available and it's different from default model, try default model
            if self.embedding_model != self.default_model:
                if await self.check_model_availability(self.default_model):
                    logger.info(f"Using default model {self.default_model} for embeddings")
                    self.embedding_model = self.default_model
                    return True
            
            # Try to pull the embedding model
            logger.info(f"Embedding model {self.embedding_model} not found, attempting to pull...")
            success = await self.pull_model(self.embedding_model)
            
            if not success and self.embedding_model != self.default_model:
                logger.info(f"Failed to pull {self.embedding_model}, falling back to {self.default_model}")
                self.embedding_model = self.default_model
                return await self.check_model_availability(self.default_model)
            
            return success
            
        except Exception as e:
            logger.error(f"Error ensuring embedding model: {str(e)}")
            return False
===== ./app/services/voice_service.py =====
import logging
from typing import Optional, Tuple
from fastapi import UploadFile
from app.config import settings
import io
import numpy as np
import torch
import torchaudio

logger = logging.getLogger(__name__)

class VoiceService:
    def __init__(self):
        self.whisper_model = None
        self.tts_model = None
        self.load_models()

    def load_models(self):
        """Lazy load voice models"""
        try:
            if settings.ENABLE_WHISPER:
                import whisper
                self.whisper_model = whisper.load_model(settings.WHISPER_MODEL)
                
            if settings.ENABLE_TTS:
                if settings.TTS_ENGINE == "coqui":
                    from TTS.api import TTS
                    self.tts_model = TTS(model_name=settings.TTS_MODEL)
                elif settings.TTS_ENGINE == "pyttsx3":
                    import pyttsx3
                    self.tts_model = pyttsx3.init()
        except ImportError as e:
            logger.warning(f"Voice models not available: {str(e)}")

    async def speech_to_text(
        self,
        audio_file: UploadFile,
        language: Optional[str] = None
    ) -> str:
        """Convert speech to text using Whisper"""
        if not self.whisper_model:
            raise RuntimeError("Whisper is not enabled or failed to load")
            
        try:
            # Read audio file
            contents = await audio_file.read()
            audio_buffer = io.BytesIO(contents)
            
            # Load audio with torchaudio
            waveform, sample_rate = torchaudio.load(audio_buffer)
            
            # Resample if needed (Whisper expects 16kHz)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(
                    orig_freq=sample_rate,
                    new_freq=16000
                )
                waveform = resampler(waveform)
            
            # Convert to numpy array
            audio_np = waveform.numpy().squeeze()
            
            # Transcribe
            result = self.whisper_model.transcribe(
                audio_np,
                language=language,
                fp16=torch.cuda.is_available()
            )
            return result["text"]
        except Exception as e:
            logger.error(f"Error in speech-to-text: {str(e)}")
            raise

    async def text_to_speech(
        self,
        text: str,
        output_format: str = "wav"
    ) -> bytes:
        """Convert text to speech"""
        if not self.tts_model:
            raise RuntimeError("TTS is not enabled or failed to load")
            
        try:
            if settings.TTS_ENGINE == "coqui":
                # Coqui TTS
                output = io.BytesIO()
                self.tts_model.tts_to_file(
                    text=text,
                    file_path=output,
                    speaker=self.tts_model.speakers[0],
                    language=self.tts_model.languages[0]
                )
                return output.getvalue()
                
            elif settings.TTS_ENGINE == "pyttsx3":
                # pyttsx3 TTS
                output = io.BytesIO()
                self.tts_model.save_to_file(text, output)
                self.tts_model.runAndWait()
                return output.getvalue()
                
            else:
                raise ValueError("Unsupported TTS engine")
        except Exception as e:
            logger.error(f"Error in text-to-speech: {str(e)}")
            raise
===== ./app/dependencies.py =====
from fastapi import Depends
from app.utils.auth import verify_token, get_current_user
from app.utils.helpers import get_db
from sqlalchemy.orm import Session

# Database dependency
def get_db_session():
    return Depends(get_db)

# Authentication dependencies
def verify_token_dep():
    return Depends(verify_token)

def get_current_user_dep():
    return Depends(get_current_user)
===== ./app/init_db.py =====
import asyncio
from sqlalchemy import text
from app.database import engine, Base
from app.models.db_models import DBAgent
import logging

logging.basicConfig()
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

async def init_db():
    async with engine.begin() as conn:
        # Create schema
        await conn.execute(text("CREATE SCHEMA IF NOT EXISTS llm"))
        # Set search path to include llm schema
        await conn.execute(text("SET search_path TO llm, public"))
        # Create tables
        await conn.run_sync(Base.metadata.create_all)
    print("Database initialized successfully!")

if __name__ == "__main__":
    asyncio.run(init_db())
