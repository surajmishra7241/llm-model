===== ./app/routers/auth.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from datetime import timedelta
from app.utils.auth import create_access_token
from app.config import settings

router = APIRouter(tags=["Authentication"])

# Mock user database - replace with real checks in production
fake_users_db = {
    "nodejs_service": {
        "username": "nodejs_service",
        "password": "nodejs_service_password",
        "id": "nodejs_service",  # Add this line
        "sub": "nodejs_service"   # Add this line for JWT compatibility
    }
}

@router.post("/token")
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    user = fake_users_db.get(form_data.username)
    if not user or user["password"] != form_data.password:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
        )
    
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=timedelta(minutes=settings.JWT_ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    return {"access_token": access_token, "token_type": "bearer"}
===== ./app/routers/health.py =====
# ./app/routers/health.py
from fastapi import APIRouter, HTTPException
from app.services.llm_service import OllamaService
import logging

router = APIRouter(prefix="/health", tags=["Health"])
logger = logging.getLogger(__name__)

@router.get("/ollama")
async def check_ollama_health():
    """Check Ollama service health and model availability"""
    try:
        service = LLMService()
        status = await service.check_ollama_status()
        return status
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models")
async def list_available_models():
    """List all available models in Ollama"""
    try:
        service = LLMService()
        models = await service.list_models()
        return {"models": models}
    except Exception as e:
        logger.error(f"Failed to list models: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/pull-model/{model_name}")
async def pull_model(model_name: str):
    """Pull a specific model"""
    try:
        service = LLMService()
        success = await service.pull_model(model_name)
        if success:
            return {"message": f"Model {model_name} pulled successfully"}
        else:
            raise HTTPException(status_code=500, detail=f"Failed to pull model {model_name}")
    except Exception as e:
        logger.error(f"Failed to pull model {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/routers/monitoring.py =====
# ./app/routers/monitoring.py
from fastapi import APIRouter, Depends
from fastapi.responses import JSONResponse
from app.utils.monitoring import get_metrics
from app.dependencies import get_current_user

router = APIRouter(prefix="/monitoring", tags=["Monitoring"])

@router.get("/metrics")
async def metrics_endpoint(user: dict = Depends(get_current_user)):
    """Get Prometheus metrics"""
    if not user.get("is_admin", False):
        return JSONResponse(
            status_code=403,
            content={"detail": "Only admin users can access metrics"}
        )
    return get_metrics()
===== ./app/routers/execute.py =====
# app/routers/execute.py
from fastapi import APIRouter, Depends, HTTPException
from app.dependencies import get_current_user, get_db
from app.services.agent_execution_service import execute_agent
from sqlalchemy.ext.asyncio import AsyncSession

router = APIRouter(prefix="/execute", tags=["Execution"])

@router.post("/{agent_id}")
async def execute_agent_endpoint(
    agent_id: str,
    request_data: dict,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    try:
        # Extract input and parameters from request_data
        input_text = request_data.get("input", "")
        parameters = request_data.get("parameters", {})
        
        # Create the input_data dict expected by execute_agent
        input_data = {
            "message": input_text,
            "parameters": parameters
        }
        
        result = await execute_agent(agent_id, user["sub"], input_data, db)
        return result
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Execution failed: {str(e)}")
===== ./app/routers/__init__.py =====
# ./app/routers/__init__.py
from .auth import router as auth_router
from .agents import router as agents_router
from .chat import router as chat_router
from .rag import router as rag_router
from .training import router as training_router
from .voice import router as voice_router
from .agent_interaction import router as agent_interaction_router
from .health import router as health_router
from .execute import router as execute_router
from .monitoring import router as monitoring_router

__all__ = [
    "auth_router",
    "agents_router",
    "chat_router",
    "rag_router",
    "training_router",
    "voice_router",
    "agent_interaction_router",
    "health_router",
    "execute_router",
    "monitoring_router"
]
===== ./app/routers/voice_agent.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Path
from fastapi.responses import StreamingResponse
from app.services.voice_service import VoiceService
from app.services.voice_agent_service import VoiceAgentService
from app.services.agent_service import AgentService
from app.dependencies import get_current_user
import io

router = APIRouter()

@router.post("/voice-chat/{agent_id}")
async def voice_chat_with_agent(
    agent_id: int = Path(..., title="The ID of the agent to chat with"),
    audio_file: UploadFile = File(...),
    user: dict = Depends(get_current_user),
    voice_service: VoiceService = Depends(),
    voice_agent_service: VoiceAgentService = Depends(),
    agent_service: AgentService = Depends(),
):
    """
    Handles a full voice conversation with an agent.
    1. Converts user's speech to text.
    2. Gets a conversational response from the agent's brain.
    3. Converts the response back to speech and streams it.
    """
    if not audio_file.content_type.startswith('audio/'):
        raise HTTPException(status_code=400, detail="File must be an audio file")

    try:
        # Get user ID from the token
        user_id = user.get("user_id")
        if not user_id:
            raise HTTPException(status_code=401, detail="Could not validate credentials")

        # 1. Get agent details
        agent = await agent_service.get_agent(agent_id, user_id)
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")

        # 2. Speech-to-Text
        user_text = await voice_service.speech_to_text(audio_file)
        if not user_text:
            # Return empty audio if transcription fails
            return StreamingResponse(io.BytesIO(), media_type="audio/wav")

        # 3. Get response from agent's brain
        agent_response_text = await voice_agent_service.voice_chat(user_id, user_text, agent)

        # 4. Text-to-Speech
        audio_data = await voice_service.text_to_speech(agent_response_text)
        
        return StreamingResponse(io.BytesIO(audio_data), media_type="audio/wav")

    except HTTPException as e:
        # Re-raise HTTP exceptions
        raise e
    except Exception as e:
        # Handle other exceptions
        raise HTTPException(status_code=500, detail=str(e))

===== ./app/routers/agents.py =====
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse
from app.services.agent_service import AgentService
from app.dependencies import get_db, get_current_user
from typing import List
from sqlalchemy import select, delete
from app.models.db_models import DBAgent
from typing import TYPE_CHECKING

router = APIRouter(prefix="/agents", tags=["agents"])
if TYPE_CHECKING:
    from app.models.agent_model import AgentCreate, AgentUpdate, AgentResponse


@router.post("", response_model=AgentResponse, status_code=status.HTTP_201_CREATED)
async def create_agent(
    agent_data: AgentCreate,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    db_agent = DBAgent(
        owner_id=user["sub"],
        name=agent_data.name,
        description=agent_data.description,
        model=agent_data.model,
        system_prompt=agent_data.system_prompt,
        is_public=agent_data.is_public,
        tools=agent_data.tools,
        agent_metadata=agent_data.metadata
    )
    db.add(db_agent)
    await db.commit()
    await db.refresh(db_agent)
    return db_agent

@router.get("", response_model=List[AgentResponse])
async def list_agents(
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    result = await db.execute(
        select(DBAgent).where(DBAgent.owner_id == user["sub"])
    )
    return result.scalars().all()

@router.get("/{agent_id}", response_model=AgentResponse)
async def get_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    agent = await service.get_agent(agent_id, user["sub"])
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    return agent

@router.put("/{agent_id}", response_model=AgentResponse)
async def update_agent(
    agent_id: str,
    agent_data: AgentUpdate,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    service = AgentService(db)
    agent = await service.update_agent(agent_id, user["sub"], agent_data)
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    return agent

@router.delete("/{agent_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_agent(
    agent_id: str,
    db: AsyncSession = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """Delete an agent and all its associated data"""
    # Import services inside the function to avoid dependency validation issues
    from app.services.rag_service import RAGService
    from app.services.conversation_service import ConversationService
    
    # Verify agent belongs to user
    result = await db.execute(
        select(DBAgent).where(
            DBAgent.id == agent_id,
            DBAgent.owner_id == user["sub"]
        )
    )
    agent = result.scalars().first()
    if not agent:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Agent not found"
        )
    
    # Create service instances inside the function
    rag_service = RAGService(db)
    conversation_service = ConversationService(db)
    
    # Delete all agent data
    await rag_service.delete_agent_data(agent_id, user["sub"])
    await conversation_service.clear_conversation(agent_id, user["sub"])
    
    # Delete agent
    await db.execute(
        delete(DBAgent).where(
            DBAgent.id == agent_id,
            DBAgent.owner_id == user["sub"]
        )
    )
    await db.commit()
===== ./app/routers/voice.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from app.models.voice_model import VoiceResponse, TextToSpeechRequest
from app.services.voice_service import VoiceService
from app.dependencies import get_current_user
from fastapi.responses import StreamingResponse
import io

router = APIRouter()

@router.post("/stt", response_model=VoiceResponse)
async def speech_to_text(
    audio_file: UploadFile = File(...),
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert speech to text"""
    try:
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
        
        text = await voice_service.speech_to_text(audio_file)
        return VoiceResponse(text=text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tts")
async def text_to_speech(
    request: TextToSpeechRequest,
    voice_service: VoiceService = Depends(),
    user: dict = Depends(get_current_user)
):
    """Convert text to speech"""
    try:
        audio_data = await voice_service.text_to_speech(request.text)
        return StreamingResponse(
            io.BytesIO(audio_data),
            media_type="audio/wav"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/routers/chat.py =====
# app/routers/chat.py
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
from typing import Optional

from app.models.response_schema import ChatRequest, ChatResponse
from app.services.llm_service import OllamaService
from app.dependencies import get_current_user, get_db
from app.services.conversation_service import ConversationService
from app.models.db_models import DBAgent
from app.services.cache import cache_service
import json

router = APIRouter(prefix="/agents/{agent_id}", tags=["chat"])

async def get_chat_history_from_cache(agent_id: str, user_id: str) -> list:
    """Get chat history from Redis cache"""
    cache_key = f"chat_history:{user_id}:{agent_id}"
    history = await cache_service.get(cache_key)
    return json.loads(history) if history else []

async def save_chat_history_to_cache(agent_id: str, user_id: str, history: list):
    """Save chat history to Redis cache"""
    cache_key = f"chat_history:{user_id}:{agent_id}"
    await cache_service.set(cache_key, json.dumps(history), ttl=86400)  # 24 hours TTL

@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(
    agent_id: str,
    chat_request: ChatRequest,
    user: dict = Depends(get_current_user),
    llm_service: OllamaService = Depends(),
    db: AsyncSession = Depends(get_db),  # Proper dependency injection
):
    # Verify agent exists and user has access
    try:
        result = await db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                or_(
                    DBAgent.owner_id == user["sub"],
                    DBAgent.is_public == True
                )
            )
        )
        agent = result.scalars().first()
        if not agent:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Agent not found or access denied"
            )
        
        # Get conversation history from Redis
        conversation_history = await get_chat_history_from_cache(agent_id, user["sub"])
        
        # Build messages with context
        messages = [
            {"role": "system", "content": agent.system_prompt},
            *conversation_history,
            {"role": "user", "content": chat_request.message}
        ]
        
        # Get response from LLM
        response = await llm_service.chat(
            messages=messages,
            model=agent.model,
            options=chat_request.options
        )
        
        # Update conversation history
        new_history = conversation_history + [
            {"role": "user", "content": chat_request.message},
            {"role": "assistant", "content": response.get("message", {}).get("content", "")}
        ]
        
        # Keep only last 10 messages to avoid too large context
        new_history = new_history[-10:]
        
        # Save updated history to Redis
        await save_chat_history_to_cache(agent_id, user["sub"], new_history)
        
        return ChatResponse(
            message=response.get("message", {}).get("content", ""),
            model=agent.model,
            context=new_history,
            tokens_used=response.get("eval_count", 0)
        )
    finally:
        await db.close()
===== ./app/routers/rag.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, status
from typing import List, Optional
from app.models.response_schema import RAGResponse, DocumentResponse, RAGQueryRequest
from app.services.rag_service import RAGService
from app.dependencies import get_current_user, get_db
from sqlalchemy.ext.asyncio import AsyncSession
import logging

router = APIRouter(prefix="/rag", tags=["RAG Operations"])
logger = logging.getLogger(__name__)

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post(
    "/upload",
    response_model=DocumentResponse,
    status_code=status.HTTP_201_CREATED
)
async def upload_document(
    file: UploadFile = File(...),
    agent_id: Optional[str] = None,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload a document for RAG processing"""
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No filename provided"
        )

    try:
        # Validate file type
        if not file.filename.lower().endswith(('.pdf', '.txt', '.md')):
            raise HTTPException(
                status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail="Only PDF, TXT, and MD files are supported"
            )

        contents = await file.read()
        
        # Extract user ID properly
        user_id = extract_user_id(user)
        
        doc_id = await rag_service.ingest_document(
            db=db,
            user_id=user_id,
            filename=file.filename,
            content=contents,
            agent_id=agent_id
        )

        return DocumentResponse(
            document_id=str(doc_id),
            filename=file.filename,
            status="success"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document upload failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process document"
        )

# rag.py
@router.post("/query", response_model=RAGResponse)
async def query_documents(
    request: RAGQueryRequest,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    agent_id: Optional[str] = None  # Add this parameter
):
    """Enhanced query endpoint with hybrid search capabilities"""
    if not request.query or len(request.query.strip()) < 3:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Query must be at least 3 characters"
        )

    try:
        user_id = extract_user_id(user)
        
        results = await rag_service.query(
            db=db,
            user_id=user_id,
            query=request.query,
            max_results=request.max_results,
            filters=request.filters,
            rewrite_query=request.rewrite_query,
            use_reranking=request.use_reranking,
            agent_id=agent_id  # Pass the agent_id
        )
        
        return RAGResponse(
            answer=results.get("answer", "No answer found"),
            documents=results.get("documents", []),
            context=results.get("context", []),
            sources=results.get("sources", []),
            debug=results.get("debug_info", {}),
            search_method=results.get("debug_info", {}).get("search_method"),
            processed_query=results.get("debug_info", {}).get("processed_query")
        )
    except Exception as e:
        logger.error(f"Query failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process query"
        )

@router.get("/documents", response_model=List[DocumentResponse])
async def list_documents(
    agent_id: Optional[str] = None,  # Add this parameter
    page: int = 1,
    per_page: int = 10,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List all documents for the user with pagination"""
    try:
        if page < 1 or per_page < 1:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Page and per_page must be positive integers"
            )

        user_id = extract_user_id(user)

        return await rag_service.list_documents(
            db=db,
            agent_id=agent_id,
            user_id=user_id,
            page=page,
            per_page=per_page
        )

    except Exception as e:
        logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve documents"
        )

@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    rag_service: RAGService = Depends(),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    agent_id: Optional[str] = None
):
    """Delete a specific document"""
    try:
        # Clean the document ID by removing any quotes or special characters
        document_id = document_id.strip('"\'')
        user_id = extract_user_id(user)
        
        logger.info(f"Attempting to delete document {document_id} for user {user_id}")
        
        success = await rag_service.delete_document(
            db=db,
            user_id=user_id,
            document_id=document_id,
            agent_id=agent_id
        )
        
        if not success:
            logger.warning(f"Document not found: {document_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or not owned by user"
            )
            
        logger.info(f"Successfully deleted document {document_id}")
        return {"status": "success", "deleted_id": document_id}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document deletion failed for {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete document: {str(e)}"
        )
===== ./app/routers/training.py =====
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks, status, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobCreate, 
    TrainingProgress,
    TrainingDataItem,
    TrainingDataType,
    is_supported_file,
    get_file_type,
    FileUploadInfo
)
from app.services.training_service import TrainingService
from app.dependencies import get_current_user
import logging
import uuid
import json
import os
from datetime import datetime

router = APIRouter()
logger = logging.getLogger(__name__)

# Global instance to maintain state across requests
_training_service_instance = TrainingService()

def get_training_service() -> TrainingService:
    """Get training service singleton instance"""
    return _training_service_instance

def extract_user_id(user: dict) -> str:
    """Extract user ID from JWT payload or user dict"""
    if isinstance(user, dict):
        # JWT tokens typically use 'sub' (subject) for user ID
        user_id = (user.get('sub') or 
                  user.get('id') or 
                  user.get('user_id') or 
                  user.get('email'))
        if user_id:
            return str(user_id)
    
    raise ValueError(f"Could not extract user ID from user object: {user}")

@router.post("", response_model=TrainingJobResponse)
async def create_training_job(
    training_data: TrainingJobCreate,
    background_tasks: BackgroundTasks,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a new training job with enhanced data support"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job for user_id: {user_id}, agent_id: {training_data.agent_id}")
        
        # Validate that at least some data is provided
        has_data = any([
            training_data.data_urls,
            training_data.training_data,
            training_data.text_data
        ])
        
        if not has_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one data source must be provided (data_urls, training_data, or text_data)"
            )
            
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=training_data.agent_id,
            data_urls=training_data.data_urls or [],
            training_data=training_data.training_data or [],
            text_data=training_data.text_data or [],
            config=training_data.config or {}
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created enhanced job: {job.id} for user: {user_id}, agent: {training_data.agent_id}")
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.post("/upload", response_model=TrainingJobResponse)
async def create_training_job_with_files(
    files: List[UploadFile],
    background_tasks: BackgroundTasks,
    agent_id: str = Form(...),
    text_data: Optional[str] = Form(None),  # JSON string of text array
    config: Optional[str] = Form("{}"),     # JSON string of config
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Create a training job with file uploads"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Creating training job with files for user_id: {user_id}, agent_id: {agent_id}")
        
        # Validate files
        if not files or len(files) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="At least one file must be uploaded"
            )
        
        uploaded_files = []
        training_data_items = []
        
        for file in files:
            if not file.filename:
                continue
                
            if not is_supported_file(file.filename):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"File type not supported: {file.filename}"
                )
            
            # Generate unique file ID
            file_id = str(uuid.uuid4())
            file_content = await file.read()
            
            # Store file information
            file_info = FileUploadInfo(
                filename=file.filename,
                content_type=file.content_type or "application/octet-stream",
                size=len(file_content),
                file_id=file_id
            )
            uploaded_files.append(file_info)
            
            # Save file temporarily for processing
            temp_path = await training_service.save_uploaded_file(
                user_id, file_id, file.filename, file_content
            )
            
            # Create training data item
            data_item = TrainingDataItem(
                type=get_file_type(file.filename),
                content=temp_path,
                metadata={
                    "filename": file.filename,
                    "file_id": file_id,
                    "content_type": file.content_type,
                    "size": len(file_content)
                }
            )
            training_data_items.append(data_item)
        
        # Parse text data if provided
        parsed_text_data = []
        if text_data:
            try:
                parsed_text_data = json.loads(text_data)
                if not isinstance(parsed_text_data, list):
                    raise ValueError("text_data must be a JSON array")
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for text_data"
                )
        
        # Parse config
        parsed_config = {}
        if config:
            try:
                parsed_config = json.loads(config)
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid JSON format for config"
                )
        
        job = await training_service.create_job(
            user_id=user_id,
            agent_id=agent_id,
            data_urls=[],
            training_data=training_data_items,
            text_data=parsed_text_data,
            uploaded_files=uploaded_files,
            config=parsed_config
        )
        
        background_tasks.add_task(
            training_service.run_training,
            job_id=job.id,
            user_id=user_id
        )
        
        logger.info(f"Created file-based job: {job.id} with {len(uploaded_files)} files")
        return job
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating training job with files: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("", response_model=List[TrainingJobResponse])
async def list_training_jobs(
    agent_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """List training jobs for an agent"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Listing jobs for user_id: {user_id}, agent_id: {agent_id}")
        
        jobs = await training_service.list_jobs(user_id, agent_id)
        logger.info(f"Found {len(jobs)} jobs for user: {user_id}, agent: {agent_id}")
        
        return jobs
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing training jobs: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )

@router.get("/{job_id}/progress", response_model=TrainingProgress)
async def get_training_progress(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get real-time training progress"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting progress for job: {job_id}, user: {user_id}")
        
        progress = await training_service.get_progress(user_id, job_id)
        if not progress:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or access denied"
            )
        
        return progress
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training progress: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.delete("/{job_id}")
async def cancel_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Cancel a training job"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Cancelling job: {job_id} for user: {user_id}")
        
        success = await training_service.cancel_job(user_id, job_id)
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found or cannot be cancelled"
            )
        
        return {"message": "Job cancelled successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/debug/all-jobs")
async def debug_all_jobs(
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Debug endpoint to see all jobs in memory"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Debug: Getting all jobs for user: {user_id}")
        
        # Access the jobs dictionary directly for debugging
        all_jobs = []
        for job_id, job in training_service.jobs.items():
            job_info = {
                "job_id": job_id,
                "user_id": job.user_id,
                "agent_id": job.agent_id,
                "status": job.status,
                "created_at": job.created_at,
                "data_sources": {
                    "urls": len(job.data_urls),
                    "training_data": len(job.training_data),
                    "text_data": len(job.text_data),
                    "uploaded_files": len(job.uploaded_files)
                }
            }
            all_jobs.append(job_info)
        
        # Filter for current user
        user_jobs = [job for job in all_jobs if job["user_id"] == user_id]
        
        return {
            "current_user_id": user_id,
            "total_jobs_in_system": len(all_jobs),
            "jobs_for_current_user": len(user_jobs),
            "all_jobs": all_jobs,
            "user_jobs": user_jobs
        }
    except Exception as e:
        logger.error(f"Debug endpoint error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{job_id}", response_model=TrainingJobResponse)
async def get_training_job(
    job_id: str,
    training_service: TrainingService = Depends(get_training_service),
    user: dict = Depends(get_current_user)
):
    """Get training job details"""
    try:
        user_id = extract_user_id(user)
        logger.info(f"Getting job: {job_id} for user: {user_id}")
        
        job = await training_service.get_job(user_id, job_id)
        if not job:
            logger.warning(f"Job not found: {job_id} for user: {user_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, 
                detail="Job not found"
            )
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training job: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=str(e)
        )
===== ./app/routers/agent_interaction.py =====
# ./app/routers/agent_interaction.py
from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, Form
from fastapi.responses import Response
from app.services.agent_interaction_service import AgentInteractionService
from app.dependencies import get_db, get_current_user
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Optional
import logging
import base64

router = APIRouter(prefix="/interact", tags=["Agent Interaction"])
logger = logging.getLogger(__name__)

@router.post("/chat/{agent_id}")
async def chat_with_agent(
    agent_id: str,
    input_data: dict,
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Chat with an agent using text"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            input_text=input_data.get("message"),
            db=db
        )
        return response
    except Exception as e:
        logger.error(f"Chat failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/voice/{agent_id}")
async def voice_interaction(
    agent_id: str,
    audio_file: UploadFile = File(...),
    user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Interact with an agent using voice"""
    service = AgentInteractionService()
    await service.initialize()
    
    try:
        # Validate audio file
        if audio_file.content_type and not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="File must be an audio file")
        
        logger.info(f"Processing voice interaction for agent {agent_id}")
        logger.info(f"Audio file: {audio_file.filename}, size: {audio_file.size if hasattr(audio_file, 'size') else 'unknown'}")
        
        response = await service.process_input(
            agent_id=agent_id,
            user_id=user["sub"],
            audio_file=audio_file,
            db=db
        )
        
        # Convert response to speech
        speech_response = await service.text_to_speech(
            response["text_response"],
            response["emotional_state"]
        )
        
        return {
            "text": response["text_response"],
            "audio": base64.b64encode(speech_response).decode() if speech_response else None,
            "emotion": response["emotional_state"],
            "context_used": response.get("context_used", [])
        }
    except Exception as e:
        logger.error(f"Voice interaction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Update the chat history endpoints in agent_interaction.py

@router.get("/history/{agent_id}")
async def get_conversation_history(
    agent_id: str,
    user: dict = Depends(get_current_user)
):
    """Get conversation history for a user with specific agent"""
    try:
        history = await cache_service.get_chat_history(user["sub"], agent_id)
        return {"history": history or []}
    except Exception as e:
        logger.error(f"Failed to get conversation history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/history/{agent_id}")
async def clear_conversation_history(
    agent_id: str,
    user: dict = Depends(get_current_user)
):
    """Clear conversation history for a user with specific agent"""
    try:
        success = await cache_service.clear_chat_history(user["sub"], agent_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to clear history")
        return {"message": "Conversation history cleared"}
    except Exception as e:
        logger.error(f"Failed to clear conversation history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
===== ./app/middleware/logging_middleware.py =====
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response
import logging
from typing import Awaitable, Callable

logger = logging.getLogger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(
        self, 
        request: Request, 
        call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        # Log incoming request
        logger.info(f"Incoming request: {request.method} {request.url}")
        
        try:
            response = await call_next(request)
            # Log successful response
            logger.info(
                f"Request completed: {request.method} {request.url} "
                f"- Status: {response.status_code}"
            )
            return response
        except HTTPException as http_exc:
            # Log HTTP exceptions
            logger.warning(
                f"HTTP Exception: {request.method} {request.url} "
                f"- Status: {http_exc.status_code} - Detail: {http_exc.detail}"
            )
            raise
        except Exception as exc:
            # Log unexpected errors
            logger.error(
                f"Unexpected error: {request.method} {request.url} "
                f"- Error: {str(exc)}",
                exc_info=True
            )
            raise
===== ./app/middleware/rate_limiter.py =====
# middleware/rate_limiter.py

from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio
from collections import defaultdict
import time
import logging

logger = logging.getLogger(__name__)

class RateLimiterMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_requests=1000, time_window=60):
        super().__init__(app)
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_counts = defaultdict(lambda: {'count': 0, 'window_start': time.time()})
        self.lock = asyncio.Lock()

    async def dispatch(self, request: Request, call_next):
        client_ip = request.client.host if request.client else "unknown"
        
        async with self.lock:
            current_time = time.time()
            record = self.request_counts[client_ip]
            
            # Reset counter if time window has passed
            if current_time - record['window_start'] > self.time_window:
                record['count'] = 0
                record['window_start'] = current_time
            
            # Check rate limit
            if record['count'] >= self.max_requests:
                logger.warning(f"Rate limit exceeded for {client_ip}")
                return JSONResponse(
                    status_code=429,
                    content={"detail": "Too many requests"},
                    headers={"Retry-After": str(self.time_window)}
                )
            
            record['count'] += 1
        
        response = await call_next(request)
        return response
===== ./app/middleware/__init__.py =====
# ./app/middleware/__init__.py
from .logging_middleware import LoggingMiddleware
from .rate_limiter import RateLimiterMiddleware

__all__ = ["LoggingMiddleware", "RateLimiterMiddleware"]
===== ./app/middleware/errorHandlingMiddleware.py =====
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
import logging
import traceback
import time
from typing import Callable

logger = logging.getLogger(__name__)

class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        start_time = time.time()
        
        try:
            response = await call_next(request)
            return response
            
        except HTTPException as e:
            # Let FastAPI handle HTTP exceptions normally
            raise e
            
        except Exception as e:
            # Log the full error with traceback
            process_time = time.time() - start_time
            
            error_details = {
                "method": request.method,
                "url": str(request.url),
                "client": getattr(request.client, "host", "unknown") if request.client else "unknown",
                "process_time": f"{process_time:.3f}s",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            
            logger.error(
                f"Unhandled exception in {request.method} {request.url.path}: {str(e)}",
                extra=error_details
            )
            
            # Return a generic error response
            return JSONResponse(
                status_code=500,
                content={
                    "detail": "Internal server error",
                    "error_type": type(e).__name__,
                    "timestamp": time.time(),
                    "path": request.url.path
                }
            )
===== ./app/middleware/metrics_middleware.py =====
from prometheus_client import Counter, Histogram, make_asgi_app
from fastapi import Request, Response
from app.config import settings
import time

if settings.PROMETHEUS_ENABLED:
    REQUEST_COUNT = Counter(
        "http_requests_total",
        "Total HTTP Requests",
        ["method", "path", "status_code"]
    )
    
    REQUEST_LATENCY = Histogram(
        "http_request_duration_seconds",
        "HTTP Request Latency",
        ["method", "path"]
    )
    
    metrics_app = make_asgi_app()
    
    async def metrics_middleware(request: Request, call_next):
        if request.url.path == "/metrics":
            return await metrics_app(request.scope, request.receive, request.send)
            
        start_time = time.time()
        method = request.method
        path = request.url.path
        
        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception:
            status_code = 500
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_COUNT.labels(method, path, status_code).inc()
            REQUEST_LATENCY.labels(method, path).observe(duration)
            
        return response
else:
    async def metrics_middleware(request: Request, call_next):
        return await call_next(request)
===== ./app/config.py =====
# config.py

from pydantic_settings import BaseSettings
from typing import List, Optional, Dict, Any
from pydantic import Field, PostgresDsn, validator, RedisDsn, HttpUrl, conint
import logging
from urllib.parse import urlparse
import os

class Settings(BaseSettings):
    # App settings
    APP_VERSION: str = "1.0.0"
    APP_NAME: str = "AI Agent Platform"
    DEBUG: bool = False
    ENVIRONMENT: str = "development"
    
    # API settings
    API_PREFIX: str = "/api/v1"
    CORS_ORIGINS: List[str] = ["*"]
    
    # Database
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_SERVER: str = "localhost"
    POSTGRES_PORT: int = 5432
    POSTGRES_DB: str = "llm_agents"
    DATABASE_URL: Optional[PostgresDsn] = None
    
    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        return str(PostgresDsn.build(
            scheme="postgresql+asyncpg",
            username=values.get("POSTGRES_USER"),
            password=values.get("POSTGRES_PASSWORD"),
            host=values.get("POSTGRES_SERVER"),
            port=values.get("POSTGRES_PORT"),
            path=f"/{values.get('POSTGRES_DB') or ''}",
        ))
    
    # Redis
    REDIS_URL: RedisDsn = "redis://localhost:6379/0"
    CACHE_ENABLED: bool = True
    CACHE_TTL: int = 300
    
    # Ollama Configuration
    OLLAMA_URL: HttpUrl = "http://localhost:11434"
    DEFAULT_OLLAMA_MODEL: str = "deepseek-r1:1.5b"
    OLLAMA_TIMEOUT: conint(gt=0) = 60  # Increased timeout
    
    # Qdrant Configuration
    QDRANT_URL: HttpUrl = "http://localhost:6333"
    QDRANT_API_KEY: Optional[str] = None
    QDRANT_MAX_WORKERS: conint(gt=0) = 8
    QDRANT_COLLECTION_NAME: str = "documents"
    QDRANT_BATCH_SIZE: conint(gt=0) = 100
    QDRANT_TIMEOUT: conint(gt=0) = 30

    RATE_LIMITING_ENABLED: bool = True
    RATE_LIMIT_MAX_REQUESTS: int = 100
    RATE_LIMIT_TIME_WINDOW: int = 60  # seconds


    HYBRID_SEARCH_ENABLED: bool = True
    HYBRID_SPARSE_WEIGHT: float = Field(0.4, ge=0, le=1)
    HYBRID_DENSE_WEIGHT: float = Field(0.6, ge=0, le=1)
    
    # Query Processing
    QUERY_REWRITING_ENABLED: bool = True
    QUERY_EXPANSION_ENABLED: bool = True
    
    # Re-ranking
    RERANKING_ENABLED: bool = True
    RERANKING_MODEL: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
    # Metadata Filtering
    ENABLE_METADATA_FILTERING: bool = True
    DEFAULT_METADATA_FIELDS: List[str] = ["source", "author", "date", "document_type"]
    
    # Embedding Model Configuration
    # Use the same model as default for embeddings initially
    EMBEDDING_MODEL: str = "deepseek-r1:1.5b"  # Changed to use main model
    EXPECTED_EMBEDDING_DIMENSION: Optional[conint(gt=0)] = None
    EMBEDDING_FALLBACK_DIMENSION: conint(gt=0) = 768
    
    # Alternative embedding models to try (in order of preference)
    EMBEDDING_MODEL_ALTERNATIVES: List[str] = [
        "deepseek-r1:1.5b",
        "nomic-embed-text",
        "all-minilm",
        "mxbai-embed-large"
    ]

    # Voice settings
    ENABLE_WHISPER: bool = True
    WHISPER_MODEL: str = "base"  # or "small", "medium", "large"
    ENABLE_TTS: bool = True
    TTS_ENGINE: str = "coqui"  # or "pyttsx3"
    TTS_MODEL: str = "tts_models/en/ljspeech/glow-tts"
    TTS_VOICE: str = "female"
    
    # Document Processing
    DEFAULT_CHUNK_SIZE: conint(gt=0) = 1000
    DEFAULT_CHUNK_OVERLAP: conint(ge=0) = 200
    MAX_DOCUMENT_SIZE_MB: conint(gt=0) = 50
    SUPPORTED_FILE_TYPES: List[str] = [".pdf", ".txt", ".docx", ".pptx", ".xlsx"]
    
    # RAG Settings
    RAG_DEFAULT_MAX_RESULTS: conint(gt=0) = 5
    RAG_DEFAULT_MIN_SCORE: float = Field(ge=0, le=1, default=0.3)
    RAG_QUERY_TIMEOUT: conint(gt=0) = 30
    
    # Auth
    JWT_SECRET_KEY: str = "your-secret-key-here"
    JWT_ALGORITHM: str = "HS256"
    JWT_ACCESS_TOKEN_EXPIRE_MINUTES: int = 11440  # 24 hours
    
    # Monitoring
    PROMETHEUS_ENABLED: bool = False
    PROMETHEUS_PORT: conint(gt=0, le=65535) = 8001
    
    # Logging
    LOG_LEVEL: str = Field(pattern="^(DEBUG|INFO|WARNING|ERROR|CRITICAL)$", default="INFO")
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # Embedding fallback strategy
    USE_DETERMINISTIC_FALLBACK: bool = True
    FALLBACK_EMBEDDING_STRATEGY: str = Field(
        pattern="^(deterministic|hash|features|skip)$", 
        default="deterministic"
    )
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True
        extra = "ignore"

# Create settings instance
settings = Settings()

def validate_config():
    """Validate and log configuration"""
    logger = logging.getLogger(__name__)
    
    try:
        # Validate URLs
        parsed_ollama = urlparse(str(settings.OLLAMA_URL))
        if not all([parsed_ollama.scheme, parsed_ollama.netloc]):
            raise ValueError("Invalid OLLAMA_URL format")
            
        parsed_qdrant = urlparse(str(settings.QDRANT_URL))
        if not all([parsed_qdrant.scheme, parsed_qdrant.netloc]):
            raise ValueError("Invalid QDRANT_URL format")
        
        # Log important config
        logger.info("=== Application Configuration ===")
        logger.info(f"Environment: {settings.ENVIRONMENT}")
        logger.info(f"Debug Mode: {settings.DEBUG}")
        logger.info("")
        logger.info("=== Ollama Configuration ===")
        logger.info(f"Ollama URL: {settings.OLLAMA_URL}")
        logger.info(f"Default Model: {settings.DEFAULT_OLLAMA_MODEL}")
        logger.info(f"Embedding Model: {settings.EMBEDDING_MODEL}")
        logger.info(f"Timeout: {settings.OLLAMA_TIMEOUT}s")
        logger.info(f"Alternative Models: {settings.EMBEDDING_MODEL_ALTERNATIVES}")
        logger.info("")
        logger.info("=== Vector Database Configuration ===")
        logger.info(f"Qdrant URL: {settings.QDRANT_URL}")
        logger.info(f"Collection Name: {settings.QDRANT_COLLECTION_NAME}")
        logger.info(f"Expected Dimension: {settings.EXPECTED_EMBEDDING_DIMENSION or 'Auto-detect'}")
        logger.info(f"Fallback Dimension: {settings.EMBEDDING_FALLBACK_DIMENSION}")
        logger.info("")
        logger.info("=== Document Processing ===")
        logger.info(f"Chunk Size: {settings.DEFAULT_CHUNK_SIZE}")
        logger.info(f"Chunk Overlap: {settings.DEFAULT_CHUNK_OVERLAP}")
        logger.info(f"Max File Size: {settings.MAX_DOCUMENT_SIZE_MB}MB")
        logger.info(f"Supported Types: {settings.SUPPORTED_FILE_TYPES}")
        logger.info("")
        logger.info("=== RAG Configuration ===")
        logger.info(f"Max Results: {settings.RAG_DEFAULT_MAX_RESULTS}")
        logger.info(f"Min Score Threshold: {settings.RAG_DEFAULT_MIN_SCORE}")
        logger.info(f"Query Timeout: {settings.RAG_QUERY_TIMEOUT}s")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {str(e)}")
        raise

# Validate on import (only in production-like environments)
if "pytest" not in os.sys.modules and __name__ != "__main__":
    try:
        validate_config()
    except Exception as e:
        print(f"Warning: Configuration validation failed: {e}")
        # Don't raise in case this is being imported during setup
===== ./app/database.py =====
# ./app/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Get the database URL as string and ensure it uses asyncpg
database_url = str(settings.DATABASE_URL).replace(
    "postgresql://", 
    "postgresql+asyncpg://"
)

engine = create_async_engine(
    database_url,
    echo=settings.DEBUG,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    connect_args={
        "server_settings": {
            "application_name": settings.APP_NAME,
            "search_path": "llm,public"
        }
    }
)

AsyncSessionLocal = sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False
)

Base = declarative_base()
===== ./app/__init__.py =====
# Initialize application package
from .config import settings

__all__ = ["settings"]
===== ./app/utils/auth.py =====
from datetime import datetime, timedelta
from jose import JWTError, jwt
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from app.config import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/auth/token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode, 
        settings.JWT_SECRET_KEY, 
        algorithm=settings.JWT_ALGORITHM
    )
    return encoded_jwt

# In utils/auth.py
async def verify_token(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            token, 
            settings.JWT_SECRET_KEY, 
            algorithms=[settings.JWT_ALGORITHM]
        )
        # Ensure sub (subject) is present
        if "sub" not in payload:
            raise credentials_exception
        return payload
    except JWTError:
        raise credentials_exception

# Add this function
def get_current_user(token_payload: dict = Depends(verify_token)):
    """
    Dependency that can be used to get the current user from the token
    """
    return token_payload
===== ./app/utils/logging.py =====
import logging
from logging.config import dictConfig
from app.config import settings
import sys

def configure_logging():
    logging_config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": settings.LOG_FORMAT,
                "datefmt": "%Y-%m-%d %H:%M:%S"
            },
        },
        "handlers": {
            "console": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.StreamHandler",
                "stream": sys.stdout,
            },
            "file": {
                "level": settings.LOG_LEVEL,
                "formatter": "standard",
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "app.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "encoding": "utf8"
            },
        },
        "loggers": {
            "app": {
                "handlers": ["console", "file"],
                "level": settings.LOG_LEVEL,
                "propagate": False
            },
            "sqlalchemy": {
                "handlers": ["console"],
                "level": "WARNING",
                "propagate": False
            },
        }
    }
    
    dictConfig(logging_config)
    logger = logging.getLogger("app")
    logger.info("Logging configured successfully")
    return logger

logger = configure_logging()
===== ./app/utils/cache_decorator.py =====
from typing import Any, Optional, Callable, Awaitable
from functools import wraps
from app.services.cache import cache_service
import json
import hashlib
import inspect

def cached(key_pattern: str, ttl: Optional[int] = None):
    """Decorator for caching async function results with smart key generation"""
    def decorator(func: Callable[..., Awaitable[Any]]):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not cache_service.enabled:
                return await func(*args, **kwargs)
            
            try:
                # Get function signature to map args to parameter names
                sig = inspect.signature(func)
                bound_args = sig.bind(*args, **kwargs)
                bound_args.apply_defaults()
                
                # Create a dictionary of all parameters
                params = dict(bound_args.arguments)
                
                # Generate query_hash if needed and not provided
                if 'query_hash' in key_pattern and 'query_hash' not in params:
                    query = params.get('query', '')
                    if query:
                        params['query_hash'] = hashlib.md5(query.encode()).hexdigest()[:8]
                    else:
                        params['query_hash'] = 'no_query'
                
                # Format the cache key with available parameters
                try:
                    cache_key = key_pattern.format(**params)
                except KeyError as e:
                    # If formatting fails, create a simple fallback key
                    func_name = func.__name__
                    args_hash = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()[:8]
                    cache_key = f"{func_name}:{args_hash}"
                
                # Try to get from cache
                cached_data = await cache_service.get(cache_key)
                if cached_data is not None:
                    return cached_data
                
                # Execute function and cache result
                result = await func(*args, **kwargs)
                await cache_service.set(cache_key, result, ttl)
                return result
                
            except Exception as e:
                # If caching fails, just execute the function
                return await func(*args, **kwargs)
                
        return wrapper
    return decorator
===== ./app/utils/monitoring.py =====
# ./app/utils/monitoring.py
from prometheus_client import start_http_server, Counter, Histogram, Gauge
from prometheus_client import generate_latest, REGISTRY
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP Requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP Request Latency',
    ['method', 'endpoint']
)

AGENT_EXECUTIONS = Counter(
    'agent_executions_total',
    'Total Agent Executions',
    ['agent_id', 'status']
)

RAG_REQUESTS = Counter(
    'rag_requests_total',
    'Total RAG Requests',
    ['status']
)

async def initialize():
    if settings.PROMETHEUS_ENABLED:
        start_http_server(settings.PROMETHEUS_PORT)
        logger.info(f"Metrics server started on port {settings.PROMETHEUS_PORT}")

def get_metrics():
    return generate_latest(REGISTRY)

def record_request(method: str, endpoint: str, status_code: int, duration: float):
    REQUEST_COUNT.labels(method, endpoint, status_code).inc()
    REQUEST_LATENCY.labels(method, endpoint).observe(duration)

def record_agent_execution(agent_id: str, success: bool):
    status = "success" if success else "failure"
    AGENT_EXECUTIONS.labels(agent_id, status).inc()

def record_rag_request(success: bool):
    status = "success" if success else "failure"
    RAG_REQUESTS.labels(status).inc()
===== ./app/utils/__init__.py =====
# Empty file to make utils a package
===== ./app/utils/chroma_async.py =====
# utils/chroma_async.py
import chromadb
import logging
from typing import List, Dict, Any, Optional, Callable, AsyncIterator
from concurrent.futures import ThreadPoolExecutor
import asyncio
from functools import partial
from contextlib import asynccontextmanager
import time
import threading
from collections import defaultdict
from app.config import settings

logger = logging.getLogger(__name__)

class AsyncChromaClient:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, path: str, embedding_function: Optional[Callable] = None, max_workers: int = 4):
        if not hasattr(self, '_initialized'):
            self.path = path
            self.embedding_function = embedding_function
            self.executor = ThreadPoolExecutor(
                max_workers=max_workers, 
                thread_name_prefix="chroma",
            )
            self._local = threading.local()
            self.collections = defaultdict(dict)
            self._initialized = True
            self._closed = False
    
    async def initialize(self):
        """Initialize the client in a thread-safe way"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            loop = asyncio.get_running_loop()
            self._local.client = await loop.run_in_executor(
                self.executor,
                partial(
                    chromadb.PersistentClient,
                    path=self.path,
                )
            )
            logger.info("AsyncChromaDB client initialized for thread %s", threading.current_thread().name)
    
    @asynccontextmanager
    async def get_collection(self, name: str, embedding_function=None, metadata=None) -> AsyncIterator[chromadb.Collection]:
        """Async context manager for collection access with connection pooling"""
        if self._closed:
            raise RuntimeError("Client is closed")
            
        await self.initialize()
        
        thread_id = threading.current_thread().ident
        collection_key = f"{name}_{thread_id}"
        
        if collection_key not in self.collections[thread_id]:
            loop = asyncio.get_running_loop()
            
            # Fix: Don't pass metadata parameter if it's None or empty
            collection_kwargs = {
                'name': name,
                'embedding_function': embedding_function or self.embedding_function,
            }
            
            # Only add metadata if it's not None and not empty
            if metadata:
                collection_kwargs['metadata'] = metadata
                
            collection = await loop.run_in_executor(
                self.executor,
                partial(
                    self._local.client.get_or_create_collection,
                    **collection_kwargs
                )
            )
            self.collections[thread_id][collection_key] = collection
        
        try:
            yield self.collections[thread_id][collection_key]
        except Exception as e:
            logger.error(f"Error in collection context for {name}: {str(e)}")
            raise
        finally:
            # Clean up if this is the last reference
            pass
    
    async def add_documents(
        self, 
        collection_name: str, 
        documents: List[str], 
        metadatas: List[Dict], 
        ids: List[str],
        embeddings: Optional[List[List[float]]] = None,
        batch_size: int = 100
    ):
        """Async document addition with improved batching and error handling"""
        if not documents:
            return
            
        async with self.get_collection(collection_name) as collection:
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i + batch_size]
                batch_metas = metadatas[i:i + batch_size] if metadatas else None
                batch_ids = ids[i:i + batch_size] if ids else None
                batch_embeds = embeddings[i:i + batch_size] if embeddings else None
                
                loop = asyncio.get_running_loop()
                
                try:
                    await loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.add,
                            documents=batch_docs,
                            metadatas=batch_metas,
                            ids=batch_ids,
                            embeddings=batch_embeds
                        )
                    )
                    
                    # Small delay between batches to prevent overwhelming the system
                    if i + batch_size < len(documents):
                        await asyncio.sleep(0.05)
                        
                except Exception as e:
                    logger.error(f"Failed to add batch {i//batch_size + 1}: {str(e)}")
                    raise
    
    async def query_collection(
        self, 
        collection_name: str, 
        query_texts: Optional[List[str]] = None, 
        query_embeddings: Optional[List[List[float]]] = None,
        n_results: int = 5, 
        where: Optional[Dict] = None, 
        where_document: Optional[Dict] = None,
        include: Optional[List[str]] = None,
        timeout: float = 30.0
    ) -> Dict[str, Any]:
        """Async query with improved error handling and timeout"""
        if not query_texts and not query_embeddings:
            raise ValueError("Either query_texts or query_embeddings must be provided")
            
        async with self.get_collection(collection_name) as collection:
            loop = asyncio.get_running_loop()
            
            try:
                return await asyncio.wait_for(
                    loop.run_in_executor(
                        self.executor,
                        partial(
                            collection.query,
                            query_texts=query_texts,
                            query_embeddings=query_embeddings,
                            n_results=n_results,
                            where=where,
                            where_document=where_document,
                            include=include or ["documents", "distances", "metadatas"]
                        )
                    ),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.error(f"Query timeout for collection {collection_name}")
                raise
            except Exception as e:
                logger.error(f"Query failed for collection {collection_name}: {str(e)}")
                raise
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        try:
            # Clear collections cache
            self.collections.clear()
            
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
                logger.info("AsyncChromaDB client closed")
                
        except Exception as e:
            logger.error(f"Error closing AsyncChromaDB client: {str(e)}")
        finally:
            self._instance = None

# Global instance with lazy initialization
_async_chroma_client = None
_async_chroma_lock = asyncio.Lock()

async def get_async_chroma_client() -> AsyncChromaClient:
    global _async_chroma_client
    async with _async_chroma_lock:
        if _async_chroma_client is None or _async_chroma_client._closed:
            _async_chroma_client = AsyncChromaClient(
                path=settings.CHROMA_PATH,
                embedding_function=None,  # Will be set per request
                max_workers=settings.CHROMA_MAX_WORKERS
            )
        return _async_chroma_client
===== ./app/utils/file_processing_training.py =====
import os
import logging
import requests
from typing import Optional, List
from io import BytesIO
import tempfile
from pathlib import Path

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup
import json
import csv

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> Optional[str]:
    """Extract text from PDF content"""
    try:
        pdf_file = BytesIO(content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        return None

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        from app.models.training_model import SUPPORTED_EXTENSIONS
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }
===== ./app/utils/file_processing.py =====
import os
import logging
import requests
from typing import Optional, List, Tuple, Dict, Any
from io import BytesIO
import tempfile
from pathlib import Path
import fitz  # PyMuPDF
import re
import json
import csv

# Document processing
import PyPDF2
import docx
from bs4 import BeautifulSoup

# Image processing (OCR)
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR dependencies not available. Install Pillow and pytesseract for image processing.")

# Audio processing (for future use)
try:
    import speech_recognition as sr
    SPEECH_AVAILABLE = True
except ImportError:
    SPEECH_AVAILABLE = False
    logging.warning("Speech recognition not available. Install SpeechRecognition for audio processing.")

logger = logging.getLogger(__name__)

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = os.path.splitext(filename.lower())[1]
    return ext in SUPPORTED_EXTENSIONS

def get_file_type(filename: str) -> str:
    """Determine the file type based on extension"""
    ext = os.path.splitext(filename.lower())[1]
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return 'image'
    elif ext in {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}:
        return 'audio'
    elif ext in {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'}:
        return 'video'
    else:
        return 'document'

def clean_text(text: str) -> str:
    """Clean text by removing excessive whitespace"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def process_document(filename: str, content: bytes) -> Optional[str]:
    """Process various document types and extract text"""
    try:
        file_ext = os.path.splitext(filename.lower())[1]
        
        if file_ext == '.pdf':
            return extract_text_from_pdf(content)
        elif file_ext in ['.doc', '.docx']:
            return extract_text_from_word(content)
        elif file_ext in ['.txt', '.md']:
            return content.decode('utf-8', errors='ignore')
        elif file_ext in ['.html', '.htm']:
            return extract_text_from_html(content)
        elif file_ext == '.json':
            return extract_text_from_json(content)
        elif file_ext == '.csv':
            return extract_text_from_csv(content)
        elif file_ext == '.xml':
            return extract_text_from_xml(content)
        elif file_ext == '.rtf':
            return extract_text_from_rtf(content)
        else:
            # Try to decode as text for unknown formats
            try:
                return content.decode('utf-8', errors='ignore')
            except:
                logger.warning(f"Unknown file format: {file_ext}")
                return None
                
    except Exception as e:
        logger.error(f"Error processing document {filename}: {str(e)}")
        return None

def process_image(filename: str, content: bytes) -> Optional[str]:
    """Process images using OCR to extract text"""
    if not OCR_AVAILABLE:
        logger.warning("OCR not available, skipping image processing")
        return None
    
    try:
        # Open image from bytes
        image = Image.open(BytesIO(content))
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Extract text using OCR
        text = pytesseract.image_to_string(image)
        
        # Clean up extracted text
        text = text.strip()
        if len(text) < 10:  # Minimum text threshold
            logger.warning(f"Very little text extracted from image: {filename}")
            return None
        
        return text
        
    except Exception as e:
        logger.error(f"Error processing image {filename}: {str(e)}")
        return None

def extract_text_from_pdf(content: bytes) -> str:
    """Extract text from PDF bytes using PyMuPDF"""
    try:
        with fitz.open(stream=content, filetype="pdf") as doc:
            text = "\n".join([page.get_text() for page in doc])
        return clean_text(text)
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {e}")
        return ""

def extract_text_from_word(content: bytes) -> Optional[str]:
    """Extract text from Word documents"""
    try:
        doc_file = BytesIO(content)
        doc = docx.Document(doc_file)
        
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    text += cell.text + " "
                text += "\n"
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from Word document: {str(e)}")
        return None

def extract_text_from_html(content: bytes) -> Optional[str]:
    """Extract text from HTML content"""
    try:
        html_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return clean_text(text)
        
    except Exception as e:
        logger.error(f"Error extracting text from HTML: {str(e)}")
        return None

def extract_text_from_json(content: bytes) -> Optional[str]:
    """Extract text from JSON content"""
    try:
        json_content = content.decode('utf-8', errors='ignore')
        data = json.loads(json_content)
        
        def extract_strings(obj, strings_list):
            """Recursively extract all string values from JSON"""
            if isinstance(obj, dict):
                for value in obj.values():
                    extract_strings(value, strings_list)
            elif isinstance(obj, list):
                for item in obj:
                    extract_strings(item, strings_list)
            elif isinstance(obj, str) and len(obj.strip()) > 2:
                strings_list.append(obj.strip())
        
        strings = []
        extract_strings(data, strings)
        
        return "\n".join(strings)
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON: {str(e)}")
        return None

def extract_text_from_csv(content: bytes) -> Optional[str]:
    """Extract text from CSV content"""
    try:
        csv_content = content.decode('utf-8', errors='ignore')
        csv_file = BytesIO(csv_content.encode())
        
        # Try different delimiters
        for delimiter in [',', ';', '\t', '|']:
            try:
                csv_file.seek(0)
                reader = csv.reader(csv_content.splitlines(), delimiter=delimiter)
                rows = list(reader)
                
                if len(rows) > 0 and len(rows[0]) > 1:
                    # Convert CSV to readable text
                    text_lines = []
                    headers = rows[0] if rows else []
                    
                    for row in rows:
                        if headers and len(row) == len(headers):
                            row_text = []
                            for header, value in zip(headers, row):
                                if value.strip():
                                    row_text.append(f"{header}: {value}")
                            if row_text:
                                text_lines.append(", ".join(row_text))
                        else:
                            text_lines.append(", ".join(cell for cell in row if cell.strip()))
                    
                    return "\n".join(text_lines)
            except:
                continue
        
        # Fallback: return raw content
        return csv_content
        
    except Exception as e:
        logger.error(f"Error extracting text from CSV: {str(e)}")
        return None

def extract_text_from_xml(content: bytes) -> Optional[str]:
    """Extract text from XML content"""
    try:
        xml_content = content.decode('utf-8', errors='ignore')
        soup = BeautifulSoup(xml_content, 'xml')
        
        # Extract all text content
        text = soup.get_text(separator=' ', strip=True)
        
        return text
        
    except Exception as e:
        logger.error(f"Error extracting text from XML: {str(e)}")
        return None

def extract_text_from_rtf(content: bytes) -> Optional[str]:
    """Extract text from RTF content (basic implementation)"""
    try:
        rtf_content = content.decode('utf-8', errors='ignore')
        
        # Very basic RTF parsing - remove RTF control codes
        import re
        
        # Remove RTF header
        text = re.sub(r'\\rtf\d+.*?(?=\\)', '', rtf_content)
        
        # Remove RTF control words
        text = re.sub(r'\\[a-z]+\d*\s?', '', text)
        
        # Remove curly braces
        text = re.sub(r'[{}]', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else None
        
    except Exception as e:
        logger.error(f"Error extracting text from RTF: {str(e)}")
        return None

def extract_text_from_url(url: str) -> Optional[str]:
    """Extract text content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content_type = response.headers.get('content-type', '').lower()
        
        if 'text/html' in content_type:
            return extract_text_from_html(response.content)
        elif 'application/json' in content_type:
            return extract_text_from_json(response.content)
        elif 'text/plain' in content_type:
            return response.text
        elif 'application/pdf' in content_type:
            return extract_text_from_pdf(response.content)
        else:
            # Try to extract as text
            try:
                return response.text
            except:
                logger.warning(f"Unknown content type for URL {url}: {content_type}")
                return None
                
    except Exception as e:
        logger.error(f"Error extracting text from URL {url}: {str(e)}")
        return None

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks"""
    if not text or len(text.strip()) < 10:
        return []
    
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        last_space = chunk.rfind(' ')
        
        # Choose the best breaking point
        break_point = max(last_period, last_newline, last_space)
        if break_point > start + chunk_size // 2:  # Only if break point is not too early
            actual_end = start + break_point + 1
        else:
            actual_end = end
        
        chunks.append(text[start:actual_end].strip())
        start = actual_end - overlap
        
        # Ensure we make progress
        if start <= 0:
            start = actual_end
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 10]

def process_audio_file(filename: str, content: bytes) -> Optional[str]:
    """Process audio files using speech recognition (future feature)"""
    if not SPEECH_AVAILABLE:
        logger.warning("Speech recognition not available")
        return None
    
    try:
        # Save audio to temporary file
        with tempfile.NamedTemporaryFile(suffix=os.path.splitext(filename)[1], delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Initialize recognizer
            r = sr.Recognizer()
            
            # Load audio file
            with sr.AudioFile(temp_path) as source:
                audio = r.record(source)
            
            # Recognize speech
            text = r.recognize_google(audio)
            return text
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"Error processing audio file {filename}: {str(e)}")
        return None

# File validation utilities
def validate_file_size(content: bytes, max_size_mb: int = 50) -> bool:
    """Validate file size"""
    size_mb = len(content) / (1024 * 1024)
    return size_mb <= max_size_mb

def validate_file_type(filename: str, allowed_extensions: set = None) -> bool:
    """Validate file type by extension"""
    if allowed_extensions is None:
        allowed_extensions = SUPPORTED_EXTENSIONS
    
    ext = os.path.splitext(filename.lower())[1]
    return ext in allowed_extensions

def get_file_info(filename: str, content: bytes) -> dict:
    """Get file information"""
    return {
        "filename": filename,
        "extension": os.path.splitext(filename.lower())[1],
        "size_bytes": len(content),
        "size_mb": round(len(content) / (1024 * 1024), 2),
        "estimated_text_length": len(content) // 2,  # Rough estimate
    }

    
===== ./app/utils/health_check.py =====
from fastapi import APIRouter, status
from fastapi.responses import JSONResponse
from app.config import settings
import logging

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/health", tags=["Health Check"])
async def health_check():
    """Basic health check endpoint"""
    try:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "healthy",
                "version": settings.APP_VERSION,
                "environment": settings.ENVIRONMENT
            }
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"status": "unhealthy", "error": str(e)}
        )
===== ./app/utils/helpers.py =====
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Convert Pydantic PostgresDsn to string and ensure proper format
def get_database_url() -> str:
    url = str(settings.DATABASE_URL)
    # Ensure there's no double slash before the database name
    if '//' in url.split('@')[-1]:
        parts = url.split('@')
        parts[-1] = parts[-1].replace('//', '/')
        url = '@'.join(parts)
    return url

# Create engine with properly formatted URL string
engine = create_async_engine(
    get_database_url(),
    future=True,
    echo=True
)

AsyncSessionLocal = sessionmaker(
    engine, 
    class_=AsyncSession,
    expire_on_commit=False
)

Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as db:
        yield db
===== ./app/utils/qdrant_async.py =====
import logging
import asyncio
import threading
from typing import List, Dict, Any, Optional, Tuple, Union
from contextlib import asynccontextmanager
from functools import partial
from concurrent.futures import ThreadPoolExecutor
import uuid
import time
from dataclasses import dataclass

from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse, ResponseHandlingException
from qdrant_client.http.models import (
    Distance, 
    VectorParams, 
    PointStruct, 
    Filter, 
    FieldCondition, 
    MatchValue,
    MatchAny,
    Range
)
from app.config import settings # Ensure this import path is correct based on your project structure

logger = logging.getLogger(__name__)

class QdrantConnectionError(Exception):
    """Custom exception for Qdrant connection issues"""
    pass

class QdrantOperationError(Exception):
    """Custom exception for Qdrant operation failures"""
    pass

@dataclass
class CollectionInfo:
    name: str
    status: str
    vector_size: int
    distance: str
    points_count: int

class AsyncQdrantClient:
    """Thread-safe async wrapper for Qdrant client with improved error handling"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, url: str = None, api_key: str = None, max_workers: int = None):
        if self._initialized:
            return
            
        self.url = url or str(settings.QDRANT_URL)
        self.api_key = api_key or settings.QDRANT_API_KEY
        self.max_workers = max_workers or settings.QDRANT_MAX_WORKERS
        self.timeout = settings.QDRANT_TIMEOUT
        self.batch_size = settings.QDRANT_BATCH_SIZE
        
        self.executor = ThreadPoolExecutor(
            max_workers=self.max_workers, 
            thread_name_prefix="qdrant_worker"
        )
        self._local = threading.local()
        self._initialized = True
        self._closed = False
        self._connection_verified = False
        
        logger.info(f"AsyncQdrantClient initialized with URL: {self.url}")
    
    def _get_client(self) -> QdrantClient:
        """Get thread-local client instance"""
        if not hasattr(self._local, 'client') or self._local.client is None:
            try:
                self._local.client = QdrantClient(
                    url=self.url,
                    api_key=self.api_key,
                    prefer_grpc=False, # Use HTTP API
                    timeout=self.timeout,
                    grpc_options={ # These are ignored if prefer_grpc is False, but kept for completeness
                        "grpc.keepalive_time_ms": 30000,
                        "grpc.max_receive_message_length": 100 * 1024 * 1024  # 100MB
                    }
                )
                logger.debug(f"Created new Qdrant client for thread {threading.current_thread().name}")
            except Exception as e:
                logger.error(f"Failed to create Qdrant client: {str(e)}", exc_info=True)
                raise QdrantConnectionError(f"Cannot connect to Qdrant at {self.url}: {str(e)}")
        
        return self._local.client
    
    async def _verify_connection(self) -> bool:
        """Verify Qdrant connection and service availability"""
        if self._connection_verified:
            return True
            
        try:
            loop = asyncio.get_running_loop()
            
            def _check_connection():
                client = self._get_client()
                try:
                    # First try to get collections as basic health check
                    collections = client.get_collections()
                    logger.info(f"Qdrant connection verified - {len(collections.collections)} collections found")
                    return True
                except Exception as e:
                    logger.error(f"Qdrant connection check failed: {str(e)}")
                    raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
            
            result = await loop.run_in_executor(self.executor, _check_connection)
            self._connection_verified = result
            logger.info("Qdrant connection verified successfully")
            return result
            
        except Exception as e:
            logger.error(f"Qdrant connection verification failed: {str(e)}", exc_info=True)
            raise QdrantConnectionError(f"Cannot verify connection to Qdrant: {str(e)}")
    
    async def initialize(self):
        """Initialize and verify the client connection"""
        if self._closed:
            raise QdrantConnectionError("Client has been closed")
            
        await self._verify_connection()
        logger.info("AsyncQdrant client initialized and verified")
    
    async def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine",
        recreate_if_exists: bool = False
    ) -> bool:
        """Create a new collection with specified parameters"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _create_collection():
                client = self._get_client()
                
                # Check if collection exists
                try:
                    collections = client.get_collections()
                    existing_names = [col.name for col in collections.collections]
                    
                    if collection_name in existing_names:
                        if recreate_if_exists:
                            logger.info(f"Recreating existing collection: {collection_name}")
                            client.delete_collection(collection_name)
                            # Give a small moment for deletion to propagate if recreating
                            time.sleep(0.1) 
                        else:
                            logger.info(f"Collection {collection_name} already exists")
                            return True
                except Exception as e:
                    logger.warning(f"Error checking existing collections: {str(e)}")
                    # If checking collections fails, and we are not forcing recreate, raise
                    if not recreate_if_exists:
                        raise QdrantOperationError(f"Could not verify collection existence: {str(e)}")
            
                # Create collection with proper configuration
                distance_enum = Distance[distance.upper()] if hasattr(Distance, distance.upper()) else Distance.COSINE
                
                result = client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=distance_enum
                    )
                )
                
                logger.info(f"Created collection {collection_name} with {vector_size}D vectors")
                return True
            
            return await loop.run_in_executor(self.executor, _create_collection)
            
        except Exception as e:
            logger.error(f"Failed to create collection {collection_name}: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Collection creation failed: {str(e)}")
    
    async def collection_exists(self, collection_name: str) -> bool:
        """Check if collection exists"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _check_collection():
                client = self._get_client()
                try:
                    collections = client.get_collections()
                    return collection_name in [col.name for col in collections.collections]
                except Exception as e:
                    logger.warning(f"Error checking collection existence: {str(e)}")
                    return False
            
            return await loop.run_in_executor(self.executor, _check_collection)
            
        except Exception as e:
            logger.error(f"Error checking collection existence: {str(e)}", exc_info=True)
            return False
    
    async def get_collection_info(self, collection_name: str) -> Optional[CollectionInfo]:
        """Get collection information"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _get_info():
                client = self._get_client()
                try:
                    info = client.get_collection(collection_name)
                    return CollectionInfo(
                        name=collection_name,
                        status=info.status.value if hasattr(info.status, 'value') else str(info.status),
                        vector_size=info.config.params.vectors.size,
                        distance=info.config.params.vectors.distance.value,
                        points_count=info.points_count
                    )
                except Exception as e:
                    logger.error(f"Error getting collection info: {str(e)}")
                    return None
            
            return await loop.run_in_executor(self.executor, _get_info)
            
        except Exception as e:
            logger.error(f"Failed to get collection info: {str(e)}", exc_info=True)
            return None
    
    async def upsert(
        self,
        collection_name: str,
        points: List[PointStruct],
        batch_size: int = None
    ) -> bool:
        """Upsert points into collection with batching"""
        await self.initialize()
        
        if not points:
            logger.warning("No points to upsert")
            return True
        
        batch_size = batch_size or self.batch_size
        
        try:
            loop = asyncio.get_running_loop()
            
            def _upsert_batch(batch_points):
                client = self._get_client()
                
                # Validate points before upsert
                for point in batch_points:
                    if not isinstance(point.vector, list) or len(point.vector) == 0:
                        raise ValueError(f"Invalid vector for point {point.id}")
                    if not isinstance(point.payload, dict):
                        raise ValueError(f"Invalid payload for point {point.id}")
                
                result = client.upsert(
                    collection_name=collection_name,
                    points=batch_points,
                    wait=True
                )
                return result
            
            # Process in batches
            total_points = len(points)
            logger.info(f"Upserting {total_points} points in batches of {batch_size}")
            
            success_count = 0
            for i in range(0, total_points, batch_size):
                batch = points[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_points + batch_size - 1) // batch_size
                
                logger.debug(f"Processing batch {batch_num}/{total_batches} ({len(batch)} points)")
                
                try:
                    result = await loop.run_in_executor(self.executor, _upsert_batch, batch)
                    if result and result.status == models.UpdateStatus.COMPLETED:
                        success_count += len(batch)
                    else:
                        logger.error(f"Batch {batch_num} upsert failed with status: {result.status if result else 'N/A'}")
                        raise QdrantOperationError(f"Batch {batch_num} upsert failed")
                    
                    # Small delay between batches to avoid overwhelming Qdrant
                    if batch_num < total_batches:
                        await asyncio.sleep(0.01)
                except Exception as e:
                    logger.error(f"Failed to upsert batch {batch_num}: {str(e)}")
                    raise
            
            logger.info(f"Successfully upserted {success_count}/{total_points} points to {collection_name}")
            return success_count == total_points
            
        except Exception as e:
            logger.error(f"Failed to upsert points: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Upsert operation failed: {str(e)}")
    
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        query_filter: Optional[Filter] = None,
        limit: int = 5,
        with_vectors: bool = False,
        with_payload: bool = True,
        score_threshold: Optional[float] = None
    ) -> List[models.ScoredPoint]:
        """Search collection with query vector"""
        await self.initialize()
        
        if not query_vector:
            raise ValueError("Query vector cannot be empty")
        
        try:
            loop = asyncio.get_running_loop()
            
            def _search():
                client = self._get_client()
                
                results = client.search(
                    collection_name=collection_name,
                    query_vector=query_vector,
                    query_filter=query_filter,
                    limit=max(1, min(limit, 1000)),
                    with_vectors=with_vectors,
                    with_payload=with_payload,
                    score_threshold=score_threshold
                )
                return results
            
            results = await loop.run_in_executor(self.executor, _search)
            
            logger.debug(f"Search returned {len(results)} results for collection {collection_name}")
            return results
            
        except ValueError as ve:
            logger.error(f"Validation error in search: {str(ve)}")
            raise
        except Exception as e:
            logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Search operation failed: {str(e)}")
    
    async def scroll(
        self,
        collection_name: str,
        scroll_filter: Optional[Filter] = None,
        limit: int = 10,
        offset: Optional[str] = None,
        with_vectors: bool = False,
        with_payload: bool = True
    ) -> Tuple[List[models.Record], Optional[str]]:
        """Scroll through collection records"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _scroll():
                client = self._get_client()
                records, next_page_offset = client.scroll(
                    collection_name=collection_name,
                    scroll_filter=scroll_filter,
                    limit=max(1, min(limit, 1000)),
                    offset=offset,
                    with_vectors=with_vectors,
                    with_payload=with_payload
                )
                return records, next_page_offset
            
            records, next_offset = await loop.run_in_executor(self.executor, _scroll)
            logger.debug(f"Scroll returned {len(records)} records from {collection_name}")
            return records, next_offset
            
        except Exception as e:
            logger.error(f"Scroll failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Scroll operation failed: {str(e)}")
    
    async def delete(
        self,
        collection_name: str,
        points_selector: models.PointsSelector
    ) -> bool:
        """Delete points from collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _delete():
                client = self._get_client()
                result = client.delete(
                    collection_name=collection_name,
                    points_selector=points_selector,
                    wait=True
                )
                return result
            
            result = await loop.run_in_executor(self.executor, _delete)
            logger.info(f"Delete operation completed for collection {collection_name}")
            return bool(result and result.status == models.UpdateStatus.COMPLETED)
            
        except Exception as e:
            logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Delete operation failed: {str(e)}")
    
    async def count_points(
        self,
        collection_name: str,
        count_filter: Optional[Filter] = None
    ) -> int:
        """Count points in collection"""
        await self.initialize()
        
        try:
            loop = asyncio.get_running_loop()
            
            def _count():
                client = self._get_client()
                result = client.count(
                    collection_name=collection_name,
                    count_filter=count_filter,
                    exact=True
                )
                return result.count
            
            count = await loop.run_in_executor(self.executor, _count)
            return count
            
        except Exception as e:
            logger.error(f"Count failed: {str(e)}", exc_info=True)
            raise QdrantOperationError(f"Count operation failed: {str(e)}")
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        try:
            await self.initialize()
            
            loop = asyncio.get_running_loop()
            
            def _health_check():
                client = self._get_client()
                start_time = time.time()
                
                try:
                    collections = client.get_collections()
                    
                    end_time = time.time()
                    
                    return {
                        "status": "healthy",
                        "url": self.url,
                        "response_time_ms": round((end_time - start_time) * 1000, 2),
                        "collections_count": len(collections.collections),
                        "timestamp": time.time()
                    }
                except Exception as e:
                    return {
                        "status": "unhealthy",
                        "url": self.url,
                        "error": str(e),
                        "timestamp": time.time()
                    }
            
            return await loop.run_in_executor(self.executor, _health_check)
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "url": self.url,
                "error": str(e),
                "timestamp": time.time()
            }
    
    async def close(self):
        """Cleanup resources"""
        if self._closed:
            return
            
        self._closed = True
        
        try:
            # Shutdown executor
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True, timeout=30)
                logger.info("AsyncQdrant executor shutdown completed")
                
            if hasattr(self._local, 'client') and self._local.client is not None:
                try:
                    self._local.client.close()
                    logger.info("Main thread Qdrant client closed.")
                except Exception as e:
                    logger.warning(f"Error closing main thread client: {str(e)}")
        except Exception as e:
            logger.error(f"Error closing AsyncQdrant client: {str(e)}", exc_info=True)
        finally:
            # Reset the singleton instance to allow re-initialization if needed
            AsyncQdrantClient._instance = None 
            logger.info("AsyncQdrant client closed")

# Global instance management
_async_qdrant_client: Optional[AsyncQdrantClient] = None
_async_qdrant_lock = asyncio.Lock()

async def get_async_qdrant_client() -> AsyncQdrantClient:
    """Get or create an async Qdrant client instance"""
    global _async_qdrant_client
    
    async with _async_qdrant_lock:
        if _async_qdrant_client is None or _async_qdrant_client._closed:
            _async_qdrant_client = AsyncQdrantClient()
            await _async_qdrant_client.initialize()
        return _async_qdrant_client

@asynccontextmanager
async def qdrant_client_context():
    """Context manager for Qdrant client"""
    client = await get_async_qdrant_client()
    try:
        yield client
    finally:
        # In a global singleton scenario, we generally don't close the client here
        # as it's meant to be reused across requests.
        # Closing should be handled at application shutdown.
        pass

# Utility functions
def create_user_filter(user_id: str, additional_filters: Optional[Dict] = None) -> Filter:
    """Create a filter for user documents with optional additional filters"""
    must = [
        FieldCondition(
            key="user_id",
            match=MatchValue(value=user_id)
        )
    ]
    
    if additional_filters:
        for key, value in additional_filters.items():
            if isinstance(value, dict):
                # Handle range filters like {"gte": 2020}
                range_params = {}
                for op, op_value in value.items():
                    if op == "gte":
                        range_params["gte"] = op_value
                    elif op == "gt":
                        range_params["gt"] = op_value
                    elif op == "lte":
                        range_params["lte"] = op_value
                    elif op == "lt":
                        range_params["lt"] = op_value
                    elif op == "eq":
                        range_params["gte"] = op_value
                        range_params["lte"] = op_value
                
                if range_params:
                    must.append(FieldCondition(
                        key=key,
                        range=Range(**range_params)
                    ))
            elif isinstance(value, list):
                must.append(FieldCondition(
                    key=key,
                    match=MatchAny(any=value)
                ))
            else:
                must.append(FieldCondition(
                    key=key,
                    match=MatchValue(value=value)
                ))
    
    return Filter(must=must)

def create_document_filter(user_id: str, doc_id: str) -> Filter:
    """Create a filter for specific document"""
    return Filter(
        must=[
            FieldCondition(
                key="user_id",
                match=MatchValue(value=user_id)
            ),
            FieldCondition(
                key="doc_id",
                match=MatchValue(value=doc_id)
            )
        ]
    )

def validate_vector(vector: List[float], expected_dim: int) -> bool:
    """Validate vector dimensions and values"""
    if not isinstance(vector, list):
        return False
    if len(vector) != expected_dim:
        return False
    if not all(isinstance(x, (int, float)) for x in vector):
        return False
    return True
===== ./app/models/agent_model.py =====
# ./app/models/agent_model.py
from datetime import datetime  # Add this import at the top
from enum import Enum
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any


class PersonalityTrait(str, Enum):
    FRIENDLY = "friendly"
    PROFESSIONAL = "professional"
    WITTY = "witty"
    EMPATHETIC = "empathetic"
    ENTHUSIASTIC = "enthusiastic"

class EmotionalAwarenessConfig(BaseModel):
    detect_emotion: bool = True
    adjust_tone: bool = True
    empathy_level: float = Field(0.8, ge=0, le=1)
    max_emotional_response_time: float = Field(1.5, gt=0)

class MemoryConfig(BaseModel):
    context_window: int = Field(10, gt=0)
    long_term_memory: bool = False
    memory_refresh_interval: int = Field(300, gt=0)  # seconds

class AgentPersonality(BaseModel):
    traits: List[PersonalityTrait] = [PersonalityTrait.FRIENDLY]
    base_tone: str = "helpful and knowledgeable"
    emotional_awareness: EmotionalAwarenessConfig = EmotionalAwarenessConfig()
    memory: MemoryConfig = MemoryConfig()

class AgentBase(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: str = Field(..., description="Ollama model to use")
    system_prompt: str = Field(..., description="System prompt defining behavior")
    is_public: bool = Field(False)
    tools: List[str] = Field([])
    personality: AgentPersonality = Field(default_factory=AgentPersonality)
    metadata: Optional[Dict[str, Any]] = Field(None, alias="agent_metadata")

class AgentCreate(AgentBase):
    pass

class AgentUpdate(BaseModel):
    name: Optional[str] = Field(None, min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    is_public: Optional[bool] = None
    tools: Optional[List[str]] = None
    personality: Optional[AgentPersonality] = None
    metadata: Optional[Dict[str, Any]] = Field(None, alias="agent_metadata")

class Agent(AgentBase):
    id: str
    owner_id: str
    rag_enabled: bool = False

class AgentResponse(AgentBase):
    id: str
    owner_id: str
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True
        populate_by_name = True




class AgentPermissions(str, Enum):
    EXECUTE = "execute"
    TRAIN = "train"
    SHARE = "share"
    MANAGE = "manage"

class AgentVisibility(str, Enum):
    PRIVATE = "private"
    TEAM = "team"
    ORGANIZATION = "organization"
    PUBLIC = "public"

class AgentRateLimit(BaseModel):
    requests_per_minute: int = Field(100, gt=0)
    concurrent_executions: int = Field(10, gt=0)

class AgentDeploymentConfig(BaseModel):
    gpu_required: bool = False
    memory_requirements: str = "1Gi"
    timeout_seconds: int = 300

class EnterpriseAgentConfig(BaseModel):
    visibility: AgentVisibility = AgentVisibility.PRIVATE
    rate_limits: AgentRateLimit = Field(default_factory=AgentRateLimit)
    deployment: AgentDeploymentConfig = Field(default_factory=AgentDeploymentConfig)
    allowed_domains: List[str] = []
    permissions: List[AgentPermissions] = [AgentPermissions.EXECUTE]
===== ./app/models/voice_model.py =====
from pydantic import BaseModel
from typing import Optional

class VoiceResponse(BaseModel):
    text: str

class TextToSpeechRequest(BaseModel):
    text: str
    voice_model: Optional[str] = None
===== ./app/models/__init__.py =====
# Empty file to make models a package
===== ./app/models/db_models.py =====
# app/models/db_models.py
from sqlalchemy import Column, String, Boolean, DateTime, ForeignKey, Integer, or_
from sqlalchemy.dialects.postgresql import JSONB, ARRAY, UUID
from sqlalchemy.orm import relationship
from app.database import Base
from datetime import datetime
import uuid

class DBAgent(Base):
    __tablename__ = "agents"
    __table_args__ = {'schema': 'llm'}  # Add schema here
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    owner_id = Column(String, nullable=False, index=True)
    name = Column(String(100), nullable=False)
    description = Column(String(500))
    model = Column(String, nullable=False)
    system_prompt = Column(String, nullable=False)
    is_public = Column(Boolean, default=False, index=True)
    tools = Column(ARRAY(String))
    agent_metadata = Column(JSONB)  
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    documents = relationship("DBDocument", back_populates="agent", cascade="all, delete-orphan")
    training_jobs = relationship("DBTrainingJob", back_populates="agent", cascade="all, delete-orphan")
    conversations = relationship("DBConversation", back_populates="agent", cascade="all, delete-orphan")

class DBDocument(Base):
    __tablename__ = "documents"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    filename = Column(String, nullable=False)
    content_type = Column(String)
    size = Column(Integer)
    qdrant_id = Column(String)
    document_metadata = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="documents")

class DBTrainingJob(Base):
    __tablename__ = "training_jobs"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    status = Column(String, nullable=False)
    config = Column(JSONB)
    result = Column(JSONB)
    error_message = Column(String)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="training_jobs")

class DBConversation(Base):
    __tablename__ = "conversations"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('llm.agents.id'), nullable=False, index=True)
    user_id = Column(String, nullable=False, index=True)
    memory_id = Column(String)
    context = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    agent = relationship("DBAgent", back_populates="conversations")
    messages = relationship("DBMessage", back_populates="conversation", cascade="all, delete-orphan")

class DBMessage(Base):
    __tablename__ = "messages"
    __table_args__ = {'schema': 'llm'}
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    conversation_id = Column(String, ForeignKey('llm.conversations.id'), nullable=False, index=True)
    role = Column(String, nullable=False)
    content = Column(String, nullable=False)
    message_metadata = Column(JSONB)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    conversation = relationship("DBConversation", back_populates="messages")
===== ./app/models/training_model.py =====
from enum import Enum
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, validator
import mimetypes

class TrainingJobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PROCESSING_FILES = "processing_files"
    EXTRACTING_CONTENT = "extracting_content"

class TrainingDataType(str, Enum):
    URL = "url"
    TEXT = "text"
    FILE = "file"
    IMAGE = "image"

class TrainingDataItem(BaseModel):
    type: TrainingDataType
    content: str  # URL, text content, or file path
    metadata: Optional[Dict[str, Any]] = {}
    
    @validator('content')
    def validate_content(cls, v, values):
        data_type = values.get('type')
        if data_type == TrainingDataType.URL:
            # Basic URL validation
            if not v.startswith(('http://', 'https://')):
                raise ValueError('URL must start with http:// or https://')
        elif data_type == TrainingDataType.TEXT:
            if len(v.strip()) < 10:
                raise ValueError('Text content must be at least 10 characters')
        return v

class TrainingJobCreate(BaseModel):
    agent_id: str
    # Backward compatibility - keep data_urls for existing API
    data_urls: Optional[List[str]] = None
    # New enhanced data structure
    training_data: Optional[List[TrainingDataItem]] = None
    # Direct text input
    text_data: Optional[List[str]] = None
    # Training configuration
    config: Optional[Dict[str, Any]] = {}
    
    @validator('training_data', 'data_urls', 'text_data')
    def validate_data_sources(cls, v, values):
        # Ensure at least one data source is provided
        data_urls = values.get('data_urls')
        text_data = values.get('text_data')
        
        if not any([v, data_urls, text_data]):
            raise ValueError('At least one data source must be provided')
        return v

class FileUploadInfo(BaseModel):
    filename: str
    content_type: str
    size: int
    file_id: str

class TrainingJobResponse(BaseModel):
    id: str
    user_id: str
    agent_id: str
    data_urls: List[str] = []
    training_data: List[TrainingDataItem] = []
    text_data: List[str] = []
    uploaded_files: List[FileUploadInfo] = []
    status: TrainingJobStatus
    created_at: datetime
    updated_at: datetime
    progress: int
    current_step: Optional[str] = None
    total_steps: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0

class TrainingProgress(BaseModel):
    job_id: str
    status: TrainingJobStatus
    progress: int
    current_step: Optional[str] = None
    processed_items: int = 0
    total_items: int = 0
    message: Optional[str] = None

# Supported file types
SUPPORTED_EXTENSIONS = {
    # Text files
    '.txt', '.md', '.rtf', '.csv', '.json', '.xml', '.html', '.htm',
    # Documents
    '.pdf', '.doc', '.docx', '.odt', '.pages',
    # Images
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg',
    # Audio (for future transcription)
    '.mp3', '.wav', '.m4a', '.flac', '.ogg',
    # Video (for future transcription)
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm'
}

def get_file_type(filename: str) -> TrainingDataType:
    """Determine the training data type based on file extension"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    ext = f'.{ext}'
    
    if ext in {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}:
        return TrainingDataType.IMAGE
    else:
        return TrainingDataType.FILE

def is_supported_file(filename: str) -> bool:
    """Check if file extension is supported"""
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    return f'.{ext}' in SUPPORTED_EXTENSIONS
===== ./app/models/response_schema.py =====
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = None
    system_prompt: Optional[str] = None
    options: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    message: str
    model: str
    context: List[Any]
    tokens_used: int



# response_schema.py
class RAGQueryRequest(BaseModel):
    query: str
    max_results: int = Field(5, gt=0, le=20)
    rewrite_query: bool = Field(True, description="Enable query rewriting")
    use_reranking: bool = Field(True, description="Enable cross-encoder re-ranking")
    filters: Optional[Dict[str, Any]] = Field(None, description="Metadata filters for retrieval")
    hybrid_search: bool = Field(True, description="Enable hybrid sparse+dense search")

class RAGResponse(BaseModel):
    answer: str
    documents: List[str]
    sources: List[str]
    context: List[str]
    debug: Optional[Dict[str, Any]] = Field(None, description="Debug information about the search process")
    search_method: Optional[str] = Field(None, description="Search method used")
    processed_query: Optional[str] = Field(None, description="Processed query after rewriting")

class DocumentResponse(BaseModel):
    document_id: str
    filename: str
    status: str = "success"

class HybridSearchConfig(BaseModel):
    sparse_weight: float = Field(0.4, ge=0, le=1)
    dense_weight: float = Field(0.6, ge=0, le=1)
    enable_hybrid: bool = True
===== ./app/scripts/fix_dimension_mismatch.py =====
# scripts/fix_dimension_mismatch.py
"""
Script to fix ChromaDB embedding dimension mismatch issues.
Run this script to reset your ChromaDB collection when you encounter dimension errors.
"""

import asyncio
import logging
import sys
import os
from pathlib import Path

# Add the parent directory to the path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.rag_service import RAGService
from app.services.llm_service import LLMService
from app.config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def check_embedding_model():
    """Check what embedding dimension your current model produces"""
    try:
        ollama = LLMService()
        test_text = "This is a test sentence to check embedding dimensions."
        
        logger.info("Testing embedding model...")
        embedding = await ollama.create_embedding(test_text)
        
        if embedding:
            logger.info(f"✅ Embedding model working")
            logger.info(f"✅ Current embedding dimension: {len(embedding)}")
            logger.info(f"✅ Model: {settings.EMBEDDING_MODEL}")
            return len(embedding)
        else:
            logger.error("❌ Failed to get embedding from model")
            return None
            
    except Exception as e:
        logger.error(f"❌ Error testing embedding model: {str(e)}")
        return None

async def reset_chroma_collection():
    """Reset the ChromaDB collection to fix dimension issues"""
    try:
        logger.info("Initializing RAG service...")
        rag_service = RAGService()
        
        logger.info("Resetting ChromaDB collection...")
        await rag_service.reset_collection()
        
        logger.info("✅ Collection reset successfully!")
        
        # Clean up
        await rag_service.close()
        
    except Exception as e:
        logger.error(f"❌ Error resetting collection: {str(e)}")
        raise

async def verify_setup():
    """Verify that everything is working correctly"""
    try:
        logger.info("Verifying setup...")
        
        # Check embedding model
        embedding_dim = await check_embedding_model()
        if not embedding_dim:
            return False
            
        # Test RAG service initialization
        rag_service = RAGService()
        await rag_service.initialize()
        
        logger.info("✅ Setup verification completed successfully!")
        
        # Clean up
        await rag_service.close()
        return True
        
    except Exception as e:
        logger.error(f"❌ Setup verification failed: {str(e)}")
        return False

async def main():
    """Main function to fix dimension mismatch issues"""
    print("🔧 ChromaDB Dimension Fix Tool")
    print("=" * 40)
    
    # Step 1: Check current embedding model
    print("\n📊 Step 1: Checking embedding model...")
    embedding_dim = await check_embedding_model()
    
    if not embedding_dim:
        print("❌ Cannot proceed without working embedding model")
        print("Please check your Ollama installation and model configuration")
        return
    
    # Step 2: Show current configuration
    print(f"\n⚙️  Current Configuration:")
    print(f"   - Embedding Model: {settings.EMBEDDING_MODEL}")
    print(f"   - Embedding Dimension: {embedding_dim}")
    print(f"   - ChromaDB Path: {settings.CHROMA_PATH}")
    print(f"   - Collection Name: {settings.CHROMA_COLLECTION_NAME}")
    
    # Step 3: Ask user if they want to reset
    print(f"\n🔄 The embedding dimension mismatch error occurs when:")
    print(f"   - Your collection expects one dimension (e.g., 384)")
    print(f"   - Your embedding model produces another (e.g., 1536)")
    print(f"   - Current model produces: {embedding_dim} dimensions")
    
    response = input(f"\n❓ Do you want to reset the ChromaDB collection? (y/N): ").lower()
    
    if response in ['y', 'yes']:
        print(f"\n🗑️  Step 3: Resetting ChromaDB collection...")
        await reset_chroma_collection()
        
        print(f"\n✅ Step 4: Verifying setup...")
        success = await verify_setup()
        
        if success:
            print(f"\n🎉 All done! Your ChromaDB is now ready to use.")
            print(f"   - Collection reset successfully")
            print(f"   - Embedding dimension: {embedding_dim}")
            print(f"   - You can now upload documents without dimension errors")
        else:
            print(f"\n❌ Verification failed. Please check the logs for errors.")
    else:
        print(f"\n📝 To fix this manually:")
        print(f"   1. Delete the ChromaDB directory: {settings.CHROMA_PATH}")
        print(f"   2. Or use a different embedding model with 384 dimensions")
        print(f"   3. Or modify your code to handle {embedding_dim} dimensions")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n\n👋 Exiting...")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        sys.exit(1)
===== ./app/main.py =====
# ./app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.routers import (
    agents, auth, chat, rag,
    training, voice, agent_interaction,
    health, execute
)
from app.middleware import LoggingMiddleware, RateLimiterMiddleware
import asyncio
import logging
from app.utils.health_check import router as health_router

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(LoggingMiddleware)

if settings.RATE_LIMITING_ENABLED:
    app.add_middleware(RateLimiterMiddleware)

# Routers
app.include_router(auth.router, prefix="/api/v1")
app.include_router(agents.router, prefix="/api/v1")
app.include_router(rag.router, prefix="/api/v1")
app.include_router(execute.router, prefix="/api/v1")
app.include_router(training.router, prefix="/api/v1/training")
app.include_router(health_router)
app.include_router(chat.router, prefix="/api/v1")
app.include_router(voice.router, prefix="/api/v1/voice")
app.include_router(agent_interaction.router, prefix="/api/v1")




if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
===== ./app/services/agent_interaction_service.py =====
# ./app/services/agent_interaction_service.py
import logging
from typing import Dict, Optional, List, AsyncIterator, Any  # Added Any here
from fastapi import UploadFile
from app.services.llm_service import OllamaService
from app.services.voice_service import VoiceService
from app.services.rag_service import RAGService
from app.models.agent_model import AgentPersonality
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio

logger = logging.getLogger(__name__)

class EmotionalAnalyzer:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        
    async def analyze_emotion(self, text: str) -> Dict[str, float]:
        """Analyze emotional tone from text"""
        prompt = """Analyze the emotional tone of this text. Respond ONLY with a JSON object containing 
        emotion scores between 0-1 for: happiness, sadness, anger, fear, surprise, neutral.

        Text: {text}""".format(text=text)
        
        try:
            response = await self.ollama.generate(
                prompt=prompt,
                model="deepseek-r1:1.5b",
                options={"temperature": 0.7}
            )
            
            # Parse the JSON response
            import json
            return json.loads(response["response"])
        except Exception as e:
            logger.error(f"Emotion analysis failed: {str(e)}")
            return {
                "happiness": 0.5,
                "sadness": 0.0,
                "anger": 0.0,
                "fear": 0.0,
                "surprise": 0.0,
                "neutral": 0.5
            }

class AgentInteractionService:
    def __init__(self):
        self.ollama = OllamaService()
        self.voice = VoiceService()
        self.rag = RAGService()
        self.emotion_analyzer = EmotionalAnalyzer(self.ollama)
        self.conversation_memory = {}
        
    async def initialize(self):
        """Initialize all required services"""
        await self.rag.initialize()
        
    async def process_input(
        self,
        agent_id: str,
        user_id: str,
        input_text: Optional[str] = None,
        audio_file: Optional[UploadFile] = None,
        db: Optional[AsyncSession] = None
    ) -> Dict[str, Any]:
        """Process user input (text or voice) and generate response"""
        # Convert audio to text if provided
        if audio_file:
            input_text = await self.voice.speech_to_text(audio_file)
            
        if not input_text:
            raise ValueError("No input text provided")
            
        # Get or create conversation memory
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = {
                "history": [],
                "last_updated": datetime.now(),
                "emotional_state": {}
            }
            
        # Analyze emotion
        emotion_scores = await self.emotion_analyzer.analyze_emotion(input_text)
        self.conversation_memory[user_id]["emotional_state"] = emotion_scores
        
        # Retrieve relevant context
        rag_results = await self.rag.query(
            db=db,
            user_id=user_id,
            query=input_text
        )
        
        # Generate response considering emotion and context
        response = await self._generate_response(
            agent_id=agent_id,
            user_id=user_id,
            input_text=input_text,
            emotion_scores=emotion_scores,
            context=rag_results.get("documents", [])
        )
        
        # Update conversation history
        self.conversation_memory[user_id]["history"].append({
            "input": input_text,
            "response": response,
            "timestamp": datetime.now()
        })
        
        return {
            "text_response": response,
            "emotional_state": emotion_scores,
            "context_used": rag_results.get("sources", [])
        }
        
    async def _generate_response(
        self,
        agent_id: str,
        user_id: str,
        input_text: str,
        emotion_scores: Dict[str, float],
        context: List[str]
    ) -> str:
        """Generate response considering emotional state and context"""
        # Get dominant emotion
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        # Build system prompt based on emotion and context
        system_prompt = self._build_system_prompt(
            agent_id=agent_id,
            dominant_emotion=dominant_emotion,
            context=context
        )
        
        # Generate response
        response = await self.ollama.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": input_text}
            ]
        )
        
        return response.get("message", {}).get("content", "")
        
    def _build_system_prompt(
        self,
        agent_id: str,
        dominant_emotion: str,
        context: List[str]
    ) -> str:
        """Build dynamic system prompt based on context and emotion"""
        # TODO: Fetch agent personality from database
        personality = AgentPersonality()  # Default personality
        
        # Base prompt
        prompt_lines = [
            f"You are {agent_id}, a highly intelligent AI assistant.",
            f"Your personality traits: {', '.join([t.value for t in personality.traits])}.",
            f"Current user emotional state: {dominant_emotion}. Adjust your tone accordingly.",
            "",
            "Context from knowledge base:",
            "\n".join(context) if context else "No relevant context found",
            "",
            "Guidelines:",
            f"- Be {personality.base_tone}",
            "- Acknowledge user's emotional state if strong",
            "- Use context when relevant but don't force it",
            "- Keep responses concise but thorough when needed",
            "- Maintain natural conversation flow"
        ]
        
        # Emotion-specific adjustments
        if dominant_emotion == "sadness":
            prompt_lines.append("- Show extra empathy and support")
        elif dominant_emotion == "anger":
            prompt_lines.append("- Remain calm and solution-focused")
        elif dominant_emotion == "happiness":
            prompt_lines.append("- Match the positive energy but stay professional")
            
        return "\n".join(prompt_lines)
        
    async def text_to_speech(
        self,
        text: str,
        emotional_state: Optional[Dict[str, float]] = None
    ) -> bytes:
        """Convert text to speech with emotional inflection"""
        # TODO: Adjust TTS parameters based on emotional state
        return await self.voice.text_to_speech(text)
        
    async def get_conversation_history(self, user_id: str) -> List[Dict[str, Any]]:
        """Get conversation history for a user"""
        return self.conversation_memory.get(user_id, {}).get("history", [])
        
    async def clear_memory(self, user_id: str):
        """Clear conversation memory for a user"""
        if user_id in self.conversation_memory:
            del self.conversation_memory[user_id]
===== ./app/services/conversation_service.py =====
# app/services/conversation_service.py
import logging
from typing import Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, delete
from app.models.db_models import DBConversation, DBMessage
from datetime import datetime
import uuid

logger = logging.getLogger(__name__)

class ConversationService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.memory_cache = {}
    
    async def get_conversation(self, agent_id: str, user_id: str) -> DBConversation:
        cache_key = f"{agent_id}_{user_id}"
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
            
        result = await self.db.execute(
            select(DBConversation).where(
                DBConversation.agent_id == agent_id,
                DBConversation.user_id == user_id
            ).order_by(DBConversation.updated_at.desc()).limit(1)
        )
        conversation = result.scalars().first()
        
        if not conversation:
            conversation = DBConversation(
                agent_id=agent_id,
                user_id=user_id,
                memory_id=str(uuid.uuid4()),
                context={"messages": []}
            )
            self.db.add(conversation)
            await self.db.commit()
            await self.db.refresh(conversation)
        
        self.memory_cache[cache_key] = conversation
        return conversation
    
    async def add_message(
        self,
        conversation: DBConversation,
        role: str,
        content: str,
        metadata: Optional[Dict] = None
    ) -> DBMessage:
        message = DBMessage(
            conversation_id=conversation.id,
            role=role,
            content=content,
            metadata=metadata or {}
        )
        self.db.add(message)
        
        # Update context (simplified - in production you'd want summarization)
        messages = conversation.context.get("messages", [])
        messages.append({"role": role, "content": content})
        if len(messages) > 10:
            messages = messages[-10:]
        
        conversation.context = {"messages": messages}
        conversation.updated_at = datetime.utcnow()
        
        await self.db.commit()
        await self.db.refresh(message)
        return message
    
    async def clear_conversation(self, agent_id: str, user_id: str) -> bool:
        try:
            # Delete messages
            await self.db.execute(
                delete(DBMessage).where(
                    DBMessage.conversation.has(
                        DBConversation.agent_id == agent_id,
                        DBConversation.user_id == user_id
                    )
                )
            )
            
            # Delete conversation
            await self.db.execute(
                delete(DBConversation).where(
                    DBConversation.agent_id == agent_id,
                    DBConversation.user_id == user_id
                )
            )
            
            await self.db.commit()
            
            # Clear cache
            cache_key = f"{agent_id}_{user_id}"
            if cache_key in self.memory_cache:
                del self.memory_cache[cache_key]
                
            return True
        except Exception as e:
            logger.error(f"Error clearing conversation: {str(e)}")
            await self.db.rollback()
            return False
===== ./app/services/training_service.py =====
import os
import asyncio
import aiofiles
import uuid
import logging
import requests
from typing import List, Optional, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import tempfile
import shutil
from pathlib import Path

from app.models.training_model import (
    TrainingJobResponse, 
    TrainingJobStatus,
    TrainingDataItem,
    TrainingDataType,
    TrainingProgress,
    FileUploadInfo
)
from app.utils.file_processing import (
    process_document,
    process_image,
    extract_text_from_url,
    chunk_text
)

logger = logging.getLogger(__name__)

class TrainingService:
    def __init__(self):
        self.jobs = {}
        self.executor = ThreadPoolExecutor(max_workers=10)
        # Create temporary directory for file uploads
        self.temp_dir = Path(tempfile.gettempdir()) / "training_uploads"  
        self.temp_dir.mkdir(exist_ok=True)
        
        # File storage with user isolation
        self.user_files = {}  # user_id -> {file_id: file_path}

    async def create_job(
        self,
        user_id: str,
        agent_id: str,
        data_urls: List[str] = None,
        training_data: List[TrainingDataItem] = None,
        text_data: List[str] = None,
        uploaded_files: List[FileUploadInfo] = None,
        config: Dict[str, Any] = None
    ) -> TrainingJobResponse:
        """Create a new enhanced training job"""
        job_id = str(uuid.uuid4())
        now = datetime.utcnow()
        
        # Convert legacy data_urls to training_data format for backward compatibility
        enhanced_training_data = training_data or []
        if data_urls:
            for url in data_urls:
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.URL,
                        content=url,
                        metadata={"source": "legacy_data_urls"}
                    )
                )
        
        # Add text data as training items
        if text_data:
            for i, text in enumerate(text_data):
                enhanced_training_data.append(
                    TrainingDataItem(
                        type=TrainingDataType.TEXT,
                        content=text,
                        metadata={"index": i, "source": "text_input"}
                    )
                )
        
        job = TrainingJobResponse(
            id=job_id,
            user_id=user_id,
            agent_id=agent_id,
            data_urls=data_urls or [],
            training_data=enhanced_training_data,
            text_data=text_data or [],
            uploaded_files=uploaded_files or [],
            status=TrainingJobStatus.PENDING,
            created_at=now,
            updated_at=now,
            progress=0,
            current_step="Initializing",
            total_steps=self._calculate_total_steps(enhanced_training_data),
            result=None,
            processed_items=0,
            total_items=len(enhanced_training_data)
        )
        
        self.jobs[job_id] = job
        logger.info(f"Created job {job_id} with {len(enhanced_training_data)} training items")
        return job

    def _calculate_total_steps(self, training_data: List[TrainingDataItem]) -> int:
        """Calculate total processing steps based on data types"""
        steps = 1  # Initialization
        
        for item in training_data:
            if item.type == TrainingDataType.URL:
                steps += 2  # Download + Process
            elif item.type == TrainingDataType.FILE:
                steps += 2  # Read + Process
            elif item.type == TrainingDataType.IMAGE:
                steps += 2  # Read + OCR/Analysis
            else:  # TEXT
                steps += 1  # Process
        
        steps += 2  # Finalization + Model training
        return steps

    async def save_uploaded_file(
        self,
        user_id: str,
        file_id: str,
        filename: str,
        content: bytes
    ) -> str:
        """Save uploaded file to temporary storage"""
        try:
            # Create user-specific directory
            user_dir = self.temp_dir / user_id
            user_dir.mkdir(exist_ok=True)
            
            # Save file with unique name
            file_path = user_dir / f"{file_id}_{filename}"
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # Track file for cleanup
            if user_id not in self.user_files:
                self.user_files[user_id] = {}
            self.user_files[user_id][file_id] = str(file_path)
            
            logger.info(f"Saved file {filename} for user {user_id} as {file_id}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"Error saving file: {str(e)}")
            raise

    async def list_jobs(self, user_id: str, agent_id: str) -> List[TrainingJobResponse]:
        """List training jobs for a user and agent"""
        user_jobs = []
        for job in self.jobs.values():
            if job.user_id == user_id and job.agent_id == agent_id:
                user_jobs.append(job)
        
        # Sort by creation date, newest first
        user_jobs.sort(key=lambda x: x.created_at, reverse=True)
        return user_jobs

    async def get_job(self, user_id: str, job_id: str) -> Optional[TrainingJobResponse]:
        """Get a specific job for a user"""
        job = self.jobs.get(job_id)
        if job and job.user_id == user_id:
            return job
        return None

    async def get_progress(self, user_id: str, job_id: str) -> Optional[TrainingProgress]:
        """Get training progress for a job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return None
        
        return TrainingProgress(
            job_id=job.id,
            status=job.status,
            progress=job.progress,
            current_step=job.current_step,
            processed_items=job.processed_items,
            total_items=job.total_items,
            message=job.error_message if job.status == TrainingJobStatus.FAILED else None
        )

    async def cancel_job(self, user_id: str, job_id: str) -> bool:
        """Cancel a training job"""
        job = await self.get_job(user_id, job_id)
        if not job:
            return False
        
        if job.status in [TrainingJobStatus.PENDING, TrainingJobStatus.PROCESSING_FILES, 
                         TrainingJobStatus.EXTRACTING_CONTENT, TrainingJobStatus.RUNNING]:
            job.status = TrainingJobStatus.FAILED
            job.error_message = "Job cancelled by user"
            job.updated_at = datetime.utcnow()
            logger.info(f"Cancelled job {job_id} for user {user_id}")
            return True
        
        return False

    async def run_training(self, job_id: str, user_id: str):
        """Run enhanced training process in background"""
        if job_id not in self.jobs:
            logger.error(f"Job {job_id} not found")
            return
        
        job = self.jobs[job_id]
        if job.user_id != user_id:
            logger.error(f"User {user_id} not authorized for job {job_id}")
            return
        
        try:
            await self._execute_training_pipeline(job)
        except Exception as e:
            logger.error(f"Training failed for job {job_id}: {str(e)}")
            job.status = TrainingJobStatus.FAILED
            job.error_message = str(e)
            job.updated_at = datetime.utcnow()
        finally:
            # Cleanup uploaded files
            await self._cleanup_user_files(user_id)

    async def _cleanup_user_files(self, user_id: str):
        """Clean up uploaded files for a user"""
        try:
            if user_id in self.user_files:
                for file_id, file_path in self.user_files[user_id].items():
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                            logger.info(f"Cleaned up file: {file_path}")
                    except Exception as e:
                        logger.error(f"Error cleaning up file {file_path}: {str(e)}")
                
                # Remove user directory if empty
                user_dir = self.temp_dir / user_id
                if user_dir.exists() and not any(user_dir.iterdir()):
                    user_dir.rmdir()
                
                # Clear from tracking
                del self.user_files[user_id]
                
        except Exception as e:
            logger.error(f"Error during cleanup for user {user_id}: {str(e)}")

    async def _execute_training_pipeline(self, job: TrainingJobResponse):
        """Execute the complete training pipeline"""
        logger.info(f"Starting training pipeline for job {job.id}")
        
        job.status = TrainingJobStatus.PROCESSING_FILES
        job.current_step = "Processing input data"
        job.updated_at = datetime.utcnow()
        
        processed_content = []
        current_step = 0
        
        # Process each training data item
        for i, item in enumerate(job.training_data):
            try:
                job.current_step = f"Processing item {i+1}/{len(job.training_data)}: {item.type.value}"
                job.processed_items = i
                job.progress = int((current_step / job.total_steps) * 100)
                job.updated_at = datetime.utcnow()
                
                content = await self._process_training_item(item)
                if content:
                    processed_content.extend(content)
                
                current_step += 2 if item.type != TrainingDataType.TEXT else 1
                
            except Exception as e:
                logger.error(f"Error processing item {i}: {str(e)}")
                # Continue with other items instead of failing completely
                continue
        
        # Update progress
        job.status = TrainingJobStatus.EXTRACTING_CONTENT
        job.current_step = "Extracting and chunking content"
        job.progress = 80
        job.updated_at = datetime.utcnow()
        
        # Chunk all processed content
        all_chunks = []
        for content in processed_content:
            chunks = chunk_text(content)
            all_chunks.extend(chunks)
        
        current_step += 1
        
        # Simulate model training
        job.status = TrainingJobStatus.RUNNING
        job.current_step = "Training model"
        job.progress = 90
        job.updated_at = datetime.utcnow()
        
        # Simulate training time based on content size
        training_time = min(len(all_chunks) * 0.1, 10)  # Max 10 seconds
        await asyncio.sleep(training_time)
        
        # Complete the job
        job.status = TrainingJobStatus.COMPLETED
        job.current_step = "Training completed"
        job.progress = 100
        job.processed_items = len(job.training_data)
        job.result = {
            "total_content_items": len(processed_content),
            "total_chunks": len(all_chunks),
            "average_chunk_size": sum(len(chunk) for chunk in all_chunks) / len(all_chunks) if all_chunks else 0,
            "training_time_seconds": training_time,
            "data_types_processed": list(set(item.type.value for item in job.training_data)),
            "accuracy": 0.95,  # Simulated
            "loss": 0.15       # Simulated
        }
        job.updated_at = datetime.utcnow()
        
        logger.info(f"Training completed for job {job.id} with {len(all_chunks)} chunks")

    async def _process_training_item(self, item: TrainingDataItem) -> List[str]:
        """Process a single training data item"""
        try:
            if item.type == TrainingDataType.URL:
                return await self._process_url(item.content)
            elif item.type == TrainingDataType.FILE:
                return await self._process_file(item.content, item.metadata)
            elif item.type == TrainingDataType.IMAGE:
                return await self._process_image_file(item.content, item.metadata)
            elif item.type == TrainingDataType.TEXT:
                return [item.content]
            else:
                logger.warning(f"Unknown data type: {item.type}")
                return []
                
        except Exception as e:
            logger.error(f"Error processing {item.type} item: {str(e)}")
            return []

    async def _process_url(self, url: str) -> List[str]:
        """Process content from URL"""
        try:
            # Run in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor,
                extract_text_from_url,
                url
            )
            return [content] if content else []
        except Exception as e:
            logger.error(f"Error processing URL {url}: {str(e)}")
            return []

    async def _process_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process uploaded file"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read file content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process document
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_document,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {str(e)}")
            return []

    async def _process_image_file(self, file_path: str, metadata: Dict[str, Any]) -> List[str]:
        """Process image file with OCR"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"Image file not found: {file_path}")
                return []
            
            filename = metadata.get('filename', 'unknown')
            
            # Read image content
            async with aiofiles.open(file_path, 'rb') as f:
                content = await f.read()
            
            # Process image
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                self.executor,
                process_image,
                filename,
                content
            )
            
            return [text] if text else []
            
        except Exception as e:
            logger.error(f"Error processing image {file_path}: {str(e)}")
            return []
===== ./app/services/rag_service.py =====
# app/services/rag_service.py
import logging
import os
import asyncio
import uuid
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from app.config import settings
from app.utils.file_processing import process_document
from app.services.llm_service import OllamaService
from sqlalchemy.ext.asyncio import AsyncSession
from app.utils.qdrant_async import get_async_qdrant_client
from qdrant_client.http import models as qdrant_models
from qdrant_client.http.models import Distance, VectorParams
from sentence_transformers import CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import _stop_words
import re
from datetime import datetime
from collections import defaultdict
import hashlib

logger = logging.getLogger(__name__)

class OllamaEmbeddingFunction:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self._embedding_dim = None
        self._model_checked = False
    
    async def _verify_embedding_model(self):
        """Verify the embedding model produces consistent dimensions"""
        if self._model_checked:
            return
            
        try:
            # Test with multiple inputs to verify consistency
            test_texts = ["test", "another test", "embedding verification"]
            embeddings = []
            
            for text in test_texts:
                embedding = await self.ollama.create_embedding(text)
                if not embedding:
                    raise ValueError(f"Empty embedding returned for text: {text}")
                embeddings.append(embedding)
            
            # Check all embeddings have same dimension
            dims = {len(e) for e in embeddings}
            if len(dims) != 1:
                raise ValueError(f"Inconsistent embedding dimensions: {dims}")
                
            self._embedding_dim = dims.pop()
            logger.info(f"Verified embedding dimension: {self._embedding_dim}")
            self._model_checked = True
            
        except Exception as e:
            logger.error(f"Embedding model verification failed: {str(e)}")
            raise

    async def _get_embedding_dimension(self) -> int:
        """Get the embedding dimension from the model"""
        if self._embedding_dim is None:
            await self._verify_embedding_model()
        return self._embedding_dim
    
    async def generate_embeddings(self, input: List[str]) -> List[List[float]]:
        """Generate embeddings with dimension validation"""
        if not input:
            return []
            
        embedding_dim = await self._get_embedding_dimension()
        embeddings = []
        
        for text in input:
            try:
                logger.debug(f"Generating embedding for text: {text[:100]}...")
                embedding = await self.ollama.create_embedding(text)
                
                if not embedding:
                    logger.warning(f"Empty embedding for text: {text[:50]}...")
                    embeddings.append([0.0] * embedding_dim)
                    continue
                    
                if hasattr(embedding, 'tolist'):
                    embedding = embedding.tolist()
                    
                # Validate dimension
                if len(embedding) != embedding_dim:
                    logger.warning(f"Embedding dimension mismatch: expected {embedding_dim}, got {len(embedding)}")
                    if len(embedding) > embedding_dim:
                        embedding = embedding[:embedding_dim]
                    else:
                        embedding.extend([0.0] * (embedding_dim - len(embedding)))
                
                # Normalize
                embedding_array = np.array(embedding)
                norm = np.linalg.norm(embedding_array)
                if norm > 0:
                    embedding = (embedding_array / norm).tolist()
                else:
                    logger.warning("Zero norm embedding detected")
                    
                embeddings.append(embedding)
                
            except Exception as e:
                logger.error(f"Embedding error: {str(e)}", exc_info=True)
                embeddings.append([0.0] * embedding_dim)
                
        return embeddings

class HybridRetriever:
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words=list(_stop_words.ENGLISH_STOP_WORDS),
            ngram_range=(1, 2)
        )
        self.tfidf_matrix = None
        self.cross_encoder = None
        self._is_tfidf_trained = False
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize cross-encoder if enabled"""
        if settings.RERANKING_ENABLED:
            try:
                self.cross_encoder = CrossEncoder(settings.RERANKING_MODEL)
                logger.info(f"Initialized cross-encoder: {settings.RERANKING_MODEL}")
            except Exception as e:
                logger.error(f"Failed to initialize cross-encoder: {str(e)}")
                self.cross_encoder = None
    
    async def train_tfidf(self, corpus: List[str]):
        """Train TF-IDF vectorizer on a corpus of documents"""
        if not corpus:
            logger.warning("No documents provided for TF-IDF training")
            return
            
        try:
            logger.info(f"Training TF-IDF on {len(corpus)} documents...")
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
            self._is_tfidf_trained = True
            logger.info(f"TF-IDF vectorizer trained with {len(self.tfidf_vectorizer.get_feature_names_out())} features")
        except Exception as e:
            logger.error(f"Failed to train TF-IDF vectorizer: {str(e)}")
            self._is_tfidf_trained = False
    
    def get_sparse_embedding(self, text: str) -> Optional[List[float]]:
        """Generate sparse TF-IDF embedding with dimension validation"""
        if not self._is_tfidf_trained:
            return None
            
        try:
            vector = self.tfidf_vectorizer.transform([text])
            return vector.toarray()[0].tolist()
        except Exception as e:
            logger.error(f"Failed to generate sparse embedding: {str(e)}")
            return None

    async def rerank_results(
        self,
        query: str,
        documents: List[str],
        scores: List[float],
        top_k: int = 5
    ) -> Tuple[List[str], List[float]]:
        """Re-rank results using cross-encoder"""
        if not self.cross_encoder or not documents:
            return documents, scores
            
        if len(documents) != len(scores):
            logger.error(f"Mismatched documents ({len(documents)}) and scores ({len(scores)})")
            return documents, scores
            
        try:
            # Create query-document pairs for cross-encoder
            pairs = [(query, doc) for doc in documents]
            
            # Get scores from cross-encoder
            rerank_scores = self.cross_encoder.predict(pairs)
            
            # Combine with original scores
            combined_scores = [
                (settings.HYBRID_DENSE_WEIGHT * score) + 
                (settings.HYBRID_SPARSE_WEIGHT * rerank_score)
                for score, rerank_score in zip(scores, rerank_scores)
            ]
            
            # Sort documents by combined scores
            sorted_indices = np.argsort(combined_scores)[::-1]
            sorted_docs = [documents[i] for i in sorted_indices[:top_k]]
            sorted_scores = [combined_scores[i] for i in sorted_indices[:top_k]]
            
            return sorted_docs, sorted_scores
        except Exception as e:
            logger.error(f"Re-ranking failed: {str(e)}")
            return documents, scores

class QueryRewriter:
    @staticmethod
    async def expand_query(query: str, ollama_service: OllamaService) -> str:
        """Expand query using LLM to generate synonyms and related terms"""
        if not settings.QUERY_REWRITING_ENABLED:
            return query
            
        try:
            prompt = f"""Expand this search query with synonyms and related terms. 
Return only a comma-separated list of terms without any additional text.

Original query: {query}

Expanded terms:"""
            
            response = await ollama_service.chat(
                messages=[{"role": "user", "content": prompt}],
                model=settings.DEFAULT_OLLAMA_MODEL,
                temperature=0.7,
                max_tokens=100
            )
            
            expanded_terms = response.get("message", {}).get("content", "").strip()
            if expanded_terms:
                expanded_terms = re.sub(r'[^a-zA-Z0-9,\s]', '', expanded_terms)
                expanded_terms = ', '.join([term.strip() for term in expanded_terms.split(',') if term.strip()])
                return f"{query}, {expanded_terms}"
            return query
        except Exception as e:
            logger.error(f"Query expansion failed: {str(e)}")
            return query
    
    @staticmethod
    async def rewrite_query(query: str, ollama_service: OllamaService) -> str:
        """Rewrite query for better retrieval using LLM"""
        if not settings.QUERY_REWRITING_ENABLED:
            return query
            
        try:
            prompt = f"""Rewrite this search query to be more effective for document retrieval. 
Keep the original meaning but optimize for semantic search. 
Return only the rewritten query without any additional text.

Original query: {query}

Rewritten query:"""
            
            response = await ollama_service.chat(
                messages=[{"role": "user", "content": prompt}],
                model=settings.DEFAULT_OLLAMA_MODEL,
                temperature=0.3,
                max_tokens=100
            )
            
            rewritten = response.get("message", {}).get("content", "").strip()
            return rewritten if rewritten else query
        except Exception as e:
            logger.error(f"Query rewriting failed: {str(e)}")
            return query

class RAGService:
    def __init__(self):
        self.ollama = OllamaService()
        self.embedding_fn = OllamaEmbeddingFunction(self.ollama)
        self.hybrid_retriever = HybridRetriever(self.ollama)
        self.query_rewriter = QueryRewriter()
        self.collection_name = settings.QDRANT_COLLECTION_NAME
        self.client = None
        self._embedding_dim = None
        self._initialized = False
        self._lock = asyncio.Lock()

    async def initialize(self):
        """Initialize the Qdrant client and collection with proper dimension handling"""
        async with self._lock:
            if self._initialized:
                return

            try:
                self.client = await get_async_qdrant_client()
                
                # Verify embedding model first
                await self.embedding_fn._verify_embedding_model()
                self._embedding_dim = await self.embedding_fn._get_embedding_dimension()

                # Check if collection exists and has correct dimensions
                collection_exists = await self.client.collection_exists(self.collection_name)
                needs_recreation = False
                
                if collection_exists:
                    collection_info = await self.client.get_collection_info(self.collection_name)
                    if collection_info:
                        if collection_info.vector_size != self._embedding_dim:
                            logger.warning(
                                f"Collection dimension mismatch: "
                                f"expected {self._embedding_dim}, got {collection_info.vector_size}. "
                                "Recreating collection..."
                            )
                            needs_recreation = True
                    else:
                        logger.warning("Could not get collection info, assuming recreation needed")
                        needs_recreation = True

                if not collection_exists or needs_recreation:
                    await self.client.create_collection(
                        collection_name=self.collection_name,
                        vector_size=self._embedding_dim,
                        distance="Cosine",
                        recreate_if_exists=True
                    )
                    logger.info(f"Created collection {self.collection_name} with dimension {self._embedding_dim}")

                # Initialize TF-IDF with sample data
                sample_docs = [
                    "Artificial intelligence is transforming industries.",
                    "Machine learning models require large datasets.",
                    "Natural language processing enables text understanding.",
                    "Deep learning uses neural networks for pattern recognition."
                ]
                await self.hybrid_retriever.train_tfidf(sample_docs)

                self._initialized = True
                logger.info("RAGService initialized successfully")

            except Exception as e:
                logger.error(f"Initialization failed: {str(e)}", exc_info=True)
                raise

    async def _ensure_collection_exists(self):
        """Ensure collection exists with proper configuration"""
        if not self._initialized:
            await self.initialize()



    def _calculate_similarity_from_score(self, score: float) -> float:
        """Convert Qdrant cosine similarity score to similarity percentage"""
        return max(0.0, min(1.0, (score + 1) / 2)) 

    async def ingest_document(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        filename: str,
        content: bytes,
        chunk_size: int = None,
        chunk_overlap: int = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Ingest document into Qdrant with proper dimension handling"""
        await self._ensure_collection_exists()
        
        chunk_size = chunk_size or settings.DEFAULT_CHUNK_SIZE
        chunk_overlap = chunk_overlap or settings.DEFAULT_CHUNK_OVERLAP
        
        try:
            logger.info(f"Starting document ingestion for {filename}")
            text = process_document(filename, content)
            if not text:
                raise ValueError("No text extracted from document")
            
            # Generate document ID with hash for uniqueness
            doc_hash = hashlib.md5(text.encode()).hexdigest()[:8]
            doc_id = f"{user_id}_{agent_id}_{os.path.splitext(filename)[0]}_{doc_hash}"
            
            # Process metadata with agent_id
            processed_metadata = {
                "user_id": user_id,
                "agent_id": agent_id,
                "filename": filename,
                "doc_id": doc_id,
                "ingested_at": datetime.utcnow().isoformat(),
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap
            }
            if metadata:
                processed_metadata.update(metadata)
            
            # Chunk the document
            chunks = self._chunk_text_with_overlap(
                text, 
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
            
            if not chunks:
                raise ValueError("No valid chunks created from document")
            
            # Generate embeddings
            embeddings = await self.embedding_fn.generate_embeddings(chunks)
            
            # Verify embedding dimensions match collection
            collection_info = await self.client.get_collection_info(self.collection_name)
            if not collection_info:
                raise ValueError("Could not get collection information")
                
            for i, embedding in enumerate(embeddings):
                if len(embedding) != collection_info.vector_size:
                    raise ValueError(
                        f"Embedding dimension mismatch at chunk {i}: "
                        f"expected {collection_info.vector_size}, got {len(embedding)}"
                    )
            
            # Prepare points for upsert
            points = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_metadata = processed_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": i,
                    "text_length": len(chunk),
                    "created_at": datetime.utcnow().isoformat()
                })
                
                point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{doc_id}_{i}"))
                
                points.append(qdrant_models.PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "text": chunk,
                        **chunk_metadata
                    }
                ))
            
            # Upsert points
            success = await self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            if not success:
                raise Exception("Failed to upsert points to Qdrant")
            
            logger.info(f"Successfully ingested document {filename} with {len(chunks)} chunks")
            return doc_id
            
        except Exception as e:
            logger.error(f"Ingest failed: {str(e)}", exc_info=True)
            raise

    def _chunk_text_with_overlap(
        self, 
        text: str, 
        chunk_size: int, 
        chunk_overlap: int,
        separator: str = "\n\n"
    ) -> List[str]:
        """Improved text chunking with semantic boundaries"""
        if not text or not text.strip():
            return []
        
        if len(text) <= chunk_size:
            return [text.strip()]
        
        # First split by major sections
        sections = re.split(r'\n{2,}', text)
        chunks = []
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
                
            if len(section) <= chunk_size:
                chunks.append(section)
                continue
                
            # Then split by sentences
            sentences = re.split(r'(?<=[.!?])\s+', section)
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                if len(current_chunk) + len(sentence) + 1 > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk)
                        
                        # Keep overlap if specified
                        if chunk_overlap > 0:
                            overlap_start = max(0, len(current_chunk) - chunk_overlap)
                            overlap_text = current_chunk[overlap_start:]
                            current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                        else:
                            current_chunk = sentence
                    else:
                        # Sentence is too long, split by words
                        words = sentence.split()
                        current_word_chunk = []
                        
                        for word in words:
                            if len(" ".join(current_word_chunk + [word])) > chunk_size:
                                if current_word_chunk:
                                    chunks.append(" ".join(current_word_chunk))
                                    current_word_chunk = current_word_chunk[-chunk_overlap:] if chunk_overlap > 0 else []
                                current_word_chunk.append(word)
                            else:
                                current_word_chunk.append(word)
                                
                        if current_word_chunk:
                            current_chunk = " ".join(current_word_chunk)
                else:
                    current_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if current_chunk:
                chunks.append(current_chunk)
        
        # Post-processing to clean up chunks
        cleaned_chunks = []
        seen_hashes = set()
        
        for chunk in chunks:
            chunk = chunk.strip()
            if not chunk or len(chunk) < 50:  # Minimum chunk size
                continue
                
            chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
            if chunk_hash not in seen_hashes:
                cleaned_chunks.append(chunk)
                seen_hashes.add(chunk_hash)
        
        logger.info(f"Split text into {len(cleaned_chunks)} chunks with overlap")
        return cleaned_chunks

    async def query(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        query: str,
        max_results: int = 5,
        min_score: float = 0.3,
        filters: Optional[Dict[str, Any]] = None,
        rewrite_query: bool = True,
        use_reranking: bool = True,
        hybrid_search: bool = True
    ) -> Dict[str, Any]:
        """Enhanced query with hybrid search and query rewriting"""
        await self._ensure_collection_exists()
        
        try:
            # Validate input
            if not query or len(query.strip()) < 3:
                raise ValueError("Query must be at least 3 characters")
            
            max_results = min(max(1, max_results), 20)  # Clamp between 1-20
            min_score = max(0.0, min(1.0, min_score))  # Clamp between 0-1

            # Query processing
            processed_query = await self._process_query(query, rewrite_query)
            
            # Search execution with agent_id filter
            search_results = await self._execute_search(
                user_id, 
                agent_id,
                processed_query, 
                max_results * 3,  # Get more results for re-ranking
                min_score,
                filters,
                hybrid_search
            )
            
            if not search_results:
                return self._empty_response(query)
            
            # Re-ranking if enabled
            if use_reranking and settings.RERANKING_ENABLED:
                search_results = await self._rerank_results(processed_query, search_results, max_results)
            else:
                search_results = search_results[:max_results]
            
            # Response generation
            return await self._generate_response(query, processed_query, search_results, use_reranking, hybrid_search)
            
        except Exception as e:
            logger.error(f"RAG query failed: {str(e)}", exc_info=True)
            raise

    async def _process_query(self, query: str, rewrite: bool) -> str:
        """Process and optimize the query"""
        processed_query = query
        
        if rewrite and settings.QUERY_REWRITING_ENABLED:
            processed_query = await self.query_rewriter.rewrite_query(query, self.ollama)
            logger.info(f"Rewritten query: {processed_query}")
            
            if settings.QUERY_EXPANSION_ENABLED:
                expanded_query = await self.query_rewriter.expand_query(processed_query, self.ollama)
                if expanded_query != processed_query:
                    logger.info(f"Expanded query: {expanded_query}")
                    processed_query = expanded_query
        
        return processed_query

    async def _execute_search(
        self,
        user_id: str,
        agent_id: str,
        query: str,
        limit: int,
        min_score: float,
        filters: Optional[Dict[str, Any]],
        hybrid_search: bool
    ) -> List[qdrant_models.ScoredPoint]:
        """Execute the search with hybrid approach if enabled"""
        from app.utils.qdrant_async import create_user_filter
        
        # Get collection info for dimension verification
        collection_info = await self.client.get_collection_info(self.collection_name)
        if not collection_info:
            raise ValueError("Could not get collection information")

        # Generate dense embeddings
        query_embeddings = await self.embedding_fn.generate_embeddings([query])
        if not query_embeddings or not query_embeddings[0]:
            raise ValueError("Failed to generate query embedding")
        
        # Verify embedding dimension matches collection
        if len(query_embeddings[0]) != collection_info.vector_size:
            raise ValueError(
                f"Embedding dimension mismatch: query has {len(query_embeddings[0])}, "
                f"collection expects {collection_info.vector_size}"
            )

        # Add agent_id to filters
        if filters is None:
            filters = {}
        filters["agent_id"] = agent_id

        # Dense vector search
        dense_results = await self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embeddings[0],
            query_filter=create_user_filter(user_id, filters),
            limit=limit,
            score_threshold=min_score
        )
        
        # Early return if hybrid search is disabled
        if not hybrid_search or not settings.HYBRID_SEARCH_ENABLED:
            return dense_results
        
        # Sparse search (TF-IDF)
        sparse_embedding = self.hybrid_retriever.get_sparse_embedding(query)
        sparse_results = []
        if sparse_embedding:
            try:
                # Qdrant requires all vectors to have same dimension as collection
                # So we need to pad/truncate sparse embeddings to match
                sparse_embedding_adjusted = sparse_embedding.copy()
                if len(sparse_embedding_adjusted) > collection_info.vector_size:
                    sparse_embedding_adjusted = sparse_embedding_adjusted[:collection_info.vector_size]
                else:
                    sparse_embedding_adjusted.extend(
                        [0.0] * (collection_info.vector_size - len(sparse_embedding_adjusted))
                    )
                
                sparse_results = await self.client.search(
                    collection_name=self.collection_name,
                    query_vector=sparse_embedding_adjusted,
                    query_filter=create_user_filter(user_id, filters),
                    limit=limit,
                    score_threshold=min_score
                )
            except Exception as e:
                logger.error(f"Sparse search failed: {str(e)}")
                # Fall back to dense results only
                return dense_results
        
        return self._combine_results(dense_results, sparse_results)

    def _combine_results(
        self,
        dense_results: List[qdrant_models.ScoredPoint],
        sparse_results: List[qdrant_models.ScoredPoint]
    ) -> List[qdrant_models.ScoredPoint]:
        """Combine dense and sparse search results"""
        if not sparse_results or not settings.HYBRID_SEARCH_ENABLED:
            return dense_results
            
        # Create a map of unique documents
        results_map = {}
        
        # Process dense results
        for result in dense_results:
            doc_id = result.payload.get("doc_id", str(result.id))
            results_map[doc_id] = {
                "result": result,
                "dense_score": result.score,
                "sparse_score": 0.0
            }
        
        # Process sparse results
        for result in sparse_results:
            doc_id = result.payload.get("doc_id", str(result.id))
            if doc_id in results_map:
                results_map[doc_id]["sparse_score"] = result.score
            else:
                results_map[doc_id] = {
                    "result": result,
                    "dense_score": 0.0,
                    "sparse_score": result.score
                }
        
        # Calculate combined scores
        combined_results = []
        for doc_data in results_map.values():
            combined_score = (
                settings.HYBRID_DENSE_WEIGHT * doc_data["dense_score"] +
                settings.HYBRID_SPARSE_WEIGHT * doc_data["sparse_score"]
            )
            new_result = doc_data["result"]
            new_result.score = combined_score
            combined_results.append(new_result)
        
        # Sort by combined score
        combined_results.sort(key=lambda x: x.score, reverse=True)
        return combined_results

    async def _rerank_results(
        self,
        query: str,
        results: List[qdrant_models.ScoredPoint],
        top_k: int
    ) -> List[qdrant_models.ScoredPoint]:
        """Re-rank results using cross-encoder"""
        if not results or not settings.RERANKING_ENABLED:
            return results[:top_k]
            
        documents = [res.payload.get("text", "") for res in results]
        scores = [res.score for res in results]
        
        reranked_docs, reranked_scores = await self.hybrid_retriever.rerank_results(
            query,
            documents,
            scores,
            top_k
        )
        
        # Reconstruct results with new order
        reranked_results = []
        doc_set = set(reranked_docs)
        for doc, score in zip(reranked_docs, reranked_scores):
            for res in results:
                if res.payload.get("text", "") == doc and res not in reranked_results:
                    res.score = score  # Update score
                    reranked_results.append(res)
                    break
        
        return reranked_results[:top_k]

    async def _generate_response(
        self,
        original_query: str,
        processed_query: str,
        results: List[qdrant_models.ScoredPoint],
        use_reranking: bool,
        hybrid_search_used: bool
    ) -> Dict[str, Any]:
        """Generate final response with LLM"""
        if not results:
            return self._empty_response(original_query)
        
        # Extract documents and metadata
        documents = []
        metadatas = []
        for result in results:
            documents.append(result.payload.get("text", ""))
            metadatas.append(result.payload)
        
        # Build context
        context = "\n\n---\n\n".join([
            f"Source: {meta.get('filename', 'Unknown')} (Chunk {meta.get('chunk_index', 0) + 1})\n{doc}"
            for doc, meta in zip(documents, metadatas)
        ])
        
        # Generate answer with LLM
        system_prompt = f"""You are a helpful AI assistant. Use the following context to answer the user's question.
If you don't know the answer based on the provided context, say so honestly. Don't make up information.
Be comprehensive but concise in your response.

Context:
{context}

Instructions:
- Answer based only on the provided context
- If the context doesn't contain enough information, say so
- Cite relevant sources when appropriate
- Be helpful and informative"""

        llm_response = await self.ollama.chat(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": original_query}  # Use original query here
            ]
        )
        
        # Prepare response
        sources = list(set(
            meta.get("filename", "Unknown") 
            for meta in metadatas 
            if meta and "filename" in meta
        ))
        
        return {
            "answer": llm_response.get("message", {}).get("content", "No response generated"),
            "documents": documents,
            "context": llm_response.get("context", []),
            "sources": sources,
            "debug_info": {
                "original_query": original_query,
                "processed_query": processed_query,
                "search_method": "hybrid" if hybrid_search_used else "dense",
                "reranking_applied": use_reranking,
                "total_results": len(results),
                "scores": [self._calculate_similarity_from_score(r.score) for r in results],
                "timestamp": datetime.utcnow().isoformat()
            }
        }

    def _empty_response(self, query: str) -> Dict[str, Any]:
        """Generate empty response structure"""
        return {
            "answer": f"No relevant documents found for your query: {query}",
            "documents": [],
            "context": [],
            "sources": [],
            "debug_info": {
                "original_query": query,
                "search_method": "none",
                "timestamp": datetime.utcnow().isoformat()
            }
        }

    async def list_documents(
        self,
        db: AsyncSession,
        user_id: str,
        agent_id: str,
        page: int = 1,
        per_page: int = 10
    ) -> List[Dict[str, Any]]:
        """List documents with pagination"""
        await self._ensure_collection_exists()
        
        try:
            if page < 1 or per_page < 1:
                raise ValueError("Page and per_page must be positive integers")

            from app.utils.qdrant_async import create_user_filter
            
            # Include agent_id in filter
            filters = {"agent_id": agent_id}
            
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id, filters),
                with_payload=True,
                limit=1000
            )
            
            if not records:
                return []

            unique_docs = {}
            for record in records:
                payload = record.payload
                if payload and "doc_id" in payload:
                    doc_id = payload["doc_id"]
                    if doc_id not in unique_docs:
                        unique_docs[doc_id] = {
                            "document_id": doc_id,
                            "filename": payload.get("filename", "unknown"),
                            "chunk_count": 0,
                            "total_text_length": 0,
                            "created_at": payload.get("created_at", 0)
                        }
                    
                    unique_docs[doc_id]["chunk_count"] += 1
                    unique_docs[doc_id]["total_text_length"] += len(payload.get("text", ""))

            doc_list = list(unique_docs.values())
            doc_list.sort(key=lambda x: x.get("created_at", 0), reverse=True)
            
            start = (page - 1) * per_page
            end = start + per_page
            return doc_list[start:end]
        except Exception as e:
            logger.error(f"Failed to list documents: {str(e)}", exc_info=True)
            raise


    async def delete_document(
        self,
        db: AsyncSession,
        user_id: str,
        document_id: str
    ) -> bool:
        """Delete a document and all its chunks"""
        await self._ensure_collection_exists()
        
        try:
            from app.utils.qdrant_async import create_document_filter
            
            success = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=qdrant_models.FilterSelector(
                    filter=create_document_filter(user_id, document_id)
                )
            )
            
            logger.info(f"Successfully deleted document {document_id} for user {user_id}")
            return success
            
        except Exception as e:
            logger.error(f"Failed to delete document {document_id}: {str(e)}", exc_info=True)
            raise

    async def get_document_stats(self, db: AsyncSession, user_id: str) -> Dict[str, Any]:
        """Get statistics about user's documents"""
        await self._ensure_collection_exists()
        
        try:
            from app.utils.qdrant_async import create_user_filter
            
            total_chunks = await self.client.count_points(
                collection_name=self.collection_name,
                count_filter=create_user_filter(user_id)
            )
            
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=create_user_filter(user_id),
                with_payload=True,
                limit=1000
            )
            
            unique_docs = set()
            total_text_length = 0
            
            for record in records:
                payload = record.payload
                if payload:
                    unique_docs.add(payload.get("doc_id", ""))
                    total_text_length += len(payload.get("text", ""))
            
            return {
                "total_documents": len(unique_docs),
                "total_chunks": total_chunks,
                "total_text_length": total_text_length,
                "average_chunk_size": total_text_length // max(total_chunks, 1),
                "embedding_dimension": self._embedding_dim
            }
            
        except Exception as e:
            logger.error(f"Failed to get document stats: {str(e)}", exc_info=True)
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "total_text_length": 0,
                "average_chunk_size": 0,
                "embedding_dimension": self._embedding_dim or 384
            }

    async def close(self):
        """Clean up resources"""
        if self.client:
            await self.client.close()
        logger.info("RAG service closed successfully")
===== ./app/services/agent_service.py =====
# Updated agent_service.py
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.db_models import DBAgent
from app.models.agent_model import AgentCreate, AgentUpdate
from datetime import datetime
from sqlalchemy import select, update, delete
import httpx
import json
from app.dependencies import get_db_session

class AgentService:
    def __init__(self, db: AsyncSession = Depends(get_db_session)):
        self.db = db

    async def create_agent(self, owner_id: str, agent_data: AgentCreate) -> DBAgent:
        db_agent = DBAgent(
            owner_id=owner_id,
            name=agent_data.name,
            description=agent_data.description,
            model=agent_data.model,
            system_prompt=agent_data.system_prompt,
            is_public=agent_data.is_public,
            tools=agent_data.tools,
            agent_metadata=agent_data.metadata
        )
        self.db.add(db_agent)
        await self.db.commit()
        await self.db.refresh(db_agent)
        return db_agent

    async def get_agent(self, agent_id: str, owner_id: str) -> DBAgent:
        result = await self.db.execute(
            select(DBAgent).where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        return result.scalars().first()

    async def list_agents(self, owner_id: str) -> list[DBAgent]:
        result = await self.db.execute(
            select(DBAgent).where(DBAgent.owner_id == owner_id)
        )
        return result.scalars().all()

    async def update_agent(self, agent_id: str, owner_id: str, update_data: AgentUpdate) -> DBAgent:
        update_dict = update_data.dict(exclude_unset=True)
        if not update_dict:
            return await self.get_agent(agent_id, owner_id)
            
        await self.db.execute(
            update(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
            .values(**update_dict)
        )
        await self.db.commit()
        return await self.get_agent(agent_id, owner_id)

    async def delete_agent(self, agent_id: str, owner_id: str) -> bool:
        result = await self.db.execute(
            delete(DBAgent)
            .where(
                DBAgent.id == agent_id,
                DBAgent.owner_id == owner_id
            )
        )
        await self.db.commit()
        return result.rowcount > 0


# Standalone execute_agent function
async def execute_agent(agent_id: str, owner_id: str, input_data: dict):
    """
    Execute an agent with the given input data.
    This function should be implemented based on your specific agent execution logic.
    """
    from app.database import get_async_session
    
    # Get database session
    async with get_async_session() as db:
        agent_service = AgentService(db)
        
        # Get the agent
        agent = await agent_service.get_agent(agent_id, owner_id)
        if not agent:
            raise ValueError(f"Agent {agent_id} not found")
        
        # Extract message and parameters
        message = input_data.get("message", "")
        parameters = input_data.get("parameters", {})
        
        # Here you would implement your agent execution logic
        # This is a placeholder implementation
        try:
            # Example: Call to Ollama or your AI service
            response = await call_ai_service(
                model=agent.model,
                system_prompt=agent.system_prompt,
                message=message,
                parameters=parameters
            )
            
            return {
                "status": "success",
                "response": response,
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }


async def call_ai_service(model: str, system_prompt: str, message: str, parameters: dict):
    """
    Call your AI service (e.g., Ollama) to execute the agent.
    Replace this with your actual AI service integration.
    """
    # Example implementation for Ollama
    try:
        async with httpx.AsyncClient() as client:
            payload = {
                "model": model,
                "prompt": f"{system_prompt}\n\nUser: {message}",
                "stream": False,
                **parameters
            }
            
            response = await client.post(
                "http://localhost:11434/api/generate",  # Adjust URL as needed
                json=payload,
                timeout=30.0
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "No response generated")
            
    except Exception as e:
        raise Exception(f"AI service call failed: {str(e)}")
===== ./app/services/cache.py =====
from redis.asyncio import Redis
from app.config import settings
import logging
from typing import Any, Optional
import json

logger = logging.getLogger(__name__)

class CacheService:
    def __init__(self):
        self.redis = None
        self.enabled = settings.CACHE_ENABLED
        self._connection_attempted = False
        
    async def connect(self):
        if self.enabled and not self._connection_attempted:
            self._connection_attempted = True
            try:
                self.redis = Redis.from_url(
                    str(settings.REDIS_URL),
                    encoding="utf-8",
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
                await self.redis.ping()
                logger.info("Connected to Redis successfully")
                self.enabled = True
            except Exception as e:
                logger.warning(f"Redis connection failed, caching disabled: {str(e)}")
                self.enabled = False
                self.redis = None
    
    async def disconnect(self):
        if self.redis:
            try:
                await self.redis.close()
                logger.info("Redis connection closed")
            except Exception as e:
                logger.error(f"Error closing Redis connection: {str(e)}")
    
    async def get(self, key: str) -> Optional[Any]:
        if not self.enabled or not self.redis:
            return None
            
        try:
            data = await self.redis.get(key)
            if data is None:
                return None
                
            try:
                return json.loads(data)
            except json.JSONDecodeError:
                return data
        except Exception as e:
            logger.warning(f"Cache get error for key '{key}': {str(e)}")
            return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            ttl = ttl or getattr(settings, 'CACHE_TTL', 300)  # Default 5 minutes
            
            if isinstance(value, (str, bytes)):
                serialized_value = value
            else:
                try:
                    serialized_value = json.dumps(value, default=str)
                except (TypeError, ValueError) as e:
                    logger.warning(f"Failed to serialize value for key '{key}': {str(e)}")
                    return False
            
            await self.redis.set(key, serialized_value, ex=ttl)
            return True
        except Exception as e:
            logger.warning(f"Cache set error for key '{key}': {str(e)}")
            return False
    
    async def delete(self, key: str) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.delete(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache delete error for key '{key}': {str(e)}")
            return False
    
    async def exists(self, key: str) -> bool:
        if not self.enabled or not self.redis:
            return False
            
        try:
            result = await self.redis.exists(key)
            return result > 0
        except Exception as e:
            logger.warning(f"Cache exists error for key '{key}': {str(e)}")
            return False
    
    async def clear_pattern(self, pattern: str) -> int:
        """Clear all keys matching a pattern"""
        if not self.enabled or not self.redis:
            return 0
            
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                return await self.redis.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"Cache clear pattern error for pattern '{pattern}': {str(e)}")
            return 0
    
    async def health_check(self) -> bool:
        """Check if Redis is healthy"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            await self.redis.ping()
            return True
        except Exception as e:
            logger.warning(f"Redis health check failed: {str(e)}")
            return False

    async def get_chat_history(self, user_id: str, agent_id: str) -> Optional[list]:
        """Get chat history from cache"""
        if not self.enabled or not self.redis:
            return None
            
        try:
            data = await self.redis.get(f"chat_history:{user_id}:{agent_id}")
            if data is None:
                return None
            return json.loads(data)
        except Exception as e:
            logger.warning(f"Error getting chat history: {str(e)}")
            return None

    async def save_chat_history(self, user_id: str, agent_id: str, history: list) -> bool:
        """Save chat history to cache"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            await self.redis.set(
                f"chat_history:{user_id}:{agent_id}",
                json.dumps(history),
                ex=86400  # 24 hours TTL
            )
            return True
        except Exception as e:
            logger.warning(f"Error saving chat history: {str(e)}")
            return False

    async def clear_chat_history(self, user_id: str, agent_id: str) -> bool:
        """Clear chat history from cache"""
        if not self.enabled or not self.redis:
            return False
            
        try:
            await self.redis.delete(f"chat_history:{user_id}:{agent_id}")
            return True
        except Exception as e:
            logger.warning(f"Error clearing chat history: {str(e)}")
            return False


# Global instance
cache_service = CacheService()
===== ./app/services/__init__.py =====
# Empty file to make services a package
===== ./app/services/llm_service.py =====
# app/services/llm_service.py
import requests
import logging
from typing import Optional, Dict, Any, List
from app.config import settings
import asyncio
import aiohttp
import hashlib
import numpy as np
import json

logger = logging.getLogger(__name__)

class OllamaService:
    def __init__(self):
        self.base_url = str(settings.OLLAMA_URL).rstrip('/')
        self.default_model = settings.DEFAULT_OLLAMA_MODEL
        self.timeout = settings.OLLAMA_TIMEOUT
        # Use the main model for embeddings if no specific embedding model
        self.embedding_model = getattr(settings, 'EMBEDDING_MODEL', 'deepseek-r1:1.5b')

    async def _make_request_async(self, endpoint: str, payload: dict) -> dict:
        """Make async request to Ollama with proper error handling"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making async request to: {url}")
            logger.debug(f"Payload: {json.dumps(payload, indent=2)}")
        
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            async with aiohttp.ClientSession(
                timeout=timeout,
                connector=aiohttp.TCPConnector(limit=10)
            ) as session:
                try:
                    async with session.post(
                        url,
                        json=payload,
                        headers={
                            'Content-Type': 'application/json',
                            'Accept': 'application/json'
                        }
                    ) as response:
                        logger.debug(f"Response status: {response.status}")
                        
                        if response.status == 404:
                            # Check if it's really a 404 or a model not found issue
                            error_text = await response.text()
                            if "model" in error_text.lower() and "not found" in error_text.lower():
                                raise ValueError(f"Model '{payload.get('model')}' not found. Please pull the model first.")
                            else:
                                raise ValueError(f"Ollama endpoint not found at {url}. Is Ollama running and accessible?")
                        
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"HTTP {response.status}: {error_text}")
                            raise ValueError(f"HTTP {response.status}: {error_text}")
                        
                        response_text = await response.text()
                        logger.debug(f"Response text: {response_text[:500]}...")
                        
                        try:
                            return json.loads(response_text)
                        except json.JSONDecodeError as e:
                            logger.error(f"Failed to parse JSON response: {response_text}")
                            raise ValueError(f"Invalid JSON response from Ollama: {str(e)}")
                    
                except aiohttp.ClientConnectorError as e:
                    logger.error(f"Connection error: {str(e)}")
                    raise ConnectionError(f"Could not connect to Ollama at {url}. Please ensure Ollama is running and accessible.")
                except asyncio.TimeoutError:
                    logger.error(f"Request timeout after {self.timeout} seconds")
                    raise TimeoutError(f"Request to Ollama timed out after {self.timeout} seconds")
                
        except Exception as e:
            logger.error(f"Error calling Ollama async: {str(e)}")
            raise RuntimeError(f"Failed to communicate with Ollama: {str(e)}")

    def _make_request_sync_wrapper(self, endpoint: str, payload: dict) -> dict:
        """Synchronous wrapper for use with run_in_executor"""
        try:
            url = f"{self.base_url}{endpoint}"
            logger.debug(f"Making sync request to: {url}")
            logger.debug(f"Payload: {json.dumps(payload, indent=2)}")
            
            response = requests.post(
                url,
                json=payload,
                headers={
                    'Content-Type': 'application/json',
                    'Accept': 'application/json'
                },
                timeout=self.timeout
            )
            
            logger.debug(f"Response status: {response.status_code}")
            logger.debug(f"Response text: {response.text[:500]}...")
            
            if response.status_code == 404:
                # Check if it's really a 404 or a model not found issue
                if "model" in response.text.lower() and "not found" in response.text.lower():
                    raise ValueError(f"Model '{payload.get('model')}' not found. Please pull the model first.")
                else:
                    raise ValueError(f"Ollama endpoint not found at {url}. Is Ollama running and accessible?")
            
            response.raise_for_status()
            
            try:
                return response.json()
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON response: {response.text}")
                raise ValueError(f"Invalid JSON response from Ollama: {str(e)}")
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Connection error: {str(e)}")
            raise ConnectionError(f"Could not connect to Ollama at {url}. Please ensure Ollama is running and accessible.")
        except requests.exceptions.Timeout:
            logger.error(f"Request timeout after {self.timeout} seconds")
            raise TimeoutError(f"Request to Ollama timed out after {self.timeout} seconds")
        except Exception as e:
            logger.error(f"Error calling Ollama sync: {str(e)}")
            raise RuntimeError(f"Failed to communicate with Ollama: {str(e)}")

    async def check_ollama_status(self) -> Dict[str, Any]:
        """Check if Ollama is running and accessible"""
        try:
            # Try to get the list of models first
            models = await self.list_models()
            
            # Check if our models are available
            model_names = [model.get('name', '') for model in models]
            
            return {
                "status": "healthy",
                "available_models": model_names,
                "default_model_available": self.default_model in model_names,
                "embedding_model_available": self.embedding_model in model_names,
                "base_url": self.base_url
            }
            
        except Exception as e:
            logger.error(f"Ollama health check failed: {str(e)}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "base_url": self.base_url
            }

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None,
        system: Optional[str] = None,
        template: Optional[str] = None,
        context: Optional[list] = None
    ) -> Dict[str, Any]:
        """Generate text using Ollama with proper payload structure"""
        payload = {
            "model": model or self.default_model,
            "prompt": prompt,
            "stream": False,
            "options": options or {},
        }
        
        # Add optional parameters only if they exist
        if system: 
            payload["system"] = system
        if template: 
            payload["template"] = template
        if context: 
            payload["context"] = context
        
        # First check if the model is available
        if not await self.check_model_availability(payload["model"]):
            logger.warning(f"Model {payload['model']} not available, attempting to pull...")
            success = await self.pull_model(payload["model"])
            if not success:
                raise ValueError(f"Model {payload['model']} is not available and could not be pulled")
        
        # Try async first, then fallback to sync
        try:
            logger.info(f"Attempting async generate request with model: {payload['model']}")
            return await self._make_request_async("/api/generate", payload)
        except Exception as e:
            logger.warning(f"Async generate request failed: {str(e)}")
            logger.info("Falling back to sync request")
            try:
                return await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self._make_request_sync_wrapper("/api/generate", payload)
                )
            except Exception as sync_e:
                logger.error(f"Both async and sync generate requests failed. Async: {str(e)}, Sync: {str(sync_e)}")
                raise RuntimeError(f"All generate request methods failed. Last error: {str(sync_e)}")

    async def chat(
        self,
        messages: list,
        model: Optional[str] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Chat with Ollama using proper message format"""
        payload = {
            "model": model or self.default_model,
            "messages": messages,
            "stream": False,
            "options": options or {},
        }
        
        # First check if the model is available
        if not await self.check_model_availability(payload["model"]):
            logger.warning(f"Model {payload['model']} not available, attempting to pull...")
            success = await self.pull_model(payload["model"])
            if not success:
                raise ValueError(f"Model {payload['model']} is not available and could not be pulled")
        
        # Try async first, then fallback to sync
        try:
            logger.info(f"Attempting async chat request with model: {payload['model']}")
            return await self._make_request_async("/api/chat", payload)
        except Exception as e:
            logger.warning(f"Async chat request failed: {str(e)}")
            logger.info("Falling back to sync request")
            try:
                return await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self._make_request_sync_wrapper("/api/chat", payload)
                )
            except Exception as sync_e:
                logger.error(f"Both async and sync chat requests failed. Async: {str(e)}, Sync: {str(sync_e)}")
                raise RuntimeError(f"All chat request methods failed. Last error: {str(sync_e)}")

    async def list_models(self) -> list:
        """List available models with improved error handling"""
        try:
            # Try async first
            try:
                url = f"{self.base_url}/api/tags"
                timeout = aiohttp.ClientTimeout(total=10)  # Shorter timeout for listing
                
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"Failed to list models: HTTP {response.status}: {error_text}")
                            raise ValueError(f"HTTP {response.status}: {error_text}")
                        
                        result = await response.json()
                        models = result.get("models", [])
                        logger.info(f"Found {len(models)} models")
                        return models
                        
            except Exception as async_error:
                logger.warning(f"Async model listing failed: {str(async_error)}")
                # Fallback to sync
                response = requests.get(
                    f"{self.base_url}/api/tags",
                    timeout=10,
                    headers={'Accept': 'application/json'}
                )
                response.raise_for_status()
                result = response.json()
                models = result.get("models", [])
                logger.info(f"Found {len(models)} models (sync)")
                return models
                
        except Exception as e:
            logger.error(f"Error listing models: {str(e)}")
            return []

    async def create_embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> List[float]:
        """Create embeddings with multiple fallback strategies"""
        target_model = model or self.embedding_model
        
        # Strategy 1: Try the embeddings endpoint
        try:
            return await self._try_embeddings_endpoint(text, target_model)
        except Exception as e:
            logger.warning(f"Embeddings endpoint failed: {str(e)}")
        
        # Strategy 2: Try using the main model for embeddings if it's different
        if target_model != self.default_model:
            try:
                logger.info(f"Trying main model {self.default_model} for embeddings")
                return await self._try_embeddings_endpoint(text, self.default_model)
            except Exception as e:
                logger.warning(f"Main model embedding failed: {str(e)}")
        
        # Strategy 3: Try generate endpoint with special prompt
        try:
            return await self._create_embedding_via_generate(text, target_model)
        except Exception as e:
            logger.warning(f"Generate-based embedding failed: {str(e)}")
        
        # Strategy 4: Fallback to deterministic hash-based embedding
        logger.warning("All embedding methods failed, using deterministic fallback")
        return self._create_deterministic_embedding(text)

    async def _try_embeddings_endpoint(self, text: str, model: str) -> List[float]:
        """Try the official embeddings endpoint with multiple payload formats"""
        
        # Try different payload formats
        payload_formats = [
            {"model": model, "prompt": text},
            {"model": model, "input": text},  
            {"name": model, "prompt": text},
            {"name": model, "input": text}
        ]
        
        endpoints = ["/api/embeddings", "/api/embed"]
        
        for endpoint in endpoints:
            for payload in payload_formats:
                try:
                    logger.debug(f"Trying {endpoint} with payload format: {list(payload.keys())}")
                    result = await self._make_request_async(endpoint, payload)
                    
                    # Handle different response formats
                    if 'embeddings' in result:
                        embeddings = result['embeddings']
                        if embeddings and len(embeddings) > 0:
                            return embeddings[0] if isinstance(embeddings[0], list) else embeddings
                    elif 'embedding' in result:
                        return result['embedding']
                    elif 'data' in result:
                        # OpenAI-style response
                        return result['data'][0]['embedding']
                    
                except Exception as e:
                    logger.debug(f"Failed {endpoint} with {list(payload.keys())}: {str(e)}")
                    continue
        
        raise ValueError("No valid embeddings endpoint found")

    async def _create_embedding_via_generate(self, text: str, model: str) -> List[float]:
        """Create embedding using generate endpoint with special prompt"""
        
        # Try a simple approach that might work with some models
        prompt = f"Embed: {text}"
        
        response = await self.generate(
            prompt=prompt,
            model=model,
            options={
                "temperature": 0.0,
                "num_predict": 1,  # Minimal generation
                "stop": ["\n", ".", "!"]
            }
        )
        
        # This is a fallback - extract features from the response
        response_text = response.get("response", "")
        
        # Create a more sophisticated embedding from the response
        return self._text_to_embedding(text + " " + response_text)

    def _create_deterministic_embedding(self, text: str) -> List[float]:
        """Create a deterministic embedding from text using multiple hash functions"""
        
        # Use multiple hash functions for better distribution
        hash_functions = [
            lambda x: hashlib.md5(x.encode()).hexdigest(),
            lambda x: hashlib.sha1(x.encode()).hexdigest(),
            lambda x: hashlib.sha256(x.encode()).hexdigest(),
        ]
        
        # Target dimension
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        embedding = []
        
        # Generate embedding using multiple hash functions
        for i, hash_func in enumerate(hash_functions):
            hash_hex = hash_func(f"{text}_{i}")
            
            # Convert hex to floats
            for j in range(0, len(hash_hex), 2):
                if len(embedding) >= target_dim:
                    break
                val = int(hash_hex[j:j+2], 16) / 255.0  # Normalize to 0-1
                embedding.append(val)
            
            if len(embedding) >= target_dim:
                break
        
        # Pad or truncate to desired dimension
        if len(embedding) < target_dim:
            embedding.extend([0.1] * (target_dim - len(embedding)))
        else:
            embedding = embedding[:target_dim]
        
        # Add some text-based features
        text_features = [
            len(text) / 1000.0,  # Text length feature
            text.count(' ') / len(text) if text else 0,  # Word density
            text.count('.') / len(text) if text else 0,  # Sentence density
        ]
        
        # Replace some values with text features
        for i, feature in enumerate(text_features):
            if i < len(embedding):
                embedding[i] = feature
        
        # Normalize the embedding
        embedding_array = np.array(embedding)
        norm = np.linalg.norm(embedding_array)
        if norm > 0:
            embedding = (embedding_array / norm).tolist()
        
        logger.info(f"Created deterministic embedding with dimension {len(embedding)}")
        return embedding

    def _text_to_embedding(self, text: str) -> List[float]:
        """Convert text to embedding using various text features"""
        
        target_dim = getattr(settings, 'EMBEDDING_FALLBACK_DIMENSION', 768)
        
        # Extract various text features
        features = []
        
        # Character-level features
        for i in range(min(len(text), 50)):
            features.append(ord(text[i]) / 255.0)
        
        # Word-level features
        words = text.lower().split()
        for word in words[:20]:  # First 20 words
            word_hash = hash(word) % 1000
            features.append(word_hash / 1000.0)
        
        # Statistical features
        features.extend([
            len(text) / 1000.0,
            len(words) / 100.0 if words else 0,
            text.count(' ') / len(text) if text else 0,
            text.count('.') / len(text) if text else 0,
            text.count(',') / len(text) if text else 0,
        ])
        
        # Pad or truncate
        if len(features) < target_dim:
            features.extend([0.1] * (target_dim - len(features)))
        else:
            features = features[:target_dim]
        
        # Normalize
        features_array = np.array(features)
        norm = np.linalg.norm(features_array)
        if norm > 0:
            features = (features_array / norm).tolist()
        
        return features

    async def check_model_availability(self, model_name: str) -> bool:
        """Check if a specific model is available"""
        try:
            models = await self.list_models()
            available_models = [model.get('name', '') for model in models]
            is_available = model_name in available_models
            logger.info(f"Model {model_name} availability: {is_available}")
            if not is_available:
                logger.info(f"Available models: {available_models}")
            return is_available
        except Exception as e:
            logger.error(f"Error checking model availability: {str(e)}")
            return False

    async def pull_model(self, model_name: str) -> bool:
        """Pull a model if it's not available"""
        try:
            payload = {"name": model_name}
            
            logger.info(f"Attempting to pull model: {model_name}")
            
            # Use longer timeout for model pulling
            try:
                timeout = aiohttp.ClientTimeout(total=300)  # 5 minutes
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.post(
                        f"{self.base_url}/api/pull",
                        json=payload
                    ) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"Failed to pull model: HTTP {response.status}: {error_text}")
                            return False
                        
                        logger.info(f"Successfully pulled model: {model_name}")
                        return True
            except Exception as async_error:
                logger.warning(f"Async pull failed: {str(async_error)}")
                # Fallback to sync
                response = requests.post(
                    f"{self.base_url}/api/pull",
                    json=payload,
                    timeout=300
                )
                if response.status_code != 200:
                    logger.error(f"Failed to pull model: HTTP {response.status_code}: {response.text}")
                    return False
                
                logger.info(f"Successfully pulled model: {model_name}")
                return True
                
        except Exception as e:
            logger.error(f"Error pulling model {model_name}: {str(e)}")
            return False

    async def ensure_embedding_model(self) -> bool:
        """Ensure embedding model is available"""
        try:
            # Check if current embedding model is available
            if await self.check_model_availability(self.embedding_model):
                logger.info(f"Embedding model {self.embedding_model} is available")
                return True
            
            # If not available and it's different from default model, try default model
            if self.embedding_model != self.default_model:
                if await self.check_model_availability(self.default_model):
                    logger.info(f"Using default model {self.default_model} for embeddings")
                    self.embedding_model = self.default_model
                    return True
            
            # Try to pull the embedding model
            logger.info(f"Embedding model {self.embedding_model} not found, attempting to pull...")
            success = await self.pull_model(self.embedding_model)
            
            if not success and self.embedding_model != self.default_model:
                logger.info(f"Failed to pull {self.embedding_model}, falling back to {self.default_model}")
                self.embedding_model = self.default_model
                return await self.check_model_availability(self.default_model)
            
            return success
            
        except Exception as e:
            logger.error(f"Error ensuring embedding model: {str(e)}")
            return False
===== ./app/services/mcp_client.py =====
# app/services/mcp_client.py
import httpx
import logging
from app.config import settings
from typing import Dict, Any
from fastapi import HTTPException, status

logger = logging.getLogger(__name__)

class MCPClient:
    def __init__(self):
        self.base_url = settings.MCP_SERVER_URL
        self.timeout = 30
    
    async def dispatch_task(self, agent_id: str, task_type: str, payload: Dict[str, Any]) -> str:
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/tasks",
                    json={
                        "agent_id": agent_id,
                        "task_type": task_type,
                        "payload": payload
                    }
                )
                response.raise_for_status()
                return response.json()["task_id"]
        except httpx.HTTPStatusError as e:
            logger.error(f"MCP server returned error: {e.response.text}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail="MCP service error"
            )
        except Exception as e:
            logger.error(f"Failed to dispatch MCP task: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="MCP service unavailable"
            )
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.base_url}/tasks/{task_id}")
                response.raise_for_status()
                return response.json()
        except httpx.HTTPStatusError as e:
            logger.error(f"MCP server returned error: {e.response.text}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail="MCP service error"
            )
        except Exception as e:
            logger.error(f"Failed to get MCP task status: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="MCP service unavailable"
            )
===== ./app/services/agent_execution_service.py =====
# app/services/agent_execution_service.py
from datetime import datetime
import httpx
from app.services.agent_service import AgentService
from sqlalchemy.ext.asyncio import AsyncSession
import logging

logger = logging.getLogger(__name__)

async def call_ai_service(
    model: str,
    system_prompt: str,
    message: str,
    parameters: dict
):
    """Call your AI service (e.g., Ollama) to execute the agent"""
    try:
        async with httpx.AsyncClient() as client:
            payload = {
                "model": model,
                "prompt": f"{system_prompt}\n\nUser: {message}",
                "stream": False,
                **parameters
            }
            
            response = await client.post(
                "http://localhost:11434/api/generate",  # Ollama endpoint
                json=payload,
                timeout=30.0
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "No response generated")
            
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTP error calling AI service: {str(e)}")
        raise Exception(f"AI service returned HTTP error: {str(e)}")
    except httpx.RequestError as e:
        logger.error(f"Request error calling AI service: {str(e)}")
        raise Exception(f"Failed to connect to AI service: {str(e)}")
    except Exception as e:
        logger.error(f"Error calling AI service: {str(e)}")
        raise Exception(f"AI service call failed: {str(e)}")

async def execute_agent(
    agent_id: str, 
    owner_id: str, 
    input_data: dict,
    db: AsyncSession
):
    """
    Execute an agent with the given input data.
    """
    agent_service = AgentService(db)
    
    try:
        # Get the agent
        agent = await agent_service.get_agent(agent_id, owner_id)
        if not agent:
            raise ValueError(f"Agent {agent_id} not found")
        
        # Extract message and parameters
        message = input_data.get("message", "")
        parameters = input_data.get("parameters", {})
        
        # Execute the agent
        try:
            response = await call_ai_service(
                model=agent.model,
                system_prompt=agent.system_prompt,
                message=message,
                parameters=parameters
            )
            
            return {
                "status": "success",
                "response": response,
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat()
            }
    except Exception as e:
        await db.rollback()
        raise
    finally:
        await db.close()
===== ./app/services/voice_service.py =====
import logging
from typing import Optional, Tuple
from fastapi import UploadFile
from app.config import settings
import io
import numpy as np
import torch
import torchaudio
import tempfile
import os

logger = logging.getLogger(__name__)

class VoiceService:
    def __init__(self):
        self.whisper_model = None
        self.tts_model = None
        self.load_models()

    def load_models(self):
        """Lazy load voice models"""
        try:
            if settings.ENABLE_WHISPER:
                import whisper
                self.whisper_model = whisper.load_model(settings.WHISPER_MODEL)
                
            if settings.ENABLE_TTS:
                if settings.TTS_ENGINE == "coqui":
                    from TTS.api import TTS
                    self.tts_model = TTS(model_name=settings.TTS_MODEL)
                elif settings.TTS_ENGINE == "pyttsx3":
                    import pyttsx3
                    self.tts_model = pyttsx3.init()
        except ImportError as e:
            logger.warning(f"Voice models not available: {str(e)}")

    async def speech_to_text(
        self,
        audio_file: UploadFile,
        language: Optional[str] = None
    ) -> str:
        """Convert speech to text using Whisper"""
        if not self.whisper_model:
            raise RuntimeError("Whisper is not enabled or failed to load")
            
        try:
            # Read audio file
            contents = await audio_file.read()
            audio_buffer = io.BytesIO(contents)
            
            # Load audio with torchaudio
            waveform, sample_rate = torchaudio.load(audio_buffer)
            
            # Resample if needed (Whisper expects 16kHz)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(
                    orig_freq=sample_rate,
                    new_freq=16000
                )
                waveform = resampler(waveform)
            
            # Convert to numpy array
            audio_np = waveform.numpy().squeeze()
            
            # Transcribe
            result = self.whisper_model.transcribe(
                audio_np,
                language=language,
                fp16=torch.cuda.is_available()
            )
            return result["text"]
        except Exception as e:
            logger.error(f"Error in speech-to-text: {str(e)}")
            raise

    async def text_to_speech(
        self,
        text: str,
        output_format: str = "wav"
    ) -> bytes:
        """Convert text to speech"""
        if not self.tts_model:
            raise RuntimeError("TTS is not enabled or failed to load")
            
        try:
            if settings.TTS_ENGINE == "coqui":
                # Coqui TTS
                output = io.BytesIO()
                self.tts_model.tts_to_file(
                    text=text,
                    file_path=output,
                    speaker=self.tts_model.speakers[0],
                    language=self.tts_model.languages[0]
                )
                return output.getvalue()
                
            elif settings.TTS_ENGINE == "pyttsx3":
                # pyttsx3 TTS - corrected implementation
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{output_format}") as tmpfile:
                    temp_filename = tmpfile.name
                
                self.tts_model.save_to_file(text, temp_filename)
                self.tts_model.runAndWait()
                
                with open(temp_filename, "rb") as f:
                    audio_data = f.read()
                
                os.remove(temp_filename)
                return audio_data
                
            else:
                raise ValueError("Unsupported TTS engine")
        except Exception as e:
            logger.error(f"Error in text-to-speech: {str(e)}")
            raise

    def is_speaking(self) -> bool:
        """Check if TTS is currently speaking (for pyttsx3)"""
        if settings.TTS_ENGINE == "pyttsx3" and self.tts_model:
            return self.tts_model.isBusy()
        return False

    def stop_speaking(self):
        """Stop TTS from speaking (for pyttsx3)"""
        if settings.TTS_ENGINE == "pyttsx3" and self.tts_model:
            self.tts_model.stop()

===== ./app/services/voice_agent_service.py =====
import logging
from fastapi import Depends
from app.services.llm_service import OllamaService
from app.services.rag_service import RAGService
from app.services.conversation_service import ConversationService
from app.models.agent_model import Agent
from app.config import settings

logger = logging.getLogger(__name__)

class VoiceAgentService:
    def __init__(
        self,
        llm_service: OllamaService = Depends(),
        rag_service: RAGService = Depends(),
        conversation_service: ConversationService = Depends(),
    ):
        self.llm_service = llm_service
        self.rag_service = rag_service
        self.conversation_service = conversation_service

    async def voice_chat(self, user_id: int, text: str, agent: Agent) -> str:
        """
        Handles the voice chat interaction.
        1. Gets conversation history.
        2. (Optional) Uses RAG to get context.
        3. Generates a response from the LLM.
        4. Stores the new interaction in the conversation history.
        """
        try:
            # 1. Get conversation history
            history = await self.conversation_service.get_conversation_history(user_id, agent.id)
            
            # 2. RAG (if enabled for the agent)
            rag_context = ""
            if agent.rag_enabled:
                rag_context = await self.rag_service.search(
                    user_id=user_id,
                    query=text,
                    collection_name=agent.id # Assuming one collection per agent
                )

            # 3. Generate response
            llm_response = await self.llm_service.generate_response(
                prompt=text,
                system_prompt=agent.prompt,
                history=history,
                context=rag_context,
                model=agent.llm_model
            )
            
            # 4. Store conversation
            await self.conversation_service.store_conversation(
                user_id=user_id,
                agent_id=agent.id,
                user_message=text,
                agent_message=llm_response
            )
            
            return llm_response
            
        except Exception as e:
            logger.error(f"Error in voice_chat service: {e}")
            # In case of an error, return a generic response
            return "I'm sorry, I encountered an error and can't respond right now."

===== ./app/dependencies.py =====
from fastapi import Depends
from app.utils.auth import verify_token, get_current_user
from app.utils.helpers import get_db
from sqlalchemy.orm import Session

# Database dependency
def get_db_session():
    return Depends(get_db)

# Authentication dependencies
def verify_token_dep():
    return Depends(verify_token)

def get_current_user_dep():
    return Depends(get_current_user)
===== ./app/init_db.py =====
import asyncio
from sqlalchemy import text
from app.database import engine, Base
from app.models.db_models import DBAgent
import logging

logging.basicConfig()
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

async def init_db():
    async with engine.begin() as conn:
        # Create schema
        await conn.execute(text("CREATE SCHEMA IF NOT EXISTS llm"))
        # Set search path to include llm schema
        await conn.execute(text("SET search_path TO llm, public"))
        # Create tables
        await conn.run_sync(Base.metadata.create_all)
    print("Database initialized successfully!")

if __name__ == "__main__":
    asyncio.run(init_db())
